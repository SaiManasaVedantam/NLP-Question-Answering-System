{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05664693-e3e0-4f50-8c50-5e6d52412f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all the required packages\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import urllib\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.wsd import lesk\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "\n",
    "# Start common things globally\n",
    "stop_words = stopwords.words('english') + list(string.punctuation)\n",
    "dependencyParser = CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "namedEntityTagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
    "\n",
    "# Performs Word tokenization on sentences\n",
    "def Tokenization(sentence):\n",
    "    tokens = [i for i in nltk.word_tokenize(sentence.lower()) if i not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Performs Word Lemmatization\n",
    "def Lemmatization(word_tokens):\n",
    "    lemmas = []\n",
    "    for token in word_tokens:\n",
    "        lemmas.append(wordnet_lemmatizer.lemmatize(token))\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "# Performs POS tagging\n",
    "def POSTagging(word_tokens):\n",
    "    POStags = nltk.pos_tag(word_tokens)\n",
    "    return POStags   \n",
    "\n",
    "\n",
    "# Obtains sentence heads\n",
    "def getHeads(sentence, word_tokens):\n",
    "    # Create a head list to add the heads\n",
    "    headList = []\n",
    "    \n",
    "    # Split the sentence\n",
    "    stripedSen = sentence.strip(\" '\\\"\")\n",
    "    if stripedSen != \"\":\n",
    "        # Perform dependency parse\n",
    "        depParse = dependencyParser.raw_parse(stripedSen)\n",
    "        parseTree = list(depParse)[0]\n",
    "        headWord = \"\"\n",
    "        headWord = [k[\"word\"] for k in parseTree.nodes.values() if k[\"head\"] == 0][0]\n",
    "        \n",
    "        # Appends head if it's not empty\n",
    "        if headWord != \"\":\n",
    "            headList.append([headWord])\n",
    "            \n",
    "        # Obtain head word based on two cases\n",
    "        else:\n",
    "            for i, pp in enumerate(tagged):\n",
    "                if pp.startswith(\"VB\"):\n",
    "                    headList.append([word_tokens[i]])\n",
    "                    break\n",
    "            if headWord == \"\":\n",
    "                for i, pp in enumerate(tagged):\n",
    "                    if pp.startswith(\"NN\"):\n",
    "                        headList.append([word_tokens[i]])\n",
    "                        break\n",
    "                        \n",
    "    # For empty sentence, we just append \"\" as head\n",
    "    else:\n",
    "        headList.append([\"\"])\n",
    " \n",
    "    return headList\n",
    "\n",
    "\n",
    "# Obtains WordNet Features\n",
    "def WordNetFeatures(sentence, word_tokens):\n",
    "    # Creates dictionaries for important word senses\n",
    "    hypernyms_list = []\n",
    "    hyponyms_list = []\n",
    "    meronyms_list = []\n",
    "    holonyms_list = []\n",
    "    synonyms_list = []\n",
    "    \n",
    "    # Populates the above dictionaries according to the word senses associated with them\n",
    "    for token in word_tokens:\n",
    "        # Extracts best sense for each word using LESK\n",
    "        best_sense = lesk(sentence, token)\n",
    "        \n",
    "        if best_sense is not None:\n",
    "            # Obtains Synonyms\n",
    "            synonym = token\n",
    "            if best_sense. lemmas()[0].name() != token:\n",
    "                synonym = best_sense.lemmas()[0].name()\n",
    "            synonyms_list.append(synonym)\n",
    "            \n",
    "            # Obtains Hypernyms\n",
    "            if best_sense.hypernyms() != []:\n",
    "                hypernyms_list.append(best_sense.hypernyms()[0].lemmas()[0].name())\n",
    "        \n",
    "            # Obtains Hyponyms\n",
    "            if best_sense.hyponyms() != []:\n",
    "                hyponyms_list.append(best_sense.hyponyms()[0].lemmas()[0].name())\n",
    "            \n",
    "            # Obtains Meronyms\n",
    "            if best_sense.part_meronyms() != []:\n",
    "                meronyms_list.append(best_sense.part_meronyms()[0].lemmas()[0].name())\n",
    "                \n",
    "            # Obtains Holonyms\n",
    "            if best_sense.part_holonyms() != []:\n",
    "                holonyms_list.append(best_sense.part_holonyms()[0].lemmas()[0].name())\n",
    "          \n",
    "        # When there's no best sense, the token itself is the Synonym\n",
    "        else:\n",
    "            synonyms_list.append(token)\n",
    "            \n",
    "    return hypernyms_list, hyponyms_list, meronyms_list, holonyms_list, synonyms_list\n",
    "   \n",
    "    \n",
    "# Performs Dependency Parsing\n",
    "def DependencyParsing(sentence):\n",
    "    # Perform dependency parsing\n",
    "    parse, = dependencyParser.raw_parse(sentence)\n",
    "    \n",
    "    # Dependency parsing to parse tree based patterns as features\n",
    "    depParseResult = list(parse.triples())\n",
    "    \n",
    "    return depParseResult\n",
    "    \n",
    "    \n",
    "# Obtains Named Entities\n",
    "def NamedEntities(sentence, tokens):\n",
    "    # Word tokenize again and use them if NEs are present\n",
    "    namedTokens = nltk.word_tokenize(sentence)\n",
    "    NEtags = None\n",
    "    \n",
    "    try:\n",
    "        NEtags = namedEntityTagger.tag(namedTokens)\n",
    "    except:\n",
    "        NEtags = namedEntityTagger.tag(tokens)\n",
    "        \n",
    "    return NEtags\n",
    "\n",
    "# NLP pipeline through which all the articles & question will pass\n",
    "def NLP_Pipeline(sentence, count, corpus_dict, articleName = None, queType = None):\n",
    "    #print(\"\\n------SENTENCE------\")\n",
    "    #print(sen)\n",
    "\n",
    "    word_tokens = Tokenization(sentence)\n",
    "    #print(\"\\nWord Tokenization : Done\")\n",
    "    #print(word_tokens)\n",
    "\n",
    "    word_NEtags = NamedEntities(sentence, word_tokens)\n",
    "    #print(\"\\nNamed Entity Tagging : Done\")\n",
    "    #print(word_NEtags)\n",
    "    \n",
    "    word_lemmas = Lemmatization(word_tokens)\n",
    "    #print(\"Word Lemmatization : Done\")\n",
    "    #print(word_lemmas)\n",
    "\n",
    "    word_POStags = POSTagging(word_tokens)\n",
    "    #print(\"POS Tagging : Done\")\n",
    "    #print(word_POStags)\n",
    "\n",
    "    hypernyms, hyponyms, meronyms, holonyms, synonyms = WordNetFeatures(sentence, word_tokens)\n",
    "    #print(\"WordNet Feature Extraction : Done\")\n",
    "    #print(holonyms)\n",
    "            \n",
    "    depParse = DependencyParsing(sentence)\n",
    "    #print(\"Dependency Parsing : Done\")\n",
    "    #print(depParse)\n",
    "\n",
    "    headList = getHeads(sentence, word_tokens)\n",
    "    #print(\"Obtaining Heads : Done\")\n",
    "    #print(headList)\n",
    "\n",
    "    # Process data format to suit the Elastic Search requirements\n",
    "    count = count + 1\n",
    "    corpus_dict[count] = {}\n",
    "            \n",
    "    corpus_dict[count][\"sentence\"] = {}\n",
    "    corpus_dict[count][\"sentence\"] = sentence\n",
    "            \n",
    "    corpus_dict[count][\"tokenized_text\"] = {}\n",
    "    corpus_dict[count][\"tokenized_text\"] = word_tokens\n",
    "            \n",
    "    corpus_dict[count][\"lemma\"] = {}\n",
    "    corpus_dict[count][\"lemma\"] = word_lemmas\n",
    "    \n",
    "    corpus_dict[count][\"ner_tag\"] = {}\n",
    "    corpus_dict[count][\"ner_tag\"] = str(dict(word_NEtags))\n",
    "            \n",
    "    corpus_dict[count][\"tagged\"] = {}\n",
    "    corpus_dict[count][\"tagged\"] = word_POStags\n",
    "            \n",
    "    corpus_dict[count][\"dependency_parse\"] = {}\n",
    "    corpus_dict[count][\"dependency_parse\"] = depParse\n",
    "            \n",
    "    corpus_dict[count][\"synonyms\"] = {}\n",
    "    corpus_dict[count][\"synonyms\"] = synonyms\n",
    "            \n",
    "    corpus_dict[count][\"hypernyms\"] = {}\n",
    "    corpus_dict[count][\"hypernyms\"] = hypernyms\n",
    "            \n",
    "    corpus_dict[count][\"hyponyms\"] = {}\n",
    "    corpus_dict[count][\"hyponyms\"] = hyponyms\n",
    "            \n",
    "    corpus_dict[count][\"meronyms\"] = {}\n",
    "    corpus_dict[count][\"meronyms\"] = meronyms\n",
    "            \n",
    "    corpus_dict[count][\"holonyms\"] = {}\n",
    "    corpus_dict[count][\"holonyms\"] = holonyms\n",
    "            \n",
    "    corpus_dict[count][\"head_word\"] = {}\n",
    "    corpus_dict[count][\"head_word\"] = headList[0]\n",
    "    \n",
    "    # For question, we don't have the article name and then it will have a questionType\n",
    "    if articleName is not None:\n",
    "        corpus_dict[count][\"file_name\"] = {}\n",
    "        corpus_dict[count][\"file_name\"] = articleName\n",
    "        \n",
    "        \n",
    "    # For question, we should add the question type\n",
    "    else:\n",
    "        corpus_dict[count][\"type_of_question\"] = {}\n",
    "        corpus_dict[count][\"type_of_question\"] = queType\n",
    "    \n",
    "    return count, corpus_dict\n",
    "    \n",
    "    \n",
    "# Main method\n",
    "if __name__ == \"__main__\":\n",
    "    # List of all article names in the repository\n",
    "    articleNames = [\"109.txt\", \"111.txt\", \"151.txt\", \"160.txt\", \"177.txt\", \n",
    "                    \"179.txt\",\"181.txt\", \"196.txt\", \"199.txt\", \"220.txt\", \n",
    "                    \"222.txt\", \"226.txt\", \"247.txt\", \"273.txt\", \"281.txt\", \n",
    "                    \"282.txt\", \"285.txt\", \"287.txt\", \"288.txt\", \"297.txt\", \n",
    "                    \"304.txt\", \"342.txt\", \"347.txt\", \"360.txt\", \"390.txt\", \n",
    "                    \"400.txt\", \"428.txt\", \"56.txt\", \"58.txt\", \"6.txt\"] \n",
    "    fileCount = len(articleNames)\n",
    "    \n",
    "    content = \"\"\n",
    "    folderPath = \"https://raw.githubusercontent.com/SaiManasaVedantam/NLP-QA-System-Datasets/main/Articles/\"\n",
    "    for i in range(fileCount):\n",
    "        print(\"\\nStarted Processing File : \" + articleNames[i])\n",
    "        fileName = folderPath + articleNames[i]\n",
    "        response = urllib.request.urlopen(fileName)\n",
    "        webContents = response.read()\n",
    "        stringTypeData = webContents.decode(\"utf-8\")\n",
    "        content = stringTypeData\n",
    "        count = 0\n",
    "        corpus_dict = {}\n",
    "\n",
    "        # Obtain wordnet lemmatizer\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # Get tokenized content\n",
    "        sentences = []\n",
    "        tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "        sentences.extend(tokenizer.tokenize(content))\n",
    "\n",
    "        # Sentence count\n",
    "        #print(\"Total Sentences After splitting the document: \", len(sentences))\n",
    "        print(\"Extracting features for each sentence in the file...\")\n",
    "    \n",
    "        # Extracting words\n",
    "        for sen in sentences:\n",
    "            count, corpus_dict = NLP_Pipeline(sen, count, corpus_dict, articleNames[i])\n",
    "                \n",
    "        output_name = '../Pipeline-Output/Parsed-' + articleNames[i]\n",
    "        with open(output_name, 'w+', encoding='utf8') as output_file:\n",
    "            json.dump(corpus_dict, output_file,  indent=4, sort_keys=True, separators=(',', ': '), ensure_ascii=False)\n",
    "        \n",
    "        print(\"Completed Processing File : \" + articleNames[i])\n",
    "        \n",
    "    print(\"Task 1 Successfully Completed !!!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d383c210-6a8e-4c65-b58a-20d268aaf96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Processing File : 109.txt\n",
      "Started Processing File : 111.txt\n",
      "Started Processing File : 151.txt\n",
      "Started Processing File : 160.txt\n",
      "Started Processing File : 177.txt\n",
      "Started Processing File : 179.txt\n",
      "Started Processing File : 181.txt\n",
      "Started Processing File : 196.txt\n",
      "Started Processing File : 199.txt\n",
      "Started Processing File : 220.txt\n",
      "Started Processing File : 222.txt\n",
      "Started Processing File : 226.txt\n",
      "Started Processing File : 247.txt\n",
      "Started Processing File : 273.txt\n",
      "Started Processing File : 281.txt\n",
      "Started Processing File : 282.txt\n",
      "Started Processing File : 285.txt\n",
      "Started Processing File : 287.txt\n",
      "Started Processing File : 288.txt\n",
      "Started Processing File : 297.txt\n",
      "Started Processing File : 304.txt\n",
      "Started Processing File : 342.txt\n",
      "Started Processing File : 347.txt\n",
      "Started Processing File : 360.txt\n",
      "Started Processing File : 390.txt\n",
      "Started Processing File : 400.txt\n",
      "Started Processing File : 428.txt\n",
      "Started Processing File : 56.txt\n",
      "Started Processing File : 58.txt\n",
      "Started Processing File : 6.txt\n",
      "Elastic Search Successfully Completed !!!\n"
     ]
    }
   ],
   "source": [
    "# Turn off unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all the required packages\n",
    "import ssl\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import RequestsHttpConnection\n",
    "\n",
    "# Main method\n",
    "if __name__ == \"__main__\":\n",
    "    # List of all article names in the repository\n",
    "    articleNames = [\"109.txt\", \"111.txt\", \"151.txt\", \"160.txt\", \"177.txt\", \n",
    "                    \"179.txt\",\"181.txt\", \"196.txt\", \"199.txt\", \"220.txt\", \n",
    "                    \"222.txt\", \"226.txt\", \"247.txt\", \"273.txt\", \"281.txt\", \n",
    "                    \"282.txt\", \"285.txt\", \"287.txt\", \"288.txt\", \"297.txt\", \n",
    "                    \"304.txt\", \"342.txt\", \"347.txt\", \"360.txt\", \"390.txt\", \n",
    "                    \"400.txt\", \"428.txt\", \"56.txt\", \"58.txt\", \"6.txt\"] \n",
    "    fileCount = len(articleNames)\n",
    "    \n",
    "    # Setup Elastic Search\n",
    "    elastic = Elasticsearch([{'host': 'localhost', 'port': 9200, 'use_ssl' : False, 'ssl_verify' : False}], timeout=30, max_retries=10)\n",
    "    \n",
    "    # Obtain requests from the page\n",
    "    req = requests.get(\"http://localhost:9200\", verify=False)\n",
    "    \n",
    "    # Use indexing\n",
    "    idx = 1\n",
    "    \n",
    "    content = \"\"\n",
    "    folderPath = \"https://raw.githubusercontent.com/SaiManasaVedantam/NLP-QA-System-Datasets/main/Pipeline-Output/Parsed-\"\n",
    "    for i in range(fileCount):\n",
    "        print(\"Started Processing File : \" + articleNames[i])\n",
    "        fileName = folderPath + articleNames[i]\n",
    "        response = urllib.request.urlopen(fileName)\n",
    "        webContents = response.read()\n",
    "        stringTypeData = webContents.decode(\"utf-8\")\n",
    "        content = stringTypeData\n",
    "        \n",
    "        # Obtain Json data from file contents\n",
    "        jsonFile = json.loads(content)\n",
    "        \n",
    "        # Creating new index \"articles\" for each line in the article\n",
    "        for key, value in jsonFile.items():\n",
    "            elastic.index(index = \"articles\", doc_type = \"text\", id = idx, body = value)\n",
    "            # print(\"Here\")\n",
    "            idx += 1\n",
    "            \n",
    "    print(\"Elastic Search Successfully Completed !!!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5ee96a9-0960-4a2d-8dfa-cc6cdb2d91b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'sentence': 'what is ukiyo-e an example of?', 'tokenized_text': ['ukiyo-e', 'example'], 'lemma': ['ukiyo-e', 'example'], 'ner_tag': \"{'what': 'O', 'is': 'O', 'ukiyo': 'O', '-': 'O', 'e': 'O', 'an': 'O', 'example': 'O', 'of': 'O', '?': 'O'}\", 'tagged': [('ukiyo-e', 'JJ'), ('example', 'NN')], 'dependency_parse': [(('what', 'WP'), 'cop', ('is', 'VBZ')), (('what', 'WP'), 'nsubj', ('example', 'NN')), (('example', 'NN'), 'dep', ('e', 'NN')), (('e', 'NN'), 'amod', ('ukiyo', 'JJ')), (('e', 'NN'), 'punct', ('-', 'HYPH')), (('example', 'NN'), 'det', ('an', 'DT')), (('example', 'NN'), 'acl', ('of', 'IN')), (('what', 'WP'), 'punct', ('?', '.'))], 'synonyms': ['ukiyo-e', 'model'], 'hypernyms': ['representation'], 'hyponyms': ['lodestar'], 'meronyms': [], 'holonyms': [], 'head_word': ['what']}, 2: {'sentence': 'A botanist or plant scientist is a scientist who specializes in this field.', 'tokenized_text': ['botanist', 'plant', 'scientist', 'scientist', 'specializes', 'field'], 'lemma': ['botanist', 'plant', 'scientist', 'scientist', 'specializes', 'field'], 'ner_tag': \"{'A': 'O', 'botanist': 'TITLE', 'or': 'O', 'plant': 'O', 'scientist': 'TITLE', 'is': 'O', 'a': 'O', 'who': 'O', 'specializes': 'O', 'in': 'O', 'this': 'O', 'field': 'O', '.': 'O'}\", 'tagged': [('botanist', 'NN'), ('plant', 'NN'), ('scientist', 'NN'), ('scientist', 'NN'), ('specializes', 'VBZ'), ('field', 'NN')], 'dependency_parse': [(('scientist', 'NN'), 'advmod', ('A', 'DT')), (('scientist', 'NN'), 'nsubj', ('botanist', 'NN')), (('botanist', 'NN'), 'conj', ('scientist', 'NN')), (('scientist', 'NN'), 'cc', ('or', 'CC')), (('scientist', 'NN'), 'compound', ('plant', 'NN')), (('scientist', 'NN'), 'cop', ('is', 'VBZ')), (('scientist', 'NN'), 'det', ('a', 'DT')), (('scientist', 'NN'), 'acl:relcl', ('specializes', 'VBZ')), (('specializes', 'VBZ'), 'nsubj', ('who', 'WP')), (('specializes', 'VBZ'), 'obl', ('field', 'NN')), (('field', 'NN'), 'case', ('in', 'IN')), (('field', 'NN'), 'det', ('this', 'DT')), (('scientist', 'NN'), 'punct', ('.', '.'))], 'synonyms': ['botanist', 'plant', 'scientist', 'scientist', 'speciate', 'sphere'], 'hypernyms': ['biologist', 'put', 'person', 'person', 'evolve', 'environment'], 'hyponyms': ['mycologist', 'bibliotist', 'bibliotist', 'distaff'], 'meronyms': [], 'holonyms': [], 'head_word': ['scientist'], 'file_name': '222.txt'}, 3: {'sentence': 'The term \"botany\" comes from the Ancient Greek word βοτάνη (botanē) meaning \"pasture\", \"grass\", or \"fodder\"; βοτάνη is in turn derived from βόσκειν (boskein), \"to feed\" or \"to graze\".', 'tokenized_text': ['term', '``', 'botany', \"''\", 'comes', 'ancient', 'greek', 'word', 'βοτάνη', 'botanē', 'meaning', '``', 'pasture', \"''\", '``', 'grass', \"''\", '``', 'fodder', \"''\", 'βοτάνη', 'turn', 'derived', 'βόσκειν', 'boskein', '``', 'feed', \"''\", '``', 'graze', \"''\"], 'lemma': ['term', '``', 'botany', \"''\", 'come', 'ancient', 'greek', 'word', 'βοτάνη', 'botanē', 'meaning', '``', 'pasture', \"''\", '``', 'grass', \"''\", '``', 'fodder', \"''\", 'βοτάνη', 'turn', 'derived', 'βόσκειν', 'boskein', '``', 'feed', \"''\", '``', 'graze', \"''\"], 'ner_tag': '{\\'The\\': \\'O\\', \\'term\\': \\'O\\', \\'``\\': \\'O\\', \\'botany\\': \\'O\\', \"\\'\\'\": \\'O\\', \\'comes\\': \\'O\\', \\'from\\': \\'O\\', \\'the\\': \\'O\\', \\'Ancient\\': \\'O\\', \\'Greek\\': \\'NATIONALITY\\', \\'word\\': \\'O\\', \\'βοτάνη\\': \\'O\\', \\'(\\': \\'O\\', \\'botanē\\': \\'O\\', \\')\\': \\'O\\', \\'meaning\\': \\'O\\', \\'pasture\\': \\'O\\', \\',\\': \\'O\\', \\'grass\\': \\'O\\', \\'or\\': \\'O\\', \\'fodder\\': \\'O\\', \\';\\': \\'O\\', \\'is\\': \\'O\\', \\'in\\': \\'O\\', \\'turn\\': \\'O\\', \\'derived\\': \\'O\\', \\'βόσκειν\\': \\'O\\', \\'boskein\\': \\'O\\', \\'to\\': \\'O\\', \\'feed\\': \\'O\\', \\'graze\\': \\'O\\', \\'.\\': \\'O\\'}', 'tagged': [('term', 'NN'), ('``', '``'), ('botany', 'NN'), (\"''\", \"''\"), ('comes', 'VBZ'), ('ancient', 'JJ'), ('greek', 'JJ'), ('word', 'NN'), ('βοτάνη', 'NNP'), ('botanē', 'NN'), ('meaning', 'VBG'), ('``', '``'), ('pasture', 'NN'), (\"''\", \"''\"), ('``', '``'), ('grass', 'NN'), (\"''\", \"''\"), ('``', '``'), ('fodder', 'NN'), (\"''\", \"''\"), ('βοτάνη', 'CC'), ('turn', 'VB'), ('derived', 'VBN'), ('βόσκειν', 'NNP'), ('boskein', 'IN'), ('``', '``'), ('feed', 'NN'), (\"''\", \"''\"), ('``', '``'), ('graze', 'NN'), (\"''\", \"''\")], 'dependency_parse': [(('comes', 'VBZ'), 'nsubj', ('botany', 'NN')), (('botany', 'NN'), 'det', ('The', 'DT')), (('botany', 'NN'), 'compound', ('term', 'NN')), (('botany', 'NN'), 'punct', ('\"', '``')), (('botany', 'NN'), 'punct', ('\"', \"''\")), (('comes', 'VBZ'), 'obl', ('βοτάνη', 'NN')), (('βοτάνη', 'NN'), 'case', ('from', 'IN')), (('βοτάνη', 'NN'), 'det', ('the', 'DT')), (('βοτάνη', 'NN'), 'amod', ('Ancient', 'JJ')), (('βοτάνη', 'NN'), 'amod', ('Greek', 'JJ')), (('βοτάνη', 'NN'), 'compound', ('word', 'NN')), (('βοτάνη', 'NN'), 'dep', ('botanē', 'NN')), (('botanē', 'NN'), 'punct', ('(', '-LRB-')), (('botanē', 'NN'), 'punct', (')', '-RRB-')), (('βοτάνη', 'NN'), 'dep', ('pasture', 'NN')), (('pasture', 'NN'), 'compound', ('meaning', 'NN')), (('pasture', 'NN'), 'punct', ('\"', \"''\")), (('pasture', 'NN'), 'punct', ('\"', \"''\")), (('comes', 'VBZ'), 'punct', (',', ',')), (('comes', 'VBZ'), 'punct', ('\"', '``')), (('comes', 'VBZ'), 'dep', ('grass', 'NN')), (('grass', 'NN'), 'punct', ('\"', \"''\")), (('grass', 'NN'), 'punct', (',', ',')), (('grass', 'NN'), 'conj', ('fodder', 'NN')), (('fodder', 'NN'), 'cc', ('or', 'CC')), (('fodder', 'NN'), 'punct', ('\"', '``')), (('fodder', 'NN'), 'punct', ('\"', \"''\")), (('grass', 'NN'), 'punct', (';', ':')), (('grass', 'NN'), 'parataxis', ('turn', 'NN')), (('turn', 'NN'), 'nsubj', ('βοτάνη', 'NN')), (('turn', 'NN'), 'cop', ('is', 'VBZ')), (('turn', 'NN'), 'case', ('in', 'IN')), (('turn', 'NN'), 'acl', ('derived', 'VBN')), (('derived', 'VBN'), 'obl', ('βόσκειν', 'NN')), (('βόσκειν', 'NN'), 'case', ('from', 'IN')), (('βόσκειν', 'NN'), 'appos', ('boskein', 'NN')), (('boskein', 'NN'), 'punct', ('(', '-LRB-')), (('boskein', 'NN'), 'punct', (')', '-RRB-')), (('turn', 'NN'), 'punct', (',', ',')), (('turn', 'NN'), 'punct', ('\"', \"''\")), (('turn', 'NN'), 'dep', ('feed', 'VB')), (('feed', 'VB'), 'mark', ('to', 'TO')), (('feed', 'VB'), 'punct', ('\"', \"''\")), (('feed', 'VB'), 'conj', ('graze', 'VB')), (('graze', 'VB'), 'cc', ('or', 'CC')), (('graze', 'VB'), 'punct', ('\"', '``')), (('graze', 'VB'), 'mark', ('to', 'TO')), (('graze', 'VB'), 'punct', ('\"', \"''\")), (('comes', 'VBZ'), 'punct', ('.', '.'))], 'synonyms': ['terminus', '``', 'vegetation', \"''\", 'hail', 'ancient', 'Greek', 'word', 'βοτάνη', 'botanē', 'mean', '``', 'pasture', \"''\", '``', 'supergrass', \"''\", '``', 'fodder', \"''\", 'βοτάνη', 'turning', 'derive', 'βόσκειν', 'boskein', '``', 'tip', \"''\", '``', 'graze', \"''\"], 'hypernyms': ['statue', 'collection', 'be', 'person', 'European', 'computer_memory_unit', 'intend', 'grassland', 'informer', 'feed', 'movement', 'give', 'injure'], 'hyponyms': ['brier', 'Achaean', 'commons', 'alfalfa', 'return'], 'meronyms': ['byte'], 'holonyms': ['kilobyte', 'country'], 'head_word': ['comes'], 'file_name': '222.txt'}, 4: {'sentence': 'Traditionally, botany has also included the study of fungi and algae by mycologists and phycologists respectively, with the study of these three groups of organisms remaining within the sphere of interest of the International Botanical Congress.', 'tokenized_text': ['traditionally', 'botany', 'also', 'included', 'study', 'fungi', 'algae', 'mycologists', 'phycologists', 'respectively', 'study', 'three', 'groups', 'organisms', 'remaining', 'within', 'sphere', 'interest', 'international', 'botanical', 'congress'], 'lemma': ['traditionally', 'botany', 'also', 'included', 'study', 'fungi', 'algae', 'mycologist', 'phycologists', 'respectively', 'study', 'three', 'group', 'organism', 'remaining', 'within', 'sphere', 'interest', 'international', 'botanical', 'congress'], 'ner_tag': \"{'Traditionally': 'O', ',': 'O', 'botany': 'O', 'has': 'O', 'also': 'O', 'included': 'O', 'the': 'O', 'study': 'O', 'of': 'O', 'fungi': 'O', 'and': 'O', 'algae': 'O', 'by': 'O', 'mycologists': 'O', 'phycologists': 'O', 'respectively': 'O', 'with': 'O', 'these': 'O', 'three': 'NUMBER', 'groups': 'O', 'organisms': 'O', 'remaining': 'O', 'within': 'O', 'sphere': 'O', 'interest': 'O', 'International': 'ORGANIZATION', 'Botanical': 'ORGANIZATION', 'Congress': 'ORGANIZATION', '.': 'O'}\", 'tagged': [('traditionally', 'RB'), ('botany', 'NN'), ('also', 'RB'), ('included', 'VBD'), ('study', 'NN'), ('fungi', 'NNS'), ('algae', 'VBP'), ('mycologists', 'NNS'), ('phycologists', 'NNS'), ('respectively', 'RB'), ('study', 'VBP'), ('three', 'CD'), ('groups', 'NNS'), ('organisms', 'VBP'), ('remaining', 'VBG'), ('within', 'IN'), ('sphere', 'JJ'), ('interest', 'NN'), ('international', 'JJ'), ('botanical', 'JJ'), ('congress', 'NN')], 'dependency_parse': [(('included', 'VBN'), 'advmod', ('Traditionally', 'RB')), (('included', 'VBN'), 'punct', (',', ',')), (('included', 'VBN'), 'nsubj', ('botany', 'NN')), (('included', 'VBN'), 'aux', ('has', 'VBZ')), (('included', 'VBN'), 'advmod', ('also', 'RB')), (('included', 'VBN'), 'obj', ('study', 'NN')), (('study', 'NN'), 'det', ('the', 'DT')), (('study', 'NN'), 'nmod', ('fungi', 'NNS')), (('fungi', 'NNS'), 'case', ('of', 'IN')), (('fungi', 'NNS'), 'conj', ('algae', 'NNS')), (('algae', 'NNS'), 'cc', ('and', 'CC')), (('study', 'NN'), 'nmod', ('mycologists', 'NNS')), (('mycologists', 'NNS'), 'case', ('by', 'IN')), (('mycologists', 'NNS'), 'conj', ('phycologists', 'NNS')), (('phycologists', 'NNS'), 'cc', ('and', 'CC')), (('included', 'VBN'), 'advmod', ('respectively', 'RB')), (('included', 'VBN'), 'punct', (',', ',')), (('included', 'VBN'), 'obl', ('study', 'NN')), (('study', 'NN'), 'case', ('with', 'IN')), (('study', 'NN'), 'det', ('the', 'DT')), (('study', 'NN'), 'nmod', ('groups', 'NNS')), (('groups', 'NNS'), 'case', ('of', 'IN')), (('groups', 'NNS'), 'det', ('these', 'DT')), (('groups', 'NNS'), 'nummod', ('three', 'CD')), (('groups', 'NNS'), 'nmod', ('organisms', 'NNS')), (('organisms', 'NNS'), 'case', ('of', 'IN')), (('study', 'NN'), 'acl', ('remaining', 'VBG')), (('remaining', 'VBG'), 'obl', ('sphere', 'NN')), (('sphere', 'NN'), 'case', ('within', 'IN')), (('sphere', 'NN'), 'det', ('the', 'DT')), (('sphere', 'NN'), 'nmod', ('interest', 'NN')), (('interest', 'NN'), 'case', ('of', 'IN')), (('interest', 'NN'), 'nmod', ('Congress', 'NNP')), (('Congress', 'NNP'), 'case', ('of', 'IN')), (('Congress', 'NNP'), 'det', ('the', 'DT')), (('Congress', 'NNP'), 'compound', ('International', 'NNP')), (('Congress', 'NNP'), 'compound', ('Botanical', 'NNP')), (('included', 'VBN'), 'punct', ('.', '.'))], 'synonyms': ['traditionally', 'vegetation', 'besides', 'include', 'survey', 'fungus', 'alga', 'mycologist', 'phycologists', 'respectively', 'survey', 'trey', 'group', 'organism', 'stay', 'inside', 'sphere', 'sake', 'International', 'botanical', 'sexual_intercourse'], 'hypernyms': ['collection', 'add', 'examination', 'organism', 'protoctist', 'botanist', 'examination', 'playing_card', 'meet', 'system', 'be', 'round_shape', 'benefit', 'socialism', 'drug', 'sexual_activity'], 'hyponyms': ['brier', 'resurvey', 'agaric', 'brown_algae', 'resurvey', 'brigade', 'bide', 'behalf', 'defloration'], 'meronyms': ['cap', 'steradian', 'insemination'], 'holonyms': [], 'head_word': ['included'], 'file_name': '222.txt'}, 5: {'sentence': 'Nowadays, botanists study approximately 400,000 species of living organisms of which some 260,000 species are vascular plants and about 248,000 are flowering plants.', 'tokenized_text': ['nowadays', 'botanists', 'study', 'approximately', '400,000', 'species', 'living', 'organisms', '260,000', 'species', 'vascular', 'plants', '248,000', 'flowering', 'plants'], 'lemma': ['nowadays', 'botanist', 'study', 'approximately', '400,000', 'specie', 'living', 'organism', '260,000', 'specie', 'vascular', 'plant', '248,000', 'flowering', 'plant'], 'ner_tag': \"{'Nowadays': 'O', ',': 'O', 'botanists': 'O', 'study': 'O', 'approximately': 'O', '400,000': 'NUMBER', 'species': 'O', 'of': 'O', 'living': 'O', 'organisms': 'O', 'which': 'O', 'some': 'O', '260,000': 'NUMBER', 'are': 'O', 'vascular': 'O', 'plants': 'O', 'and': 'O', 'about': 'O', '248,000': 'NUMBER', 'flowering': 'O', '.': 'O'}\", 'tagged': [('nowadays', 'NNS'), ('botanists', 'NNS'), ('study', 'VBP'), ('approximately', 'RB'), ('400,000', 'CD'), ('species', 'NNS'), ('living', 'VBG'), ('organisms', 'JJ'), ('260,000', 'CD'), ('species', 'NNS'), ('vascular', 'JJ'), ('plants', 'NNS'), ('248,000', 'CD'), ('flowering', 'NN'), ('plants', 'NNS')], 'dependency_parse': [(('flowering', 'VBG'), 'advmod', ('Nowadays', 'RB')), (('flowering', 'VBG'), 'punct', (',', ',')), (('flowering', 'VBG'), 'dep', ('study', 'NN')), (('study', 'NN'), 'compound', ('botanists', 'NNS')), (('flowering', 'VBG'), 'nsubj', ('species', 'NNS')), (('species', 'NNS'), 'nummod', ('400,000', 'CD')), (('400,000', 'CD'), 'advmod', ('approximately', 'RB')), (('species', 'NNS'), 'nmod', ('organisms', 'NNS')), (('organisms', 'NNS'), 'case', ('of', 'IN')), (('organisms', 'NNS'), 'amod', ('living', 'VBG')), (('organisms', 'NNS'), 'acl:relcl', ('plants', 'NNS')), (('plants', 'NNS'), 'obl', ('which', 'WDT')), (('which', 'WDT'), 'case', ('of', 'IN')), (('plants', 'NNS'), 'nsubj', ('species', 'NNS')), (('species', 'NNS'), 'det', ('some', 'DT')), (('species', 'NNS'), 'nummod', ('260,000', 'CD')), (('plants', 'NNS'), 'cop', ('are', 'VBP')), (('plants', 'NNS'), 'amod', ('vascular', 'JJ')), (('plants', 'NNS'), 'conj', ('248,000', 'CD')), (('248,000', 'CD'), 'cc', ('and', 'CC')), (('248,000', 'CD'), 'case', ('about', 'IN')), (('flowering', 'VBG'), 'aux', ('are', 'VBP')), (('flowering', 'VBG'), 'obj', ('plants', 'NNS')), (('flowering', 'VBG'), 'punct', ('.', '.'))], 'synonyms': ['present', 'botanist', 'survey', 'approximately', '400,000', 'species', 'live', 'organism', '260,000', 'species', 'vascular', 'plant', '248,000', 'unfolding', 'plant'], 'hypernyms': ['time', 'biologist', 'examination', 'kind', 'system', 'kind', 'put', 'development', 'put'], 'hyponyms': ['date', 'mycologist', 'resurvey'], 'meronyms': [], 'holonyms': [], 'head_word': ['flowering'], 'file_name': '222.txt'}, 6: {'sentence': 'Botany originated in prehistory as herbalism with the efforts of early humans to identify – and later cultivate – edible, medicinal and poisonous plants, making it one of the oldest branches of science.', 'tokenized_text': ['botany', 'originated', 'prehistory', 'herbalism', 'efforts', 'early', 'humans', 'identify', '–', 'later', 'cultivate', '–', 'edible', 'medicinal', 'poisonous', 'plants', 'making', 'one', 'oldest', 'branches', 'science'], 'lemma': ['botany', 'originated', 'prehistory', 'herbalism', 'effort', 'early', 'human', 'identify', '–', 'later', 'cultivate', '–', 'edible', 'medicinal', 'poisonous', 'plant', 'making', 'one', 'oldest', 'branch', 'science'], 'ner_tag': \"{'Botany': 'O', 'originated': 'O', 'in': 'O', 'prehistory': 'O', 'as': 'O', 'herbalism': 'O', 'with': 'O', 'the': 'O', 'efforts': 'O', 'of': 'O', 'early': 'O', 'humans': 'O', 'to': 'O', 'identify': 'O', '–': 'O', 'and': 'O', 'later': 'O', 'cultivate': 'O', 'edible': 'O', ',': 'O', 'medicinal': 'O', 'poisonous': 'O', 'plants': 'O', 'making': 'O', 'it': 'O', 'one': 'NUMBER', 'oldest': 'O', 'branches': 'O', 'science': 'O', '.': 'O'}\", 'tagged': [('botany', 'NN'), ('originated', 'VBD'), ('prehistory', 'JJ'), ('herbalism', 'NN'), ('efforts', 'NNS'), ('early', 'RB'), ('humans', 'NNS'), ('identify', 'VBP'), ('–', 'RB'), ('later', 'RB'), ('cultivate', 'VB'), ('–', 'NNP'), ('edible', 'JJ'), ('medicinal', 'JJ'), ('poisonous', 'JJ'), ('plants', 'NNS'), ('making', 'VBG'), ('one', 'CD'), ('oldest', 'JJS'), ('branches', 'NNS'), ('science', 'NN')], 'dependency_parse': [(('originated', 'VBD'), 'nsubj', ('Botany', 'NN')), (('originated', 'VBD'), 'obl', ('prehistory', 'NN')), (('prehistory', 'NN'), 'case', ('in', 'IN')), (('originated', 'VBD'), 'obl', ('herbalism', 'NN')), (('herbalism', 'NN'), 'case', ('as', 'IN')), (('herbalism', 'NN'), 'nmod', ('efforts', 'NNS')), (('efforts', 'NNS'), 'case', ('with', 'IN')), (('efforts', 'NNS'), 'det', ('the', 'DT')), (('efforts', 'NNS'), 'nmod', ('humans', 'NNS')), (('humans', 'NNS'), 'case', ('of', 'IN')), (('humans', 'NNS'), 'amod', ('early', 'JJ')), (('efforts', 'NNS'), 'acl', ('identify', 'VB')), (('identify', 'VB'), 'mark', ('to', 'TO')), (('identify', 'VB'), 'dep', ('–', 'SYM')), (('identify', 'VB'), 'conj', ('cultivate', 'VBP')), (('cultivate', 'VBP'), 'cc', ('and', 'CC')), (('cultivate', 'VBP'), 'advmod', ('later', 'RB')), (('identify', 'VB'), 'dep', ('–', 'SYM')), (('identify', 'VB'), 'obj', ('plants', 'NNS')), (('plants', 'NNS'), 'amod', ('edible', 'JJ')), (('edible', 'JJ'), 'punct', (',', ',')), (('edible', 'JJ'), 'conj', ('medicinal', 'JJ')), (('edible', 'JJ'), 'conj', ('poisonous', 'JJ')), (('poisonous', 'JJ'), 'cc', ('and', 'CC')), (('plants', 'NNS'), 'punct', (',', ',')), (('plants', 'NNS'), 'acl', ('making', 'VBG')), (('making', 'VBG'), 'xcomp', ('one', 'CD')), (('one', 'CD'), 'nsubj', ('it', 'PRP')), (('one', 'CD'), 'nmod', ('branches', 'NNS')), (('branches', 'NNS'), 'case', ('of', 'IN')), (('branches', 'NNS'), 'det', ('the', 'DT')), (('branches', 'NNS'), 'amod', ('oldest', 'JJS')), (('branches', 'NNS'), 'nmod', ('science', 'NN')), (('science', 'NN'), 'case', ('of', 'IN')), (('originated', 'VBD'), 'punct', ('.', '.'))], 'synonyms': ['vegetation', 'originate', 'prehistory', 'herbalism', 'feat', 'early', 'world', 'name', '–', 'subsequently', 'educate', '–', 'edible', 'medicative', 'poisonous', 'plant', 'take', 'one', 'Old', 'outgrowth', 'science'], 'hypernyms': ['collection', 'begin', 'time_period', 'accomplishment', 'group', 'denote', 'polish', 'put', 'head', 'consequence', 'discipline'], 'hyponyms': ['brier', 'derring-do', 'announce', 'sophisticate', 'agrobiology'], 'meronyms': ['Bronze_Age', 'scientific_theory'], 'holonyms': ['scientific_knowledge'], 'head_word': ['originated'], 'file_name': '222.txt'}, 7: {'sentence': 'Medieval physic gardens, often attached to monasteries, contained plants of medical importance.', 'tokenized_text': ['medieval', 'physic', 'gardens', 'often', 'attached', 'monasteries', 'contained', 'plants', 'medical', 'importance'], 'lemma': ['medieval', 'physic', 'garden', 'often', 'attached', 'monastery', 'contained', 'plant', 'medical', 'importance'], 'ner_tag': \"{'Medieval': 'DATE', 'physic': 'O', 'gardens': 'O', ',': 'O', 'often': 'O', 'attached': 'O', 'to': 'O', 'monasteries': 'O', 'contained': 'O', 'plants': 'O', 'of': 'O', 'medical': 'O', 'importance': 'O', '.': 'O'}\", 'tagged': [('medieval', 'JJ'), ('physic', 'JJ'), ('gardens', 'NNS'), ('often', 'RB'), ('attached', 'JJ'), ('monasteries', 'NNS'), ('contained', 'VBD'), ('plants', 'NNS'), ('medical', 'JJ'), ('importance', 'NN')], 'dependency_parse': [(('contained', 'VBD'), 'nsubj', ('gardens', 'NNS')), (('gardens', 'NNS'), 'amod', ('Medieval', 'JJ')), (('gardens', 'NNS'), 'amod', ('physic', 'JJ')), (('gardens', 'NNS'), 'punct', (',', ',')), (('gardens', 'NNS'), 'acl', ('attached', 'VBN')), (('attached', 'VBN'), 'advmod', ('often', 'RB')), (('attached', 'VBN'), 'obl', ('monasteries', 'NNS')), (('monasteries', 'NNS'), 'case', ('to', 'IN')), (('gardens', 'NNS'), 'punct', (',', ',')), (('contained', 'VBD'), 'obj', ('plants', 'NNS')), (('plants', 'NNS'), 'nmod', ('importance', 'NN')), (('importance', 'NN'), 'case', ('of', 'IN')), (('importance', 'NN'), 'amod', ('medical', 'JJ')), (('contained', 'VBD'), 'punct', ('.', '.'))], 'synonyms': ['medieval', 'purgative', 'garden', 'often', 'impound', 'monastery', 'incorporate', 'plant', 'checkup', 'importance'], 'hypernyms': ['medicine', 'yard', 'take', 'religious_residence', 'include', 'put', 'examination', 'standing'], 'hyponyms': ['aloes', 'condemn', 'abbey', 'emphasis'], 'meronyms': ['patio', 'cell', 'ballistocardiogram'], 'holonyms': [], 'head_word': ['contained'], 'file_name': '222.txt'}, 8: {'sentence': 'They were forerunners of the first botanical gardens attached to universities, founded from the 1540s onwards.', 'tokenized_text': ['forerunners', 'first', 'botanical', 'gardens', 'attached', 'universities', 'founded', '1540s', 'onwards'], 'lemma': ['forerunner', 'first', 'botanical', 'garden', 'attached', 'university', 'founded', '1540s', 'onwards'], 'ner_tag': \"{'They': 'O', 'were': 'O', 'forerunners': 'O', 'of': 'O', 'the': 'DATE', 'first': 'ORDINAL', 'botanical': 'O', 'gardens': 'O', 'attached': 'O', 'to': 'O', 'universities': 'O', ',': 'O', 'founded': 'O', 'from': 'O', '1540s': 'DATE', 'onwards': 'O', '.': 'O'}\", 'tagged': [('forerunners', 'NNS'), ('first', 'RB'), ('botanical', 'JJ'), ('gardens', 'NNS'), ('attached', 'VBN'), ('universities', 'NNS'), ('founded', 'VBD'), ('1540s', 'CD'), ('onwards', 'NNS')], 'dependency_parse': [(('forerunners', 'NNS'), 'nsubj', ('They', 'PRP')), (('forerunners', 'NNS'), 'cop', ('were', 'VBD')), (('forerunners', 'NNS'), 'nmod', ('gardens', 'NNS')), (('gardens', 'NNS'), 'case', ('of', 'IN')), (('gardens', 'NNS'), 'det', ('the', 'DT')), (('gardens', 'NNS'), 'amod', ('first', 'JJ')), (('gardens', 'NNS'), 'amod', ('botanical', 'JJ')), (('forerunners', 'NNS'), 'acl', ('attached', 'VBN')), (('attached', 'VBN'), 'obl', ('universities', 'NNS')), (('universities', 'NNS'), 'case', ('to', 'IN')), (('forerunners', 'NNS'), 'punct', (',', ',')), (('forerunners', 'NNS'), 'acl', ('founded', 'VBN')), (('founded', 'VBN'), 'obl', ('1540s', 'NNS')), (('1540s', 'NNS'), 'case', ('from', 'IN')), (('1540s', 'NNS'), 'det', ('the', 'DT')), (('1540s', 'NNS'), 'advmod', ('onwards', 'RB')), (('forerunners', 'NNS'), 'punct', ('.', '.'))], 'synonyms': ['precursor', 'first_gear', 'botanical', 'garden', 'impound', 'university', 'establish', '1540s', 'ahead'], 'hypernyms': ['person', 'gear', 'drug', 'yard', 'take', 'educational_institution'], 'hyponyms': ['predecessor', 'condemn', 'multiversity', 'build'], 'meronyms': ['patio'], 'holonyms': ['car'], 'head_word': ['forerunners'], 'file_name': '222.txt'}, 9: {'sentence': 'One of the earliest was the Padua botanical garden.', 'tokenized_text': ['one', 'earliest', 'padua', 'botanical', 'garden'], 'lemma': ['one', 'earliest', 'padua', 'botanical', 'garden'], 'ner_tag': \"{'One': 'NUMBER', 'of': 'O', 'the': 'O', 'earliest': 'O', 'was': 'O', 'Padua': 'LOCATION', 'botanical': 'O', 'garden': 'O', '.': 'O'}\", 'tagged': [('one', 'CD'), ('earliest', 'JJS'), ('padua', 'NN'), ('botanical', 'JJ'), ('garden', 'NN')], 'dependency_parse': [(('garden', 'NN'), 'nsubj', ('One', 'CD')), (('One', 'CD'), 'nmod', ('earliest', 'JJS')), (('earliest', 'JJS'), 'case', ('of', 'IN')), (('earliest', 'JJS'), 'det', ('the', 'DT')), (('garden', 'NN'), 'cop', ('was', 'VBD')), (('garden', 'NN'), 'det', ('the', 'DT')), (('garden', 'NN'), 'compound', ('Padua', 'NNP')), (('garden', 'NN'), 'amod', ('botanical', 'JJ')), (('garden', 'NN'), 'punct', ('.', '.'))], 'synonyms': ['one', 'early', 'Padua', 'botanical', 'garden'], 'hypernyms': ['drug', 'yard'], 'hyponyms': [], 'meronyms': ['patio'], 'holonyms': ['Veneto'], 'head_word': ['garden'], 'file_name': '222.txt'}, 10: {'sentence': 'These gardens facilitated the academic study of plants.', 'tokenized_text': ['gardens', 'facilitated', 'academic', 'study', 'plants'], 'lemma': ['garden', 'facilitated', 'academic', 'study', 'plant'], 'ner_tag': \"{'These': 'O', 'gardens': 'O', 'facilitated': 'O', 'the': 'O', 'academic': 'O', 'study': 'O', 'of': 'O', 'plants': 'O', '.': 'O'}\", 'tagged': [('gardens', 'NNS'), ('facilitated', 'VBN'), ('academic', 'JJ'), ('study', 'NN'), ('plants', 'NNS')], 'dependency_parse': [(('facilitated', 'VBD'), 'nsubj', ('gardens', 'NNS')), (('gardens', 'NNS'), 'det', ('These', 'DT')), (('facilitated', 'VBD'), 'obj', ('study', 'NN')), (('study', 'NN'), 'det', ('the', 'DT')), (('study', 'NN'), 'amod', ('academic', 'JJ')), (('study', 'NN'), 'nmod', ('plants', 'NNS')), (('plants', 'NNS'), 'case', ('of', 'IN')), (('facilitated', 'VBD'), 'punct', ('.', '.'))], 'synonyms': ['garden', 'help', 'academician', 'survey', 'plant'], 'hypernyms': ['yard', 'serve', 'educator', 'examination', 'put'], 'hyponyms': ['professor', 'resurvey'], 'meronyms': ['patio'], 'holonyms': [], 'head_word': ['facilitated'], 'file_name': '222.txt'}, 11: {'sentence': 'Efforts to catalogue and describe their collections were the beginnings of plant taxonomy, and led in 1753 to the binomial system of Carl Linnaeus that remains in use to this day.', 'tokenized_text': ['efforts', 'catalogue', 'describe', 'collections', 'beginnings', 'plant', 'taxonomy', 'led', '1753', 'binomial', 'system', 'carl', 'linnaeus', 'remains', 'use', 'day'], 'lemma': ['effort', 'catalogue', 'describe', 'collection', 'beginning', 'plant', 'taxonomy', 'led', '1753', 'binomial', 'system', 'carl', 'linnaeus', 'remains', 'use', 'day'], 'ner_tag': \"{'Efforts': 'O', 'to': 'O', 'catalogue': 'O', 'and': 'O', 'describe': 'O', 'their': 'O', 'collections': 'O', 'were': 'O', 'the': 'O', 'beginnings': 'O', 'of': 'O', 'plant': 'O', 'taxonomy': 'O', ',': 'O', 'led': 'O', 'in': 'O', '1753': 'DATE', 'binomial': 'O', 'system': 'O', 'Carl': 'PERSON', 'Linnaeus': 'PERSON', 'that': 'O', 'remains': 'O', 'use': 'O', 'this': 'DATE', 'day': 'DATE', '.': 'O'}\", 'tagged': [('efforts', 'NNS'), ('catalogue', 'NN'), ('describe', 'NN'), ('collections', 'NNS'), ('beginnings', 'NNS'), ('plant', 'NN'), ('taxonomy', 'NN'), ('led', 'VBD'), ('1753', 'CD'), ('binomial', 'JJ'), ('system', 'NN'), ('carl', 'JJ'), ('linnaeus', 'NN'), ('remains', 'VBZ'), ('use', 'JJ'), ('day', 'NN')], 'dependency_parse': [(('beginnings', 'NNS'), 'nsubj', ('Efforts', 'NNS')), (('Efforts', 'NNS'), 'nmod', ('catalogue', 'NN')), (('catalogue', 'NN'), 'case', ('to', 'IN')), (('Efforts', 'NNS'), 'conj', ('describe', 'VB')), (('describe', 'VB'), 'cc', ('and', 'CC')), (('describe', 'VB'), 'obj', ('collections', 'NNS')), (('collections', 'NNS'), 'nmod:poss', ('their', 'PRP$')), (('beginnings', 'NNS'), 'cop', ('were', 'VBD')), (('beginnings', 'NNS'), 'det', ('the', 'DT')), (('beginnings', 'NNS'), 'nmod', ('taxonomy', 'NN')), (('taxonomy', 'NN'), 'case', ('of', 'IN')), (('taxonomy', 'NN'), 'compound', ('plant', 'NN')), (('beginnings', 'NNS'), 'punct', (',', ',')), (('beginnings', 'NNS'), 'conj', ('led', 'VBD')), (('led', 'VBD'), 'cc', ('and', 'CC')), (('led', 'VBD'), 'obl', ('1753', 'CD')), (('1753', 'CD'), 'case', ('in', 'IN')), (('led', 'VBD'), 'obl', ('system', 'NN')), (('system', 'NN'), 'case', ('to', 'IN')), (('system', 'NN'), 'det', ('the', 'DT')), (('system', 'NN'), 'amod', ('binomial', 'JJ')), (('system', 'NN'), 'nmod', ('Linnaeus', 'NNP')), (('Linnaeus', 'NNP'), 'case', ('of', 'IN')), (('Linnaeus', 'NNP'), 'compound', ('Carl', 'NNP')), (('system', 'NN'), 'acl:relcl', ('remains', 'VBZ')), (('remains', 'VBZ'), 'nsubj', ('that', 'WDT')), (('remains', 'VBZ'), 'obl', ('use', 'NN')), (('use', 'NN'), 'case', ('in', 'IN')), (('use', 'NN'), 'nmod', ('day', 'NN')), (('day', 'NN'), 'case', ('to', 'IN')), (('day', 'NN'), 'det', ('this', 'DT')), (('beginnings', 'NNS'), 'punct', ('.', '.'))], 'synonyms': ['feat', 'catalogue', 'trace', 'solicitation', 'beginning', 'plant', 'taxonomy', 'run', '1753', 'binomial', 'system', 'carl', 'Linnaeus', 'stay', 'use', 'sidereal_day'], 'hypernyms': ['accomplishment', 'compose', 'mark', 'request', 'change_of_state', 'put', 'hierarchy', 'be', 'quantity', 'live_body', 'be', 'sidereal_time'], 'hyponyms': ['derring-do', 'circumscribe', 'whip-round', 'activation', 'come', 'bide', 'address'], 'meronyms': [], 'holonyms': [], 'head_word': ['beginnings'], 'file_name': '222.txt'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nqueType = \"\"\\nhintWord = \"\"\\nif \" what \" in input:\\n    queType=\"what\"\\n    hintWord=\"ORGANIZATION\"\\n    \\nif \" who \" or \" whom \" in input:\\n    queType=\"who\"\\n    hintWord=\"PERSON\"\\n    \\nif \" when \" in input:\\n    queType=\"when\"\\n    hintWord=\"DATE\"\\n\\n\\nlemmas = Lemmatization(input)\\nword_tokens = Tokenization(input)\\nhypernyms, hyponyms, meronyms, holonyms, synonyms = WordNetFeatures(sen, word_tokens)\\n# theQuery =  keywords + \" \" +  heuristics +  \" \" +\\' \\'.join(similarWords) + \" \" + \\' \\'.join(lemma)+  \" \" +\\' \\'.join(stems)\\nquery =  \\' \\'.join(lemmas) + \" \" + \\' \\'.join(hyponyms)\\n\\nquerybody = {\\n        \"query\": {\\n            \"dis_max\": {\\n                \"queries\": [\\n                    # { \"match\": { \"lemma\": {\"query\": spclQuery,\"boost\": 2}  }},\\n                    {\"multi_match\": {\\'query\\': query, \"fields\": [\\n                        # \"lemma^2.0\", \"synonyms^0.5\", \"meronyms^0.1\", \"holonyms^0.1\", \"hypernyms^0.1\", \"hyponyms^0.1\"]}},\\n                        \"lemma^2\", \"synonyms\", \"meronyms^0.5\", \"holonyms^0.5\", \"hypernyms^0.5\", \"hyponyms^0.5\"]}},\\n                ]\\n            }\\n        }\\n    }\\n\\nprint(query)\\nans2 = elastic.search(index=\"articles\", body=querybody)\\nanswers = ans2[\\'hits\\'][\\'hits\\']\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtains features in the question by using the result obtained from the NLP Pipeline\n",
    "def questionFeatures(question):\n",
    "    # Get all the wordnet features\n",
    "    WNfeatures = question[1]['synonyms'] + \n",
    "                 question[1]['meronyms'] + \n",
    "                 question[1]['hyponyms'] + \n",
    "                 question[1]['holonyms'] + \n",
    "                 question[1]['hypernyms']\n",
    "       \n",
    "    # Create hints for easy search using Named Entities and the Sentence head\n",
    "    head = question[1]['head_word'][0]\n",
    "    NEs = question[1]['ner_tag']\n",
    "    NEhints = \"\"\n",
    "    namedEntities = []\n",
    "    \n",
    "    for word, entity in NEs.items():\n",
    "        namedEntities.append(entity)\n",
    "        if entity == 'ORGANIZATION' or entity == 'LOCATION' or entity == 'PERSON':\n",
    "            NEhints += \" \" + word + \" \"\n",
    "            \n",
    "    NEhints += \" \" + head + \" \"\n",
    "    \n",
    "    # Obtain question type and other features\n",
    "    queType = question[1]['type_of_question']\n",
    "    lemmas = question[1]['lemma']\n",
    "    depParse = question[1]['dependency_parse']\n",
    "\n",
    "    depList = list(list(x) for x in depParse)\n",
    "    depElements = []\n",
    "    \n",
    "    for i in depList:\n",
    "        if i[1] == 'nsubj' || i[1] == 'dobj':\n",
    "            depElements.append(i[0])\n",
    "     \n",
    "    # Retrieve main elements from the dependency parse result\n",
    "    dependencyList = list(list(x) for x in depElements)\n",
    "\n",
    "    return NEhints, WNfeatures, queType, lemmas, dependencyList\n",
    "\n",
    "\n",
    "# Check and obtain matched sentences using the query string\n",
    "def GetMatchedSentences(queryStr, dependencyList):\n",
    "    querybody = {\n",
    "        \"query\": {\n",
    "            \"dis_max\": {\n",
    "                \"queries\": [\n",
    "                    # { \"match\": { \"lemma\": {\"query\": spclQuery,\"boost\": 2}  }},\n",
    "                    {\"multi_match\": {'query': queryStr, \"fields\": [\n",
    "                        # \"lemma^2.0\", \"synonyms^0.5\", \"meronyms^0.1\", \"holonyms^0.1\", \"hypernyms^0.1\", \"hyponyms^0.1\"]}},\n",
    "                        \"lemma^2\", \"ner_tag\", \"synonyms\", \"meronyms^0.5\", \"holonyms^0.5\", \"hypernyms^0.5\", \"hyponyms^0.5\"]}},\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    result = elastic.search(index = \"articles\", body=querybody)\n",
    "    answers = result['hits']['hits']\n",
    "    depParses, sentences, scores, articles, NEs = [], [], [], [], []\n",
    "    \n",
    "    for i in range(len(answers)):\n",
    "        sentence = result['hits']['hits'][i]['_source']['sentence']\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        score = result['hits']['hits'][i]['_score']\n",
    "        scores.append(score)\n",
    "        \n",
    "        depParse = result['hits']['hits'][i]['_source']['dependency_parse']\n",
    "        depParses.append(depParse)\n",
    "        \n",
    "        article = result['hits']['hits'][i]['_source']['file_name']\n",
    "        articles.append(article)\n",
    "        \n",
    "        NE = result['hits']['hits'][i]['_source']['ner_tag']\n",
    "        NEs.append(NE)\n",
    "        \n",
    "    return sentences, scores, depParses, articles, NEs\n",
    "\n",
    "\n",
    "question = \"what is ukiyo-e an example of?\"\n",
    "tokens = nltk.word_tokenize(question)\n",
    "\n",
    "count, queFromPipeline = NLP_Pipeline(question, count = 0, corpus_dict, None)\n",
    "NEhints, WNfeatures, queType, lemmas, dependencyList = questionFeatures(queFromPipeline)\n",
    "queryStr = NEhints + \" \" +' '.join(WNfeatures) + \" \" + ' '.join(lemmas)\n",
    "sentences, scores, depParses, articles, NEs = GetMatchedSentences(queryStr, dependencyList)\n",
    "#relevantSentences = computeScore(queType, NEhints, sentences, scores, depParses, articles, NEs, dependencyList)\n",
    "\n",
    "for sent in sentences:\n",
    "    print(sent, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "queType = \"\"\n",
    "hintWord = \"\"\n",
    "if \" what \" in input:\n",
    "    queType=\"what\"\n",
    "    hintWord=\"ORGANIZATION\"\n",
    "    \n",
    "if \" who \" or \" whom \" in input:\n",
    "    queType=\"who\"\n",
    "    hintWord=\"PERSON\"\n",
    "    \n",
    "if \" when \" in input:\n",
    "    queType=\"when\"\n",
    "    hintWord=\"DATE\"\n",
    "\n",
    "\n",
    "lemmas = Lemmatization(input)\n",
    "word_tokens = Tokenization(input)\n",
    "hypernyms, hyponyms, meronyms, holonyms, synonyms = WordNetFeatures(sen, word_tokens)\n",
    "# theQuery =  keywords + \" \" +  heuristics +  \" \" +' '.join(similarWords) + \" \" + ' '.join(lemma)+  \" \" +' '.join(stems)\n",
    "query =  ' '.join(lemmas) + \" \" + ' '.join(hyponyms)\n",
    "\n",
    "querybody = {\n",
    "        \"query\": {\n",
    "            \"dis_max\": {\n",
    "                \"queries\": [\n",
    "                    # { \"match\": { \"lemma\": {\"query\": spclQuery,\"boost\": 2}  }},\n",
    "                    {\"multi_match\": {'query': query, \"fields\": [\n",
    "                        # \"lemma^2.0\", \"synonyms^0.5\", \"meronyms^0.1\", \"holonyms^0.1\", \"hypernyms^0.1\", \"hyponyms^0.1\"]}},\n",
    "                        \"lemma^2\", \"synonyms\", \"meronyms^0.5\", \"holonyms^0.5\", \"hypernyms^0.5\", \"hyponyms^0.5\"]}},\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(query)\n",
    "ans2 = elastic.search(index=\"articles\", body=querybody)\n",
    "answers = ans2['hits']['hits']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0660f188-b531-48f8-8996-eaac343dbefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E, P, D). \n",
      "\n",
      "In the mid-1950s Karl H. Beyer, James M. Sprague, John E. Baer, and Frederick C. Novello of Merck and Co. discovered and developed chlorothiazide, which remains the most widely used antihypertensive drug today. \n",
      "\n",
      "To the west, E. University Boulevard leads to the Fourth Avenue Shopping District. \n",
      "\n",
      "Japanese comics and cartooning (manga),[g] have a history that has been seen as far back as the anthropomorphic characters in the 12th-to-13th-century Chōjū-jinbutsu-giga, 17th-century toba-e and kibyōshi picture books, and woodblock prints such as ukiyo-e which were popular between the 17th and 20th centuries. \n",
      "\n",
      "An experimental study by German cognitive psychologists L. Schwabe and O. Wolf demonstrates how learning under stress also decreases memory recall in humans. \n",
      "\n",
      "Cartoonists began creating comics for mature audiences, and the term \"Ninth Art\"[e] was coined, as comics began to attract public and academic attention as an artform. \n",
      "\n",
      "Helping his father in Visible Speech demonstrations and lectures brought Bell to Susanna E. Hull's private school for the deaf in South Kensington, London. \n",
      "\n",
      "The ukiyo-e artist Hokusai popularized the Japanese term for comics and cartooning, manga, in the early 19th century. \n",
      "\n",
      "The March 1906 Scientific American article by American pioneer William E. Meacham explained the basic principle of hydrofoils and hydroplanes. \n",
      "\n",
      "Switzerland lies between latitudes 45° and 48° N, and longitudes 5° and 11° E. It contains three basic topographical areas: the Swiss Alps to the south, the Swiss Plateau or Central Plateau, and the Jura mountains on the west. \n",
      "\n",
      "[79.64776, 53.256454, 46.504635, 46.504597, 44.866554, 42.89763, 41.519165, 41.36401, 40.615734, 38.939987]\n",
      "['177.txt', '179.txt', '390.txt', '58.txt', '177.txt', '58.txt', '56.txt', '58.txt', '56.txt', '247.txt']\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "scores = []\n",
    "articles = []\n",
    "for i in range(len(answers)):\n",
    "    sent = ans2['hits']['hits'][i]['_source']['sentence']\n",
    "    score = ans2['hits']['hits'][i]['_score']\n",
    "    article = ans2['hits']['hits'][i]['_source']['file_name']\n",
    "    sentences.append(sent)\n",
    "    scores.append(score)\n",
    "    articles.append(article)\n",
    "    \n",
    "for sent in sentences:\n",
    "    print(sent, \"\\n\")\n",
    "    \n",
    "print(scores)\n",
    "print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168acef-74ca-418d-844d-51eb77214bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
