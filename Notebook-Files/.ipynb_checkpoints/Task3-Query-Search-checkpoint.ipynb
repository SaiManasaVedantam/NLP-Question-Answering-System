{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db9886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We perform Task1 and Task2 for the query given. Then, we perform search, rank them and obtain the output.\n",
    "\n",
    "# Turn off unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all the required packages\n",
    "import json\n",
    "import nltk\n",
    "import urllib\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.wsd import lesk\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "\n",
    "\n",
    "# Global initializations\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "dependencyParser = CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "\n",
    "\n",
    "# Build the Question Pipeline for our Question types\n",
    "def QuestionPipeline(question):\n",
    "    corpus_dict = {}\n",
    "    count = 0\n",
    "    tokens = nltk.word_tokenize(question)\n",
    "    \n",
    "    # Identify current question type & lemmas\n",
    "    question_types = [\"what\", \"when\", \"who\", \"whom\", \"What\", \"When\", \"Who\", \"Whom\"]\n",
    "    Qtype = [i for i in question_types if i in tokens]\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Perform POS tagging to extract POS features\n",
    "    POStags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Perform Dependency parsing\n",
    "    parse, = dependencyParser.raw_parse(question)\n",
    "    depParseResult = list(parse.triples())\n",
    "    \n",
    "    # Extract best sense of the word using LESK\n",
    "    best_sense = [lesk(question, word) for word in tokens]\n",
    "    \n",
    "    # Obtain heads\n",
    "    headList = GenerateHeads(question, tokens)\n",
    "    \n",
    "    # Obtain WordNet features\n",
    "    hypernyms, hyponyms, meronyms, holonyms, synonyms = WordNetFeatures(question, tokens)\n",
    "\n",
    "    # Update the corpus dictionary\n",
    "    count = count + 1\n",
    "    corpus_dict[count] = {}\n",
    "    \n",
    "    corpus_dict[count][\"sentence\"] = {}\n",
    "    corpus_dict[count][\"sentence\"] = question\n",
    "    \n",
    "    corpus_dict[count][\"type_of_question\"] = {}\n",
    "    corpus_dict[count][\"type_of_question\"] = Qtype\n",
    "    \n",
    "    corpus_dict[count][\"tokenized_text\"] = {}\n",
    "    corpus_dict[count][\"tokenized_text\"] = tokens\n",
    "    \n",
    "    corpus_dict[count][\"lemma\"] = {}\n",
    "    corpus_dict[count][\"lemma\"] = lemmas\n",
    "    \n",
    "    corpus_dict[count][\"tagged\"] = {}\n",
    "    corpus_dict[count][\"tagged\"] = POStags\n",
    "    \n",
    "    corpus_dict[count][\"dependency_parse\"] = {}\n",
    "    corpus_dict[count][\"dependency_parse\"] = depParseResult\n",
    "    \n",
    "    corpus_dict[count][\"synonyms\"] = {}\n",
    "    corpus_dict[count][\"synonyms\"] = synonyms\n",
    "            \n",
    "    corpus_dict[count][\"hypernyms\"] = {}\n",
    "    corpus_dict[count][\"hypernyms\"] = hypernyms\n",
    "            \n",
    "    corpus_dict[count][\"hyponyms\"] = {}\n",
    "    corpus_dict[count][\"hyponyms\"] = hyponyms\n",
    "            \n",
    "    corpus_dict[count][\"meronyms\"] = {}\n",
    "    corpus_dict[count][\"meronyms\"] = meronyms\n",
    "            \n",
    "    corpus_dict[count][\"holonyms\"] = {}\n",
    "    corpus_dict[count][\"holonyms\"] = holonyms\n",
    "            \n",
    "    corpus_dict[count][\"head_word\"] = {}\n",
    "    corpus_dict[count][\"head_word\"] = headList[0]\n",
    "    \n",
    "    return corpus_dict\n",
    "    \n",
    "    \n",
    "# Obtains sentence heads\n",
    "def GenerateHeads(sentence, word_tokens):\n",
    "    # Set up dependency parser\n",
    "    dependencyParser = CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "    headList = []\n",
    "    \n",
    "    # Split the sentence\n",
    "    stripedSen = sentence.strip(\" '\\\"\")\n",
    "    if stripedSen != \"\":\n",
    "        # Perform dependency parse\n",
    "        depParse = dependencyParser.raw_parse(stripedSen)\n",
    "        parseTree = list(depParse)[0]\n",
    "        headWord = \"\"\n",
    "        headWord = [k[\"word\"] for k in parseTree.nodes.values() if k[\"head\"] == 0][0]\n",
    "        \n",
    "        # Appends head if it's not empty\n",
    "        if headWord != \"\":\n",
    "            headList.append([headWord])\n",
    "            \n",
    "        # Obtain head word based on two cases\n",
    "        else:\n",
    "            for i, pp in enumerate(tagged):\n",
    "                if pp.startswith(\"VB\"):\n",
    "                    headList.append([word_tokens[i]])\n",
    "                    break\n",
    "            if headWord == \"\":\n",
    "                for i, pp in enumerate(tagged):\n",
    "                    if pp.startswith(\"NN\"):\n",
    "                        headList.append([word_tokens[i]])\n",
    "                        break\n",
    "                        \n",
    "    # For empty sentence, we just append \"\" as head\n",
    "    else:\n",
    "        headList.append([\"\"])\n",
    " \n",
    "    return headList\n",
    "\n",
    "\n",
    "# Obtains WordNet Features\n",
    "def WordNetFeatures(sentence, word_tokens):\n",
    "    # Creates dictionaries for important word senses\n",
    "    hypernyms_list = []\n",
    "    hyponyms_list = []\n",
    "    meronyms_list = []\n",
    "    holonyms_list = []\n",
    "    synonyms_list = []\n",
    "    \n",
    "    # Populates the above dictionaries according to the word senses associated with them\n",
    "    for token in word_tokens:\n",
    "        # Extracts best sense for each word using LESK\n",
    "        best_sense = lesk(sentence, token)\n",
    "        \n",
    "        if best_sense is not None:\n",
    "            # Obtains Synonyms\n",
    "            synonym = token\n",
    "            if best_sense. lemmas()[0].name() != token:\n",
    "                synonym = best_sense.lemmas()[0].name()\n",
    "            synonyms_list.append(synonym)\n",
    "            \n",
    "            # Obtains Hypernyms\n",
    "            if best_sense.hypernyms() != []:\n",
    "                hypernyms_list.append(best_sense.hypernyms()[0].lemmas()[0].name())\n",
    "        \n",
    "            # Obtains Hyponyms\n",
    "            if best_sense.hyponyms() != []:\n",
    "                hyponyms_list.append(best_sense.hyponyms()[0].lemmas()[0].name())\n",
    "            \n",
    "            # Obtains Meronyms\n",
    "            if best_sense.part_meronyms() != []:\n",
    "                meronyms_list.append(best_sense.part_meronyms()[0].lemmas()[0].name())\n",
    "                \n",
    "            # Obtains Holonyms\n",
    "            if best_sense.part_holonyms() != []:\n",
    "                holonyms_list.append(best_sense.part_holonyms()[0].lemmas()[0].name())\n",
    "          \n",
    "        # When there's no best sense, the token itself is the Synonym\n",
    "        else:\n",
    "            synonyms_list.append(token)\n",
    "            \n",
    "    return hypernyms_list, hyponyms_list, meronyms_list, holonyms_list, synonyms_list\n",
    "   \n",
    "    \n",
    "# Obtains features of the question\n",
    "def quesFeatures(queFromPipeline):\n",
    "    similarWords, depElements = [], []\n",
    "    ques_type = queFromPipeline[1]['type_of_question']\n",
    "    lemma = queFromPipeline[1]['lemma']\n",
    "    depParse = queFromPipeline[1]['dependency_parse']\n",
    "    dep_list = list(list(x) for x in depParse)\n",
    "\n",
    "    for i in dep_list:\n",
    "        if i[1] == 'nsubj':\n",
    "            depElements.append(i[0])\n",
    "        if i[1] == 'dobj':\n",
    "            depElements.append(i[0])\n",
    "            \n",
    "    dep_list2 = list(list(x) for x in depElements)\n",
    "    \n",
    "    similarWords = queFromPipeline[1]['synonyms'] + \n",
    "                   queFromPipeline[1]['meronyms'] + \n",
    "                   queFromPipeline[1]['hyponyms'] + \n",
    "                   queFromPipeline[1]['hypernyms'] + \n",
    "                   queFromPipeline[1]['holonyms']\n",
    "    \n",
    "    return similarWords, ques_type, lemma, dep_list2\n",
    "\n",
    "\n",
    "# Checks match\n",
    "def query_match(theQuery, dep_list2):\n",
    "    querybody = {\n",
    "        \"query\": {\n",
    "            \"dis_max\": {\n",
    "                \"queries\": [\n",
    "                    # { \"match\": { \"lemma\": {\"query\": spclQuery,\"boost\": 2}  }},\n",
    "                    {\"multi_match\": {'query': theQuery, \"fields\": [\n",
    "                        # \"lemma^2.0\", \"synonyms^0.5\", \"meronyms^0.1\", \"holonyms^0.1\", \"hypernyms^0.1\", \"hyponyms^0.1\"]}},\n",
    "                        \"lemma^2\", \"ner_tag\", \"synonyms\", \"meronyms^0.5\", \"holonyms^0.5\", \"hypernyms^0.5\", \"hyponyms^0.5\"]}},]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "    ans = es.search(index=\"articles\", body=querybody)\n",
    "    answers = ans['hits']['hits']\n",
    "    depParses, sentences, scores, articles = [], [], [], []\n",
    "   \n",
    "    for i in range(len(answers)):\n",
    "        sentence = ans['hits']['hits'][i]['_source']['sentence']\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        score = ans['hits']['hits'][i]['_score']\n",
    "        scores.append(score)\n",
    "        \n",
    "        depParse = ans['hits']['hits'][i]['_source']['dependency_parse']\n",
    "        depParses.append(depParse)\n",
    "        \n",
    "        article = ans['hits']['hits'][i]['_source']['file_name']\n",
    "        articles.append(article)\n",
    "        \n",
    "        # print(\"Sentence: '{}' DepParse: '{}' Score:'{}'\".format(sent, depparse, score))\n",
    "        # print(\"--------------------------------------------\")\n",
    "    return sentences, scores, depParses, articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1976a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
