{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214d2f83",
   "metadata": {},
   "source": [
    "# Closed-Domain Question-Answering System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedd6679",
   "metadata": {},
   "source": [
    "### All required package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965747f9-7be1-48b9-a15a-7567efc5f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all the required packages\n",
    "import csv\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import urllib\n",
    "import os.path\n",
    "from pprint import pprint\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.wsd import lesk\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd664e6",
   "metadata": {},
   "source": [
    "### Global declarations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "397dda37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start common things globally\n",
    "stop_words = stopwords.words('english') + list(string.punctuation)\n",
    "dependencyParser = CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "namedEntityTagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "\n",
    "UNKNOWN_ANSWER = \"****** Our System did not learn the knowledge required to answer this query ******\"\n",
    "UNAVAILABLE_ARTICLE = \"unavailable\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850a3254",
   "metadata": {},
   "source": [
    "### Methods to perform Tokenization, Lemmatization, Stemming and Part-of-Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914f4f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs Word tokenization on sentences\n",
    "def Tokenization(sentence):\n",
    "    tokens = [i for i in nltk.word_tokenize(sentence.lower()) if i not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Performs Word Lemmatization : Uses context\n",
    "def Lemmatization(word_tokens):\n",
    "    lemmas = []\n",
    "    for token in word_tokens:\n",
    "        lemmas.append(wordnet_lemmatizer.lemmatize(token))\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "# Performs Stemming : Uses word stem\n",
    "def Stemming(word_tokens):\n",
    "    stems = [porter.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "# Performs POS tagging\n",
    "def POSTagging(sentence):\n",
    "    word_tokens = [i for i in nltk.word_tokenize(sentence.lower()) if i not in stop_words]\n",
    "    POStags = nltk.pos_tag(word_tokens)\n",
    "    return POStags   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8c08b",
   "metadata": {},
   "source": [
    "### Methods to perform Dependency Parsing and Named Entity identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a764a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs Dependency Parsing\n",
    "def DependencyParsing(sentence):\n",
    "    # Perform dependency parsing\n",
    "    parse, = dependencyParser.raw_parse(sentence)\n",
    "    \n",
    "    # Dependency parsing to parse tree based patterns as features\n",
    "    depParseResult = list(parse.triples())\n",
    "    \n",
    "    return depParseResult\n",
    "\n",
    "\n",
    "# Obtains Named Entities\n",
    "def NamedEntities(sentence, tokens):\n",
    "    # Word tokenize again and use them if NEs are present\n",
    "    namedTokens = nltk.word_tokenize(sentence)\n",
    "    NEtags = None\n",
    "    \n",
    "    try:\n",
    "        NEtags = namedEntityTagger.tag(namedTokens)\n",
    "    except:\n",
    "        NEtags = namedEntityTagger.tag(tokens)\n",
    "        \n",
    "    return NEtags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2101a6",
   "metadata": {},
   "source": [
    "### Method to obtain sentence Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7eb3f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains sentence heads\n",
    "def getHeads(sentence, word_tokens):\n",
    "    # Create a head list to add the heads\n",
    "    headList = []\n",
    "    \n",
    "    # Split the sentence\n",
    "    stripedSen = sentence.strip(\" '\\\"\")\n",
    "    if stripedSen != \"\":\n",
    "        # Perform dependency parse\n",
    "        depParse = dependencyParser.raw_parse(stripedSen)\n",
    "        parseTree = list(depParse)[0]\n",
    "        headWord = \"\"\n",
    "        headWord = [k[\"word\"] for k in parseTree.nodes.values() if k[\"head\"] == 0][0]\n",
    "        \n",
    "        # Appends head if it's not empty\n",
    "        if headWord != \"\":\n",
    "            headList.append([headWord])\n",
    "            \n",
    "        # Obtain head word based on two cases\n",
    "        else:\n",
    "            for i, pp in enumerate(tagged):\n",
    "                if pp.startswith(\"VB\"):\n",
    "                    headList.append([word_tokens[i]])\n",
    "                    break\n",
    "            if headWord == \"\":\n",
    "                for i, pp in enumerate(tagged):\n",
    "                    if pp.startswith(\"NN\"):\n",
    "                        headList.append([word_tokens[i]])\n",
    "                        break\n",
    "                        \n",
    "    # For empty sentence, we just append \"\" as head\n",
    "    else:\n",
    "        headList.append([\"\"])\n",
    " \n",
    "    return headList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ddec6c",
   "metadata": {},
   "source": [
    "### Method to obtain WordNet features like Synonyms, Meronyms, Hypernyms, Hyponyms & Holonyms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17836052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains WordNet Features\n",
    "def WordNetFeatures(sentence, word_tokens):\n",
    "    # Creates dictionaries for important word senses\n",
    "    hypernyms_list = []\n",
    "    hyponyms_list = []\n",
    "    meronyms_list = []\n",
    "    holonyms_list = []\n",
    "    synonyms_list = []\n",
    "    \n",
    "    # Populates the above dictionaries according to the word senses associated with them\n",
    "    for token in word_tokens:\n",
    "        # Extracts best sense for each word using LESK\n",
    "        best_sense = lesk(sentence, token)\n",
    "        \n",
    "        if best_sense is not None:\n",
    "            # Obtains Synonyms\n",
    "            synonym = token\n",
    "            if best_sense. lemmas()[0].name() != token:\n",
    "                synonym = best_sense.lemmas()[0].name()\n",
    "            synonyms_list.append(synonym)\n",
    "            \n",
    "            # Obtains Hypernyms\n",
    "            if best_sense.hypernyms() != []:\n",
    "                hypernyms_list.append(best_sense.hypernyms()[0].lemmas()[0].name())\n",
    "        \n",
    "            # Obtains Hyponyms\n",
    "            if best_sense.hyponyms() != []:\n",
    "                hyponyms_list.append(best_sense.hyponyms()[0].lemmas()[0].name())\n",
    "            \n",
    "            # Obtains Meronyms\n",
    "            if best_sense.part_meronyms() != []:\n",
    "                meronyms_list.append(best_sense.part_meronyms()[0].lemmas()[0].name())\n",
    "                \n",
    "            # Obtains Holonyms\n",
    "            if best_sense.part_holonyms() != []:\n",
    "                holonyms_list.append(best_sense.part_holonyms()[0].lemmas()[0].name())\n",
    "          \n",
    "        # When there's no best sense, the token itself is the Synonym\n",
    "        else:\n",
    "            synonyms_list.append(token)\n",
    "            \n",
    "    return hypernyms_list, hyponyms_list, meronyms_list, holonyms_list, synonyms_list\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae05636",
   "metadata": {},
   "source": [
    "### Method to build NLP Pipeline end-to-end using all the features used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6dbdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP pipeline through which all the articles & question will pass\n",
    "def NLP_Pipeline(sentence, count, data_dict, articleName = None):\n",
    "    #print(\"\\n------SENTENCE------\")\n",
    "    #print(sen)\n",
    "\n",
    "    word_tokens = Tokenization(sentence)\n",
    "    #print(\"\\nWord Tokenization : Done\")\n",
    "    #print(word_tokens)\n",
    "\n",
    "    word_NEtags = NamedEntities(sentence, word_tokens)\n",
    "    #print(\"\\nNamed Entity Tagging : Done\")\n",
    "    #print(word_NEtags)\n",
    "    \n",
    "    word_lemmas = Lemmatization(word_tokens)\n",
    "    #print(\"Word Lemmatization : Done\")\n",
    "    #print(word_lemmas)\n",
    "    \n",
    "    word_stems = Stemming(word_tokens)\n",
    "    #print(\"Word Stemming : Done\")\n",
    "    #print(word_stems)\n",
    "\n",
    "    word_POStags = POSTagging(sentence)\n",
    "    #print(\"POS Tagging : Done\")\n",
    "    #print(word_POStags)\n",
    "\n",
    "    hypernyms, hyponyms, meronyms, holonyms, synonyms = WordNetFeatures(sentence, word_tokens)\n",
    "    #print(\"WordNet Feature Extraction : Done\")\n",
    "    #print(holonyms)\n",
    "            \n",
    "    depParse = DependencyParsing(sentence)\n",
    "    #print(\"Dependency Parsing : Done\")\n",
    "    #pprint(depParse)\n",
    "\n",
    "    headList = getHeads(sentence, word_tokens)\n",
    "    #print(\"Obtaining Heads : Done\")\n",
    "    #print(headList)\n",
    "\n",
    "    # Process data format to suit the Elastic Search requirements\n",
    "    count = count + 1\n",
    "    data_dict[count] = {}\n",
    "            \n",
    "    data_dict[count][\"sentence\"] = {}\n",
    "    data_dict[count][\"sentence\"] = sentence\n",
    "            \n",
    "    data_dict[count][\"tokenized_text\"] = {}\n",
    "    data_dict[count][\"tokenized_text\"] = word_tokens\n",
    "            \n",
    "    data_dict[count][\"lemma\"] = {}\n",
    "    data_dict[count][\"lemma\"] = word_lemmas\n",
    "    \n",
    "    data_dict[count][\"stems\"] = {}\n",
    "    data_dict[count][\"stems\"] = word_stems\n",
    "    \n",
    "    data_dict[count][\"ner_tag\"] = {}\n",
    "    if articleName is not None:\n",
    "        data_dict[count][\"ner_tag\"] = str(dict(word_NEtags))\n",
    "    else:\n",
    "        data_dict[count][\"ner_tag\"] = dict(word_NEtags)\n",
    "            \n",
    "    data_dict[count][\"tags\"] = {}\n",
    "    data_dict[count][\"tags\"] = word_POStags\n",
    "            \n",
    "    data_dict[count][\"dependency_parse\"] = {}\n",
    "    data_dict[count][\"dependency_parse\"] = depParse\n",
    "            \n",
    "    data_dict[count][\"synonyms\"] = {}\n",
    "    data_dict[count][\"synonyms\"] = synonyms\n",
    "            \n",
    "    data_dict[count][\"hypernyms\"] = {}\n",
    "    data_dict[count][\"hypernyms\"] = hypernyms\n",
    "            \n",
    "    data_dict[count][\"hyponyms\"] = {}\n",
    "    data_dict[count][\"hyponyms\"] = hyponyms\n",
    "            \n",
    "    data_dict[count][\"meronyms\"] = {}\n",
    "    data_dict[count][\"meronyms\"] = meronyms\n",
    "            \n",
    "    data_dict[count][\"holonyms\"] = {}\n",
    "    data_dict[count][\"holonyms\"] = holonyms\n",
    "            \n",
    "    data_dict[count][\"head_word\"] = {}\n",
    "    data_dict[count][\"head_word\"] = headList[0]\n",
    "    \n",
    "    # For question, we don't have the article name and then it will have a questionType\n",
    "    if articleName is not None:\n",
    "        data_dict[count][\"file_name\"] = {}\n",
    "        data_dict[count][\"file_name\"] = articleName\n",
    "        \n",
    "        \n",
    "    # For question, we should add the question type\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        questionTypes = [\"who\", \"when\", \"what\", \"whom\", \"whose\"]\n",
    "        queType = [i for i in questionTypes if i in tokens]\n",
    "        data_dict[count][\"question_type\"] = {}\n",
    "        data_dict[count][\"question_type\"] = queType\n",
    "    \n",
    "    return count, data_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170ed57",
   "metadata": {},
   "source": [
    "## TASK 1 - Building an NLP Pipeline end-to-end in training phase by using 30 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd44ea0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Builds NLP Pipeline in the Task 1\n",
    "\n",
    "def task1():\n",
    "    # List of all article names in the repository\n",
    "    articleNames = [\"109.txt\", \"111.txt\", \"151.txt\", \"160.txt\", \"177.txt\", \n",
    "                    \"179.txt\",\"181.txt\", \"196.txt\", \"199.txt\", \"220.txt\", \n",
    "                    \"222.txt\", \"226.txt\", \"247.txt\", \"273.txt\", \"281.txt\", \n",
    "                    \"282.txt\", \"285.txt\", \"287.txt\", \"288.txt\", \"297.txt\", \n",
    "                    \"304.txt\", \"342.txt\", \"347.txt\", \"360.txt\", \"390.txt\", \n",
    "                    \"400.txt\", \"428.txt\", \"56.txt\", \"58.txt\", \"6.txt\"] \n",
    "    fileCount = len(articleNames)\n",
    "    \n",
    "    content = \"\"\n",
    "    urlPath = \"https://raw.githubusercontent.com/SaiManasaVedantam/NLP-QA-System-Datasets/main/Articles/\"\n",
    "\n",
    "    for i in range(fileCount):\n",
    "        print(\"\\nStarted Processing File : \" + articleNames[i])\n",
    "        fileName = urlPath + articleNames[i]\n",
    "        response = urllib.request.urlopen(fileName)\n",
    "        webContents = response.read()\n",
    "        stringTypeData = webContents.decode(\"utf-8\")\n",
    "        content = stringTypeData\n",
    "        count = 0\n",
    "        data_dict = {}\n",
    "\n",
    "        # Get tokenized sentences\n",
    "        sentences = []\n",
    "        sentences.extend(tokenizer.tokenize(content))\n",
    "\n",
    "        # Sentence count\n",
    "        #print(\"Total Sentences After splitting the document: \", len(sentences))\n",
    "        print(\"Extracting features for each sentence in the file...\")\n",
    "\n",
    "        # Extracting words\n",
    "        for sen in sentences:\n",
    "            count, data_dict = NLP_Pipeline(sen, count, data_dict, articleNames[i])\n",
    "\n",
    "        output_name = '../Pipeline-Output/Parsed-' + articleNames[i]\n",
    "        with open(output_name, 'w+', encoding='utf8') as output_file:\n",
    "            json.dump(data_dict, output_file,  indent=4, sort_keys=True, separators=(',', ': '), ensure_ascii=False)\n",
    "\n",
    "        print(\"Completed Processing File : \" + articleNames[i])\n",
    "\n",
    "    print(\"\\nTask 1 Successfully Completed !!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09392432",
   "metadata": {},
   "source": [
    "## TASK 2 - Indexing and obtaining answer for the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ffc5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required packages\n",
    "import ssl\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import RequestsHttpConnection\n",
    "\n",
    "# Setup Elastic Search\n",
    "elastic = Elasticsearch([{'host': 'localhost', 'port': 9200, 'use_ssl' : False, 'ssl_verify' : False}], timeout=30, max_retries=10)\n",
    "\n",
    "# Obtain requests from the page\n",
    "req = requests.get(\"http://localhost:9200\", verify=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fc6c0",
   "metadata": {},
   "source": [
    "### Implements sentence indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1b4a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Indexing using Elastic Search\n",
    "\n",
    "def task2_part1():\n",
    "    # List of all article names in the repository\n",
    "    articleNames = [\"109.txt\", \"111.txt\", \"151.txt\", \"160.txt\", \"177.txt\", \n",
    "                    \"179.txt\",\"181.txt\", \"196.txt\", \"199.txt\", \"220.txt\", \n",
    "                    \"222.txt\", \"226.txt\", \"247.txt\", \"273.txt\", \"281.txt\", \n",
    "                    \"282.txt\", \"285.txt\", \"287.txt\", \"288.txt\", \"297.txt\", \n",
    "                    \"304.txt\", \"342.txt\", \"347.txt\", \"360.txt\", \"390.txt\", \n",
    "                    \"400.txt\", \"428.txt\", \"56.txt\", \"58.txt\", \"6.txt\"] \n",
    "    fileCount = len(articleNames)\n",
    "\n",
    "    # Use indexing\n",
    "    idx = 1\n",
    "\n",
    "    content = \"\"\n",
    "    urlPath = \"https://raw.githubusercontent.com/SaiManasaVedantam/NLP-QA-System-Datasets/main/Pipeline-Output/Parsed-\"\n",
    "\n",
    "    for i in range(fileCount):\n",
    "        print(\"\\nStarted Processing File : \" + articleNames[i])\n",
    "        fileName = urlPath + articleNames[i]\n",
    "        response = urllib.request.urlopen(fileName)\n",
    "        webContents = response.read()\n",
    "        stringTypeData = webContents.decode(\"utf-8\")\n",
    "        content = stringTypeData\n",
    "\n",
    "        # Obtain Json data from file contents\n",
    "        jsonFile = json.loads(content)\n",
    "\n",
    "        # Creating new index \"articles\" for each line in the article\n",
    "        for key, value in jsonFile.items():\n",
    "            elastic.index(index = \"articles\", doc_type = \"text\", id = idx, body = value)\n",
    "            # print(\"Here\")\n",
    "            idx += 1\n",
    "\n",
    "        print(\"Finished Processing File : \" + articleNames[i])\n",
    "\n",
    "    print(\"\\nElastic Search Successfully Completed !!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5073a801",
   "metadata": {},
   "source": [
    "### Uses the indexed content, generate question features from the pipeline & obtain the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0baf545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import dateparser\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c7e92",
   "metadata": {},
   "source": [
    "### Extracts features associated with the query by passing it through the NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9693d18-0157-43fb-bc95-b1550019eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains features in the question by using the result obtained from the NLP Pipeline\n",
    "def questionFeatures(question):\n",
    "    # Get all the wordnet features\n",
    "    WNfeatures = question[1]['synonyms'] + question[1]['meronyms'] + question[1]['hyponyms'] + question[1]['holonyms'] + question[1]['hypernyms']\n",
    "       \n",
    "    # Create hints for easy search using Named Entities and the Sentence head\n",
    "    head = question[1]['head_word'][0]\n",
    "    NEs = question[1]['ner_tag']\n",
    "    NEhints = \"\"\n",
    "    namedEntities = []\n",
    "    \n",
    "    for word, entity in NEs.items():\n",
    "        namedEntities.append(entity)\n",
    "        if entity == 'ORGANIZATION' or entity == 'LOCATION' or entity == 'PERSON':\n",
    "            NEhints += \" \" + word + \" \"\n",
    "        if entity == 'TIME' or entity == 'DATE' or entity == 'NUMBER':\n",
    "            NEhints += \" \" + word + \" \"\n",
    "        \n",
    "    NEhints += \" \" + head + \" \"\n",
    "    \n",
    "    # Obtain question type and other features\n",
    "    queType = question[1]['question_type']\n",
    "    lemmas = question[1]['lemma']\n",
    "    stems = question[1]['stems']\n",
    "    depParse = question[1]['dependency_parse']\n",
    "\n",
    "    depList = list(list(x) for x in depParse)\n",
    "    depElements = []\n",
    "    \n",
    "    for i in depList:\n",
    "        if i[1] == 'nsubj' or i[1] == 'dobj':\n",
    "            depElements.append(i[0])\n",
    "     \n",
    "    # Retrieve main elements from the dependency parse result\n",
    "    dependencyList = list(list(x) for x in depElements)\n",
    "\n",
    "    return NEhints, WNfeatures, queType, lemmas, stems, dependencyList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84141e0e",
   "metadata": {},
   "source": [
    "### Query the indexed content with features & their importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdf70bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and obtain matched sentences using the query string\n",
    "def GetMatchedSentences(queryStr, dependencyList):\n",
    "    # Used Lemmas with 2.2 importance\n",
    "    # Named Entities, Synonyms with 1.9 importance\n",
    "    # Holonyms, Meronyms, POS tags with 0.2 importance\n",
    "    # Hypernyms, Hyponyms with 0.4 importance\n",
    "    # Heads with 1.6 importance\n",
    "    querybody = {\n",
    "        \"query\": {\n",
    "            \"dis_max\": {\n",
    "                \"queries\": [\n",
    "                    # Boost the value of each feature as per the need\n",
    "                    {\"multi_match\": {'query': queryStr, \"fields\": [\n",
    "                        \"lemma^2.2\", \"ner_tag^1.9\", \"synonyms^1.9\", \"holonyms^0.2\", \"meronyms^0.2\", \"hypernyms^0.4\", \"hyponyms^0.4\", \"head_word^1.6\", \"tags^0.2\"]}},\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    result = elastic.search(index = \"articles\", body=querybody)\n",
    "    answers = result['hits']['hits']\n",
    "    depParses, sentences, scores, articles, NEs = [], [], [], [], []\n",
    "    \n",
    "    for i in range(len(answers)):\n",
    "        sentence = result['hits']['hits'][i]['_source']['sentence']\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        score = result['hits']['hits'][i]['_score']\n",
    "        scores.append(score)\n",
    "        \n",
    "        depParse = result['hits']['hits'][i]['_source']['dependency_parse']\n",
    "        depParses.append(depParse)\n",
    "        \n",
    "        article = result['hits']['hits'][i]['_source']['file_name']\n",
    "        articles.append(article)\n",
    "        \n",
    "        NE = result['hits']['hits'][i]['_source']['ner_tag']\n",
    "        NEs.append(NE)\n",
    "        \n",
    "    return sentences, scores, depParses, articles, NEs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af2442",
   "metadata": {},
   "source": [
    "### Find scores associated with each sentence that satisfies the query & implement deeper NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c0e66f9-9ce3-40d3-9395-16e0daab6232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the match score to know how well a statement is matched\n",
    "def FindScore(queType, NEhints, sentences, scores, depParses, articles, NEs, dependencyList):\n",
    "    # Add additional World Knowledge to implement a much deeper NLP pipeline\n",
    "    # Named Entities\n",
    "    \n",
    "    # IMPLEMENTS A DEEPER NLP PIPELINE USING THE ADDITIONAL FEATURES\n",
    "    organizations = ['ORGANIZATION']\n",
    "    persons = ['PERSON']\n",
    "    locations = ['LOCATION', 'PLACE', 'CITY', 'COUNTRY', 'STATE_OR_PROVINCE']\n",
    "    times = ['TIME', 'DATE', 'NUMBER']\n",
    "    # times2 = ['BC', 'AD', 'CENTURY']\n",
    "    \n",
    "    # Feeding world knowledge for a deeper pipeline\n",
    "    keywords = NEhints.split()\n",
    "    keywords = [item.lower() for item in keywords]\n",
    "    \n",
    "    # Obtain relations using Dependency Parse result\n",
    "    count = 0\n",
    "    relations = []\n",
    "    for dep in depParses:\n",
    "        for i in dep:\n",
    "            if i[1] == 'nsubj' or i[1] == 'dboj':\n",
    "                if i[0] in dependencyList:\n",
    "                    relations.append([count,i[0]])\n",
    "        count += 1\n",
    "        \n",
    "    # Get question type\n",
    "    if len(queType) == 0:\n",
    "        return None\n",
    "    \n",
    "    #print(type(queType))\n",
    "    questionType = queType[0].lower()\n",
    "    answers = [] \n",
    "\n",
    "    # Handle different question types\n",
    "    if questionType == 'who' or questionType == 'whom' or questionType == 'whose':\n",
    "        for NE in NEs:\n",
    "            # Obtain all the named entities which are initially stored as a stringified dictionary\n",
    "            NEdict = eval(NE)\n",
    "            ans = []\n",
    "            for key, value in NEdict.items():\n",
    "                if value in persons or organizations:\n",
    "                    ans.append(key)\n",
    "                if (ans != [] and key == ',') or (ans != [] and key == 'and'):\n",
    "                    ans.append(key)\n",
    "                    \n",
    "            answers.append(' '.join(ans))\n",
    "\n",
    "    if questionType == 'when':\n",
    "        for NE in NEs:\n",
    "            # Obtain all the named entities which are initially stored as a stringified dictionary\n",
    "            NEdict = eval(NE)\n",
    "            ans = []\n",
    "            for key, value in NEdict.items():\n",
    "                if value in times and dateparser.parse(key) is not None:\n",
    "                    ans.append(key)\n",
    "\n",
    "            answers.append(' '.join(ans))\n",
    "            \n",
    "    for idx in range(len(answers)):\n",
    "        if len(answers[idx]) < 3:\n",
    "            scores[idx] -= 100\n",
    "\n",
    "    results = zip(sentences, articles, scores)\n",
    "    sortedResults = sorted(results, key = lambda x: x[2])\n",
    "\n",
    "    return reversed(sortedResults)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b31431b",
   "metadata": {},
   "source": [
    "### Method to retrieve data from the Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8952ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains contents from validation set & returns list of questions\n",
    "def getValidationData():\n",
    "    valFile = open(\"Validation-Data.txt\", encoding='UTF-8')\n",
    "    valData = valFile.read()\n",
    "    valData = valData.strip()\n",
    "    valList = valData.split(\"\\n\")\n",
    "    \n",
    "    totalQue = []\n",
    "    totalAns = []\n",
    "    \n",
    "    for articleQueList in valList:\n",
    "        queList = articleQueList.split(\"]]\")\n",
    "        questions = ast.literal_eval(queList[0] + \"]]\")\n",
    "        \n",
    "        for QApair in questions[1]:\n",
    "            question = re.sub('\\?', '', QApair[0])\n",
    "            totalQue.append(question)\n",
    "            answer = QApair[1]\n",
    "            totalAns.append(answer)\n",
    " \n",
    "    return totalQue, totalAns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ee9e2c",
   "metadata": {},
   "source": [
    "### Method to obtain the best possible answer for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9180976e-853a-403d-8ca7-7dcddad23e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains best possible answer for the query\n",
    "def getAnswer(question):\n",
    "    count = 0\n",
    "    data_dict = {}\n",
    "\n",
    "    # Pass the question through NLP pipeline\n",
    "    count, queFromPipeline = NLP_Pipeline(question.lower(), count, data_dict, None)\n",
    "\n",
    "    # Obtain features of the question which already passed through the NLP pipeline\n",
    "    NEhints, WNfeatures, queType, lemmas, stems, dependencyList = questionFeatures(queFromPipeline)\n",
    "\n",
    "    # Form a query string with best possible features for reliable answers\n",
    "    queryStr = NEhints + \" \" +' '.join(WNfeatures) + \" \" + ' '.join(lemmas) +  \" \" +' '.join(stems)\n",
    "\n",
    "    # Run the match query against indexed articles and obtain matched sentences\n",
    "    sentences, scores, depParses, articles, NEs = GetMatchedSentences(queryStr, dependencyList)\n",
    "    #print(articles)\n",
    "\n",
    "    # Obtain only the relevant sentences\n",
    "    relevantSentences = FindScore(queType, NEhints, sentences, scores, depParses, articles, NEs, dependencyList)\n",
    "    if relevantSentences is None:\n",
    "        return UNKNOWN_ANSWER, UNAVAILABLE_ARTICLE\n",
    "    \n",
    "    #print(tuple(relevantSentences))\n",
    "\n",
    "    answer_candidates = []\n",
    "    article_candidates = []\n",
    "\n",
    "    for ans in relevantSentences:\n",
    "        #print(ans)\n",
    "        answer_candidates.append(ans[0])\n",
    "        article_candidates.append(ans[1])\n",
    "\n",
    "    # Result sentence\n",
    "    answer = None if len(answer_candidates) == 0 else answer_candidates[0]\n",
    "    article = None if len(article_candidates) == 0 else article_candidates[0]\n",
    "    \n",
    "    return answer, article\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19579804",
   "metadata": {},
   "source": [
    "### Method to run the NLP pipeline on Validation set & obtain accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "094c3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the pipeline on the validation set and obtains accuracy\n",
    "def validateAndGetAccuracy():\n",
    "    questions, answers = getValidationData()\n",
    "    total = len(questions)\n",
    "    correct = 0\n",
    "    idx = 1\n",
    "  \n",
    "    for que, expectedAns in zip(questions, answers):\n",
    "        #print(\"\\n\", que)\n",
    "            \n",
    "        obtainedAns, obtainedArticle = getAnswer(que)\n",
    "        #print(obtainedAns)\n",
    "        #print(obtainedArticle)\n",
    "\n",
    "        if obtainedAns is None:\n",
    "            continue\n",
    "                \n",
    "        elif expectedAns in obtainedAns:\n",
    "            correct += 1\n",
    "            \n",
    "        # Tracks how many questions are completed & prints status for every 500 questions\n",
    "        if idx % 500 == 0:\n",
    "            print(\"Completed answering\", idx, \"questions in Validation Data\")\n",
    "        idx += 1\n",
    "        \n",
    "    errors = total - correct\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(\"Correct: \", correct, \"\\t Total: \", total, \"\\t Incorrect: \", errors)\n",
    "    print(\"Validation Accuracy: \", round(accuracy, 2), \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f45f9de",
   "metadata": {},
   "source": [
    "### Runs the NLP pipeline on the sample input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b024953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the pipeline on the sample questions with different levels of complexity\n",
    "def runPipelineOnSample(inputFile, outputFile):\n",
    "    questions = readInput(inputFile)\n",
    "    answers = readInput(outputFile)\n",
    "    for question, answer in zip(questions, answers):\n",
    "        obtainedAns, obtainedArticle = getAnswer(question)\n",
    "        print(\"\\nExpected: \", answer)\n",
    "        print(\"\\nObtained: \", obtainedAns)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009053f",
   "metadata": {},
   "source": [
    "## TASK 3 - Read an input file, run the NLP pipeline, obtain answers in csv format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3c9d1",
   "metadata": {},
   "source": [
    "### Methods to read input data & to check if the file specified is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58482b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads content from the input file using fileName & returns questions\n",
    "# It considers the relative path to be in the same location as this ipynb\n",
    "def readInput(fileName):\n",
    "    inputData = open(fileName).read()\n",
    "    inputData = inputData.strip()\n",
    "    questions = inputData.splitlines()\n",
    "   \n",
    "    return questions\n",
    "\n",
    "# Checks if the given file exists in the path\n",
    "def checkFile(fileName):\n",
    "    if os.path.isfile(fileName):\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8645e7c",
   "metadata": {},
   "source": [
    "### Method to process the input file and generate output in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "164da1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces output in the required format & save as .csv\n",
    "def processAndGenerateOutput(questions):\n",
    "    # Saves the output for all questions in a list\n",
    "    headers = [\"Question\", \"Answer's Article-ID\", \"Answer\"]\n",
    "    finalOutput = []\n",
    "    finalOutput.append(headers)\n",
    "    \n",
    "    for que in questions:\n",
    "        obtainedAns, obtainedArticle = getAnswer(que)\n",
    "        \n",
    "        # Stores output for each question in a list\n",
    "        outputData = []\n",
    "        outputData.append(que)\n",
    "        outputData.append(obtainedArticle)\n",
    "        outputData.append(obtainedAns)\n",
    "\n",
    "        # Appends each question's output to the final output list\n",
    "        finalOutput.append(outputData)\n",
    "        \n",
    "        with open('Output.csv', 'w+', encoding='UTF8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(finalOutput)\n",
    "        \n",
    "    print(\"The output CSV file is ready!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e36d4",
   "metadata": {},
   "source": [
    "## TRAINING & TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c05f37",
   "metadata": {},
   "source": [
    "### Use a flag to train the system if it isn't already trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2180bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a boolean flag to check if the System is already trained\n",
    "isTrained = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9044c87",
   "metadata": {},
   "source": [
    "### Method to train the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b00093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSystem():\n",
    "    # Average training time to pass articles through Pipeline : 25 mins\n",
    "    task1()\n",
    "    \n",
    "    # Average training time to index the articles : 7 mins\n",
    "    task2_part1()\n",
    "    \n",
    "    # Sets the flag to True to avoid training as long as the session is active\n",
    "    global isTrained\n",
    "    isTrained = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3484e8c8",
   "metadata": {},
   "source": [
    "### Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b122d63e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started Processing File : 109.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 109.txt\n",
      "\n",
      "Started Processing File : 111.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 111.txt\n",
      "\n",
      "Started Processing File : 151.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 151.txt\n",
      "\n",
      "Started Processing File : 160.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 160.txt\n",
      "\n",
      "Started Processing File : 177.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 177.txt\n",
      "\n",
      "Started Processing File : 179.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 179.txt\n",
      "\n",
      "Started Processing File : 181.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 181.txt\n",
      "\n",
      "Started Processing File : 196.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 196.txt\n",
      "\n",
      "Started Processing File : 199.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 199.txt\n",
      "\n",
      "Started Processing File : 220.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 220.txt\n",
      "\n",
      "Started Processing File : 222.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 222.txt\n",
      "\n",
      "Started Processing File : 226.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 226.txt\n",
      "\n",
      "Started Processing File : 247.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 247.txt\n",
      "\n",
      "Started Processing File : 273.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 273.txt\n",
      "\n",
      "Started Processing File : 281.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 281.txt\n",
      "\n",
      "Started Processing File : 282.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 282.txt\n",
      "\n",
      "Started Processing File : 285.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 285.txt\n",
      "\n",
      "Started Processing File : 287.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 287.txt\n",
      "\n",
      "Started Processing File : 288.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 288.txt\n",
      "\n",
      "Started Processing File : 297.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 297.txt\n",
      "\n",
      "Started Processing File : 304.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 304.txt\n",
      "\n",
      "Started Processing File : 342.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 342.txt\n",
      "\n",
      "Started Processing File : 347.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 347.txt\n",
      "\n",
      "Started Processing File : 360.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 360.txt\n",
      "\n",
      "Started Processing File : 390.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 390.txt\n",
      "\n",
      "Started Processing File : 400.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 400.txt\n",
      "\n",
      "Started Processing File : 428.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 428.txt\n",
      "\n",
      "Started Processing File : 56.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 56.txt\n",
      "\n",
      "Started Processing File : 58.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 58.txt\n",
      "\n",
      "Started Processing File : 6.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 6.txt\n",
      "\n",
      "Task 1 Successfully Completed !!!\n",
      "\n",
      "Started Processing File : 109.txt\n",
      "Finished Processing File : 109.txt\n",
      "\n",
      "Started Processing File : 111.txt\n",
      "Finished Processing File : 111.txt\n",
      "\n",
      "Started Processing File : 151.txt\n",
      "Finished Processing File : 151.txt\n",
      "\n",
      "Started Processing File : 160.txt\n",
      "Finished Processing File : 160.txt\n",
      "\n",
      "Started Processing File : 177.txt\n",
      "Finished Processing File : 177.txt\n",
      "\n",
      "Started Processing File : 179.txt\n",
      "Finished Processing File : 179.txt\n",
      "\n",
      "Started Processing File : 181.txt\n",
      "Finished Processing File : 181.txt\n",
      "\n",
      "Started Processing File : 196.txt\n",
      "Finished Processing File : 196.txt\n",
      "\n",
      "Started Processing File : 199.txt\n",
      "Finished Processing File : 199.txt\n",
      "\n",
      "Started Processing File : 220.txt\n",
      "Finished Processing File : 220.txt\n",
      "\n",
      "Started Processing File : 222.txt\n",
      "Finished Processing File : 222.txt\n",
      "\n",
      "Started Processing File : 226.txt\n",
      "Finished Processing File : 226.txt\n",
      "\n",
      "Started Processing File : 247.txt\n",
      "Finished Processing File : 247.txt\n",
      "\n",
      "Started Processing File : 273.txt\n",
      "Finished Processing File : 273.txt\n",
      "\n",
      "Started Processing File : 281.txt\n",
      "Finished Processing File : 281.txt\n",
      "\n",
      "Started Processing File : 282.txt\n",
      "Finished Processing File : 282.txt\n",
      "\n",
      "Started Processing File : 285.txt\n",
      "Finished Processing File : 285.txt\n",
      "\n",
      "Started Processing File : 287.txt\n",
      "Finished Processing File : 287.txt\n",
      "\n",
      "Started Processing File : 288.txt\n",
      "Finished Processing File : 288.txt\n",
      "\n",
      "Started Processing File : 297.txt\n",
      "Finished Processing File : 297.txt\n",
      "\n",
      "Started Processing File : 304.txt\n",
      "Finished Processing File : 304.txt\n",
      "\n",
      "Started Processing File : 342.txt\n",
      "Finished Processing File : 342.txt\n",
      "\n",
      "Started Processing File : 347.txt\n",
      "Finished Processing File : 347.txt\n",
      "\n",
      "Started Processing File : 360.txt\n",
      "Finished Processing File : 360.txt\n",
      "\n",
      "Started Processing File : 390.txt\n",
      "Finished Processing File : 390.txt\n",
      "\n",
      "Started Processing File : 400.txt\n",
      "Finished Processing File : 400.txt\n",
      "\n",
      "Started Processing File : 428.txt\n",
      "Finished Processing File : 428.txt\n",
      "\n",
      "Started Processing File : 56.txt\n",
      "Finished Processing File : 56.txt\n",
      "\n",
      "Started Processing File : 58.txt\n",
      "Finished Processing File : 58.txt\n",
      "\n",
      "Started Processing File : 6.txt\n",
      "Finished Processing File : 6.txt\n",
      "\n",
      "Elastic Search Successfully Completed !!!\n",
      "\n",
      "Training the Question Answering System is successfully completed !!\n",
      "\n",
      "Enter input file name along with extension (.txt only): Sample-Que.txt\n",
      "\n",
      "Started obtained answers for the questions posed...\n",
      "The output CSV file is ready!!\n",
      "\n",
      "Accuracy on Validation Dataset: \n",
      "Completed answering 500 questions in Validation Data\n",
      "Completed answering 1000 questions in Validation Data\n",
      "Completed answering 1500 questions in Validation Data\n",
      "Completed answering 2000 questions in Validation Data\n",
      "Completed answering 2500 questions in Validation Data\n",
      "Correct:  1260 \t Total:  2505 \t Incorrect:  1245\n",
      "Validation Accuracy:  50.3 %\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    # Train the system once\n",
    "    if isTrained is False:\n",
    "          TrainSystem()\n",
    "        \n",
    "    print(\"\\nTraining the Question Answering System is successfully completed !!\")\n",
    "    \n",
    "    # Filename for testing: Sample-Questions.txt  (8 Correct, 3 Incorrect)\n",
    "    fName = input(\"\\nEnter input file name along with extension (.txt only): \")\n",
    "    exists = True\n",
    "    exists = checkFile(fName)\n",
    "    \n",
    "    if exists == False:\n",
    "        print(\"\\nFile does not exist in the expected path !!\")\n",
    "        print(\"Retry with valid file name !!\")\n",
    "        \n",
    "    else:\n",
    "        questions = readInput(fName)\n",
    "        print(\"\\nStarted obtained answers for the questions posed...\")\n",
    "        processAndGenerateOutput(questions)\n",
    "        \n",
    "        # Average time to run on the Validation dataset : 5 mins\n",
    "        print(\"\\nAccuracy on Validation Dataset: \")\n",
    "        validateAndGetAccuracy()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe07ec2b",
   "metadata": {},
   "source": [
    "## Demonstrating the capability of the System in handling several features & scenarios "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8c158",
   "metadata": {},
   "source": [
    "### Handling questions that doesn't come under the scope of question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a888678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('****** Our System did not learn the knowledge required to answer this query ******', 'unavailable')\n",
      "('****** Our System did not learn the knowledge required to answer this query ******', 'unavailable')\n"
     ]
    }
   ],
   "source": [
    "# No article contains the answer \n",
    "# We don't know the answer to this question because it is beyond the scope of data we fed to system\n",
    "\n",
    "print(getAnswer(\"IIIIII DDDDD KKKKKK MMMMMM LLLLLLLL\"))\n",
    "print(getAnswer(\"Where did Bell spend time in canadian Home?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e17689",
   "metadata": {},
   "source": [
    "### Handling questions that have more confusing question type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e27d8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Another part of long-term memory is episodic memory, \"which attempts to capture information such as \\'what\\', \\'when\\' and \\'where\\'\".', '177.txt')\n"
     ]
    }
   ],
   "source": [
    "print(getAnswer(\"what what why why how how when where\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81be534",
   "metadata": {},
   "source": [
    "### Handling Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17c7d5a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('These advantages offset the high stress, physical exertion costs, and other risks of the migration.', '109.txt')\n"
     ]
    }
   ],
   "source": [
    "# Pros = Advantages, Neutralize = Offset\n",
    "# Expected answer:\n",
    "# These advantages offset the high stress, physical exertion costs, and other risks of the migration, 109.txt\n",
    "\n",
    "print(getAnswer(\"What pros neutralize risk of migration?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3549b",
   "metadata": {},
   "source": [
    "### Handling Hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cc49abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Japanese comics magazine typically run to hundreds of pages.', '58.txt')\n"
     ]
    }
   ],
   "source": [
    "# Magazine is the direct hyponym of Publication\n",
    "# Expected answer:\n",
    "# Japanese comics magazine typically run to hundreds of pages.\n",
    "\n",
    "print(getAnswer(\"What Japanese publication run to many pages?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abfa30a",
   "metadata": {},
   "source": [
    "### Handling Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07f552df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Geneva is the birthplace of the Red Cross and Red Crescent Movement and the Geneva Conventions and, since 2006, hosts the United Nations Human Rights Council.', '247.txt')\n"
     ]
    }
   ],
   "source": [
    "# Provenance is the direct hypernym of Birthplace\n",
    "# Expected answer:\n",
    "# Geneva is the birthplace of the Red Cross and Red Crescent Movement and the Geneva Conventions and, since 2006, \n",
    "# hosts the United Nations Human Rights Council\n",
    "\n",
    "print(getAnswer(\"What is the provenance of Red Cross?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5803de95",
   "metadata": {},
   "source": [
    "### Handling Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ef5c296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Pagan's collapse was followed by 250 years of political fragmentation that lasted well into the 16th century.\", '226.txt')\n"
     ]
    }
   ],
   "source": [
    "# Question types who, whom and whose are centered at who being the root word\n",
    "# Pagan's collapse was followed by 250 years of political fragmentation that lasted well into the 16th century.\n",
    "# Expected answer:\n",
    "\n",
    "print(getAnswer(\"Whose collapse caused political fragmentation?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513d5b16",
   "metadata": {},
   "source": [
    "### Handling Sentence Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "589a989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('It may be blended with the hot bitumen in tanks, but its granular form allows it to be fed in the mixer or in the recycling ring of normal asphalt plants.', '181.txt')\n"
     ]
    }
   ],
   "source": [
    "# Sentence head : Blend\n",
    "# Expected answer:\n",
    "# It may be blended with the hot bitumen in tanks, but its granular form allows it to be fed in the mixer or\n",
    "# in the recycling ring of normal asphalt plants.\n",
    "\n",
    "print(getAnswer(\"What can be blended in tanks?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa0dfd",
   "metadata": {},
   "source": [
    "### Handling Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b561900e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Asphalt/bitumen is typically stored and transported at temperatures around 150 °C (302 °F).', '181.txt')\n"
     ]
    }
   ],
   "source": [
    "# 150 or 302 are NUMBER entities\n",
    "# Expected answer:\n",
    "# Asphalt/bitumen is typically stored and transported at temperatures around 150 °C (302 °F).\n",
    "\n",
    "print(getAnswer(\"At what temperature is Asphalt stored?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805fe8a2",
   "metadata": {},
   "source": [
    "### Handling Meronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45558d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Some children undertook work as apprentices to respectable trades, such as building or as domestic servants (there were over 120,000 domestic servants in London in the mid-18th century).', '273.txt')\n"
     ]
    }
   ],
   "source": [
    "# Plumbing is a part of Construction or Building work\n",
    "# Expected answer:\n",
    "# Some children undertook work as apprentices to respectable trades, such as building or as domestic servants \n",
    "# (there were over 120,000 domestic servants in London in the mid-18th century).\n",
    "\n",
    "print(getAnswer(\"When did children start apprentice work in plumbing or house servants for trades?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192cc5b2",
   "metadata": {},
   "source": [
    "### Handling Holonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "567b4427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('In addition, the feathers of a bird suffer from wear-and-tear and require to be molted.', '109.txt')\n"
     ]
    }
   ],
   "source": [
    "# Bird is the holonym of feather\n",
    "# Expected answer:\n",
    "# In addition, the feathers of a bird suffer from wear-and-tear and require to be molted. \n",
    "\n",
    "print(getAnswer(\"What part of bird need to be molted to reduce suffering?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1609ae1c",
   "metadata": {},
   "source": [
    "### Testing on a sample Easy questions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc2955ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected:  The most obvious function of clothing is to improve the comfort of the wearer, by protecting the wearer from the elements.\n",
      "\n",
      "Obtained:  The most obvious function of clothing is to improve the comfort of the wearer, by protecting the wearer from the elements.\n",
      "\n",
      "Expected:  The current FBI Director is James B. Comey, who was appointed in 2013 by Barack Obama.\n",
      "\n",
      "Obtained:  The current FBI Director is James B. Comey, who was appointed in 2013 by Barack Obama.\n",
      "\n",
      "Expected:  Dyed flax fibers that could have been used in clothing have been found in a prehistoric cave in the Republic of Georgia that date back to 36,000 BP.\n",
      "\n",
      "Obtained:  Dyed flax fibers that could have been used in clothing have been found in a prehistoric cave in the Republic of Georgia that date back to 36,000 BP.\n",
      "\n",
      "Expected:  Non-material culture refers to the non physical ideas that individuals have about their culture, including values, belief system, rules, norms, morals, language, organizations, and institutions.\n",
      "\n",
      "Obtained:  Culture can be any of two types, non-material culture or material culture.\n",
      "\n",
      "Expected:  Alexander Graham Bell (March 3, 1847 – August 2, 1922) was a Scottish-born[N 3] scientist, inventor, engineer and innovator who is credited with patenting the first practical telephone.\n",
      "\n",
      "Obtained:  Some of Bell's kites are on display at the Alexander Graham Bell National Historic Site.\n",
      "\n",
      "Expected:  The hypothesis that plant growth and development is coordinated by plant hormones or plant growth regulators first emerged in the late 19th century.\n",
      "\n",
      "Obtained:  The hypothesis that plant growth and development is coordinated by plant hormones or plant growth regulators first emerged in the late 19th century.\n",
      "\n",
      "Expected:  Common sportswear garments include shorts, T-shirts, tennis shirts, leotards, tracksuits, and trainers.\n",
      "\n",
      "Obtained:  Spandex is also preferable for active sports that require form fitting garments, such as volleyball, wrestling, track & field, dance, gymnastics and swimming.\n"
     ]
    }
   ],
   "source": [
    "runPipelineOnSample(\"Easy-Que.txt\", \"Easy-Ans.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b0272",
   "metadata": {},
   "source": [
    "### Testing on a sample Medium questions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78b9e97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected:  The concept material culture covers the physical expressions of culture, such as technology, architecture and art, whereas the immaterial aspects of culture such as principles of social organization (including, practices of political organization and social institutions), mythology, philosophy, literature (both written and oral), and science make up the intangible cultural heritage of a society.\n",
      "\n",
      "Obtained:  The concept material culture covers the physical expressions of culture, such as technology, architecture and art, whereas the immaterial aspects of culture such as principles of social organization (including, practices of political organization and social institutions), mythology, philosophy, literature (both written and oral), and science make up the intangible cultural heritage of a society.\n",
      "\n",
      "Expected:  In 1207, the Mongol ruler Genghis Khan (r. 1206–1227) conquered and subjugated the ethnic Tangut state of the Western Xia (1038–1227).\n",
      "\n",
      "Obtained:  In 1207, the Mongol ruler Genghis Khan (r. 1206–1227) conquered and subjugated the ethnic Tangut state of the Western Xia (1038–1227).\n",
      "\n",
      "Expected:  Rolpe Dorje, 4th Karmapa Lama (1340–1383) rejected the Hongwu Emperor's invitation, although he did send some disciples as envoys to the court in Nanjing.\n",
      "\n",
      "Obtained:  Rolpe Dorje, 4th Karmapa Lama (1340–1383) rejected the Hongwu Emperor's invitation, although he did send some disciples as envoys to the court in Nanjing.\n",
      "\n",
      "Expected:  A gramophone record (phonograph record in American English) or vinyl record, commonly known as a \"record\", is an analogue sound storage medium in the form of a flat polyvinyl chloride (previously shellac) disc with an inscribed, modulated spiral groove.\n",
      "\n",
      "Obtained:  A gramophone record (phonograph record in American English) or vinyl record, commonly known as a \"record\", is an analogue sound storage medium in the form of a flat polyvinyl chloride (previously shellac) disc with an inscribed, modulated spiral groove.\n",
      "\n",
      "Expected:  Western Electric's innovations resulted in a greatly expanded and more even frequency response, creating a dramatically fuller, clearer and more natural-sounding recording.\n",
      "\n",
      "Obtained:  Western Electric's innovations resulted in a greatly expanded and more even frequency response, creating a dramatically fuller, clearer and more natural-sounding recording.\n",
      "\n",
      "Expected:  Total deaths from diarrhea are estimated at 1.26 million in 2013 – down from 2.58 million in 1990.\n",
      "\n",
      "Obtained:  Total deaths from diarrhea are estimated at 1.26 million in 2013 – down from 2.58 million in 1990.\n",
      "\n",
      "Expected:  Inflammatory diarrhea occurs when there is damage to the mucosal lining or brush border, which leads to a passive loss of protein-rich fluids and a decreased ability to absorb these lost fluids.\n",
      "\n",
      "Obtained:  Inflammatory diarrhea occurs when there is damage to the mucosal lining or brush border, which leads to a passive loss of protein-rich fluids and a decreased ability to absorb these lost fluids.\n"
     ]
    }
   ],
   "source": [
    "runPipelineOnSample(\"Medium-Que.txt\", \"Medium-Ans.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3fcb0",
   "metadata": {},
   "source": [
    "### Testing on a sample Hard questions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bec035b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected:  The Wanli Emperor (r. 1572–1620) made attempts to reestablish Sino-Tibetan relations after the Mongol-Tibetan alliance initiated in 1578, which affected the foreign policy of the subsequent Qing dynasty (1644–1912) of China in their support for the Dalai Lama of the Gelug school.\n",
      "\n",
      "Obtained:  The Wanli Emperor (r. 1572–1620) made attempts to reestablish Sino-Tibetan relations after the Mongol-Tibetan alliance initiated in 1578, which affected the foreign policy of the subsequent Qing dynasty (1644–1912) of China in their support for the Dalai Lama of the Gelug school.\n",
      "\n",
      "Expected:  Hand washing in developing countries, however, is compromised by poverty as acknowledged by the CDC: \"Handwashing is integral to disease prevention in all parts of the world; however, access to soap and water is limited in a number of less developed countries.\n",
      "\n",
      "Obtained:  Hand washing in developing countries, however, is compromised by poverty as acknowledged by the CDC: \"Handwashing is integral to disease prevention in all parts of the world; however, access to soap and water is limited in a number of less developed countries.\n",
      "\n",
      "Expected:  Historically, Iran has been referred to as Persia by the West, due mainly to the writings of Greek historians who called Iran Persis (Greek: Περσίς), meaning \"land of the Persians.\"\n",
      "\n",
      "Obtained:  Today, both Persia and Iran are used in cultural contexts; although, Iran is the name used officially in political contexts.\n",
      "\n",
      "Expected:  Comprising a land area of 1,648,195 km2 (636,372 sq mi), it is the second-largest country in the Middle East and the 18th-largest in the world.\n",
      "\n",
      "Obtained:  Comprising a land area of 1,648,195 km2 (636,372 sq mi), it is the second-largest country in the Middle East and the 18th-largest in the world.\n"
     ]
    }
   ],
   "source": [
    "runPipelineOnSample(\"Hard-Que.txt\", \"Hard-Ans.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53136f9c",
   "metadata": {},
   "source": [
    "### Testing on a sample of mix of questions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fc17c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected:  \"Subsequently, Khomeini accepted a truce mediated by the UN.\"\n",
      "\n",
      "Obtained:  Subsequently, Khomeini accepted a truce mediated by the UN.\n",
      "\n",
      "Expected:  \"The empire collapsed in 330 BC following the conquests of Alexander the Great.\"\n",
      "\n",
      "Obtained:  Alexander's march east put him in confrontation with the Nanda Empire of Magadha and the Gangaridai of Bengal.\n",
      "\n",
      "Expected:  \"The Leader of the Revolution (\"Supreme Leader\") is responsible for delineation and supervision of the general policies of the Islamic Republic of Iran.\"\n",
      "\n",
      "Obtained:  The Leader of the Revolution (\"Supreme Leader\") is responsible for delineation and supervision of the general policies of the Islamic Republic of Iran.\n",
      "\n",
      "Expected:  \"Roughly 150 Tucson companies are involved in the design and manufacture of optics and optoelectronics systems, earning Tucson the nickname Optics Valley.\"\n",
      "\n",
      "Obtained:  Roughly 150 Tucson companies are involved in the design and manufacture of optics and optoelectronics systems, earning Tucson the nickname \"Optics Valley\".\n",
      "\n",
      "Expected:  \"Arizona, south of the Gila River was legally bought from Mexico in the Gadsden Purchase on June 8, 1854.\"\n",
      "\n",
      "Obtained:  Arizona, south of the Gila River was legally bought from Mexico in the Gadsden Purchase on June 8, 1854.\n",
      "\n",
      "Expected:  \"Arizona, south of the Gila River was legally bought from Mexico in the Gadsden Purchase on June 8, 1854.\"\n",
      "\n",
      "Obtained:  Arizona, south of the Gila River was legally bought from Mexico in the Gadsden Purchase on June 8, 1854.\n",
      "\n",
      "Expected:  \"The Fajr-3 (MIRV) is currently Iran's most advanced ballistic missile, it is a liquid fuel missile with an undisclosed range which was developed and produced domestically.\"\n",
      "\n",
      "Obtained:  The Fajr-3 (MIRV) is currently Iran's most advanced ballistic missile, it is a liquid fuel missile with an undisclosed range which was developed and produced domestically.\n",
      "\n",
      "Expected:  \"In 1941, Reza Shah was forced to abdicate in favor of his son, Mohammad Reza Pahlavi, and established the Persian Corridor, a massive supply route that would last until the end of the ongoing war.\"\n",
      "\n",
      "Obtained:  In 1935, Reza Shah requested the international community to refer to the country by its native name, Iran.\n",
      "\n",
      "Expected:  “On November 4, 1979, a group of students seized the United States Embassy and took the embassy with 52 personnel and citizens hostage, after the United States refused to return Mohammad Reza Pahlavi to Iran to face trial in the court of the new regime.”\n",
      "\n",
      "Obtained:  On November 4, 1979, a group of students seized the United States Embassy and took the embassy with 52 personnel and citizens hostage, after the United States refused to return Mohammad Reza Pahlavi to Iran to face trial in the court of the new regime.\n",
      "\n",
      "Expected:  \"The Leader of the Revolution (\"Supreme Leader\") is responsible for delineation and supervision of the general policies of the Islamic Republic of Iran.\"\n",
      "\n",
      "Obtained:  The Assembly elects the Supreme Leader and has the constitutional authority to remove the Supreme Leader from power at any time.\n",
      "\n",
      "Expected:  \"The Fajr-3 (MIRV) is currently Iran's most advanced ballistic missile, it is a liquid fuel missile with an undisclosed range which was developed and produced domestically.\"\n",
      "\n",
      "Obtained:  The Fajr-3 (MIRV) is currently Iran's most advanced ballistic missile, it is a liquid fuel missile with an undisclosed range which was developed and produced domestically.\n",
      "\n",
      "Expected:  \"****** Our System did not learn the knowledge required to answer this query ******\"\n",
      "\n",
      "Obtained:  ****** Our System did not learn the knowledge required to answer this query ******\n"
     ]
    }
   ],
   "source": [
    "runPipelineOnSample(\"Sample-Que.txt\", \"Sample-Ans.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb810e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
