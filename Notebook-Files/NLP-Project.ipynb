{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965747f9-7be1-48b9-a15a-7567efc5f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all the required packages\n",
    "import csv\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import urllib\n",
    "import os.path\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.wsd import lesk\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397dda37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start common things globally\n",
    "stop_words = stopwords.words('english') + list(string.punctuation)\n",
    "dependencyParser = CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "namedEntityTagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "914f4f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs Word tokenization on sentences\n",
    "def Tokenization(sentence):\n",
    "    tokens = [i for i in nltk.word_tokenize(sentence.lower()) if i not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Performs Word Lemmatization : Uses context\n",
    "def Lemmatization(word_tokens):\n",
    "    lemmas = []\n",
    "    for token in word_tokens:\n",
    "        lemmas.append(wordnet_lemmatizer.lemmatize(token))\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "# Performs Stemming : Uses word stem\n",
    "def Stemming(word_tokens):\n",
    "    stems = [porter.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "# Performs POS tagging\n",
    "def POSTagging(sentence):\n",
    "    word_tokens = [i for i in nltk.word_tokenize(sentence.lower()) if i not in stop_words]\n",
    "    POStags = nltk.pos_tag(word_tokens)\n",
    "    return POStags   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a764a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs Dependency Parsing\n",
    "def DependencyParsing(sentence):\n",
    "    # Perform dependency parsing\n",
    "    parse, = dependencyParser.raw_parse(sentence)\n",
    "    \n",
    "    # Dependency parsing to parse tree based patterns as features\n",
    "    depParseResult = list(parse.triples())\n",
    "    \n",
    "    return depParseResult\n",
    "\n",
    "\n",
    "# Obtains Named Entities\n",
    "def NamedEntities(sentence, tokens):\n",
    "    # Word tokenize again and use them if NEs are present\n",
    "    namedTokens = nltk.word_tokenize(sentence)\n",
    "    NEtags = None\n",
    "    \n",
    "    try:\n",
    "        NEtags = namedEntityTagger.tag(namedTokens)\n",
    "    except:\n",
    "        NEtags = namedEntityTagger.tag(tokens)\n",
    "        \n",
    "    return NEtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7eb3f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains sentence heads\n",
    "def getHeads(sentence, word_tokens):\n",
    "    # Create a head list to add the heads\n",
    "    headList = []\n",
    "    \n",
    "    # Split the sentence\n",
    "    stripedSen = sentence.strip(\" '\\\"\")\n",
    "    if stripedSen != \"\":\n",
    "        # Perform dependency parse\n",
    "        depParse = dependencyParser.raw_parse(stripedSen)\n",
    "        parseTree = list(depParse)[0]\n",
    "        headWord = \"\"\n",
    "        headWord = [k[\"word\"] for k in parseTree.nodes.values() if k[\"head\"] == 0][0]\n",
    "        \n",
    "        # Appends head if it's not empty\n",
    "        if headWord != \"\":\n",
    "            headList.append([headWord])\n",
    "            \n",
    "        # Obtain head word based on two cases\n",
    "        else:\n",
    "            for i, pp in enumerate(tagged):\n",
    "                if pp.startswith(\"VB\"):\n",
    "                    headList.append([word_tokens[i]])\n",
    "                    break\n",
    "            if headWord == \"\":\n",
    "                for i, pp in enumerate(tagged):\n",
    "                    if pp.startswith(\"NN\"):\n",
    "                        headList.append([word_tokens[i]])\n",
    "                        break\n",
    "                        \n",
    "    # For empty sentence, we just append \"\" as head\n",
    "    else:\n",
    "        headList.append([\"\"])\n",
    " \n",
    "    return headList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17836052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains WordNet Features\n",
    "def WordNetFeatures(sentence, word_tokens):\n",
    "    # Creates dictionaries for important word senses\n",
    "    hypernyms_list = []\n",
    "    hyponyms_list = []\n",
    "    meronyms_list = []\n",
    "    holonyms_list = []\n",
    "    synonyms_list = []\n",
    "    \n",
    "    # Populates the above dictionaries according to the word senses associated with them\n",
    "    for token in word_tokens:\n",
    "        # Extracts best sense for each word using LESK\n",
    "        best_sense = lesk(sentence, token)\n",
    "        \n",
    "        if best_sense is not None:\n",
    "            # Obtains Synonyms\n",
    "            synonym = token\n",
    "            if best_sense. lemmas()[0].name() != token:\n",
    "                synonym = best_sense.lemmas()[0].name()\n",
    "            synonyms_list.append(synonym)\n",
    "            \n",
    "            # Obtains Hypernyms\n",
    "            if best_sense.hypernyms() != []:\n",
    "                hypernyms_list.append(best_sense.hypernyms()[0].lemmas()[0].name())\n",
    "        \n",
    "            # Obtains Hyponyms\n",
    "            if best_sense.hyponyms() != []:\n",
    "                hyponyms_list.append(best_sense.hyponyms()[0].lemmas()[0].name())\n",
    "            \n",
    "            # Obtains Meronyms\n",
    "            if best_sense.part_meronyms() != []:\n",
    "                meronyms_list.append(best_sense.part_meronyms()[0].lemmas()[0].name())\n",
    "                \n",
    "            # Obtains Holonyms\n",
    "            if best_sense.part_holonyms() != []:\n",
    "                holonyms_list.append(best_sense.part_holonyms()[0].lemmas()[0].name())\n",
    "          \n",
    "        # When there's no best sense, the token itself is the Synonym\n",
    "        else:\n",
    "            synonyms_list.append(token)\n",
    "            \n",
    "    return hypernyms_list, hyponyms_list, meronyms_list, holonyms_list, synonyms_list\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe6dbdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP pipeline through which all the articles & question will pass\n",
    "def NLP_Pipeline(sentence, count, data_dict, articleName = None):\n",
    "    #print(\"\\n------SENTENCE------\")\n",
    "    #print(sen)\n",
    "\n",
    "    word_tokens = Tokenization(sentence)\n",
    "    #print(\"\\nWord Tokenization : Done\")\n",
    "    #print(word_tokens)\n",
    "\n",
    "    word_NEtags = NamedEntities(sentence, word_tokens)\n",
    "    #print(\"\\nNamed Entity Tagging : Done\")\n",
    "    #print(word_NEtags)\n",
    "    \n",
    "    word_lemmas = Lemmatization(word_tokens)\n",
    "    #print(\"Word Lemmatization : Done\")\n",
    "    #print(word_lemmas)\n",
    "    \n",
    "    word_stems = Stemming(word_tokens)\n",
    "    #print(\"Word Stemming : Done\")\n",
    "    #print(word_stems)\n",
    "\n",
    "    word_POStags = POSTagging(sentence)\n",
    "    #print(\"POS Tagging : Done\")\n",
    "    #print(word_POStags)\n",
    "\n",
    "    hypernyms, hyponyms, meronyms, holonyms, synonyms = WordNetFeatures(sentence, word_tokens)\n",
    "    #print(\"WordNet Feature Extraction : Done\")\n",
    "    #print(holonyms)\n",
    "            \n",
    "    depParse = DependencyParsing(sentence)\n",
    "    #print(\"Dependency Parsing : Done\")\n",
    "    #print(depParse)\n",
    "\n",
    "    headList = getHeads(sentence, word_tokens)\n",
    "    #print(\"Obtaining Heads : Done\")\n",
    "    #print(headList)\n",
    "\n",
    "    # Process data format to suit the Elastic Search requirements\n",
    "    count = count + 1\n",
    "    data_dict[count] = {}\n",
    "            \n",
    "    data_dict[count][\"sentence\"] = {}\n",
    "    data_dict[count][\"sentence\"] = sentence\n",
    "            \n",
    "    data_dict[count][\"tokenized_text\"] = {}\n",
    "    data_dict[count][\"tokenized_text\"] = word_tokens\n",
    "            \n",
    "    data_dict[count][\"lemma\"] = {}\n",
    "    data_dict[count][\"lemma\"] = word_lemmas\n",
    "    \n",
    "    data_dict[count][\"stems\"] = {}\n",
    "    data_dict[count][\"stems\"] = word_stems\n",
    "    \n",
    "    data_dict[count][\"ner_tag\"] = {}\n",
    "    if articleName is not None:\n",
    "        data_dict[count][\"ner_tag\"] = str(dict(word_NEtags))\n",
    "    else:\n",
    "        data_dict[count][\"ner_tag\"] = dict(word_NEtags)\n",
    "            \n",
    "    data_dict[count][\"tags\"] = {}\n",
    "    data_dict[count][\"tags\"] = word_POStags\n",
    "            \n",
    "    data_dict[count][\"dependency_parse\"] = {}\n",
    "    data_dict[count][\"dependency_parse\"] = depParse\n",
    "            \n",
    "    data_dict[count][\"synonyms\"] = {}\n",
    "    data_dict[count][\"synonyms\"] = synonyms\n",
    "            \n",
    "    data_dict[count][\"hypernyms\"] = {}\n",
    "    data_dict[count][\"hypernyms\"] = hypernyms\n",
    "            \n",
    "    data_dict[count][\"hyponyms\"] = {}\n",
    "    data_dict[count][\"hyponyms\"] = hyponyms\n",
    "            \n",
    "    data_dict[count][\"meronyms\"] = {}\n",
    "    data_dict[count][\"meronyms\"] = meronyms\n",
    "            \n",
    "    data_dict[count][\"holonyms\"] = {}\n",
    "    data_dict[count][\"holonyms\"] = holonyms\n",
    "            \n",
    "    data_dict[count][\"head_word\"] = {}\n",
    "    data_dict[count][\"head_word\"] = headList[0]\n",
    "    \n",
    "    # For question, we don't have the article name and then it will have a questionType\n",
    "    if articleName is not None:\n",
    "        data_dict[count][\"file_name\"] = {}\n",
    "        data_dict[count][\"file_name\"] = articleName\n",
    "        \n",
    "        \n",
    "    # For question, we should add the question type\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        questionTypes = [\"who\", \"when\", \"what\", \"whom\"]\n",
    "        queType = [i for i in questionTypes if i in tokens]\n",
    "        data_dict[count][\"question_type\"] = {}\n",
    "        data_dict[count][\"question_type\"] = queType\n",
    "    \n",
    "    return count, data_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd44ea0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ------------------------ TASK 1 ------------------------ \"\"\"\n",
    "# Builds NLP Pipeline in the Task 1\n",
    "\n",
    "def task1():\n",
    "    # List of all article names in the repository\n",
    "    articleNames = [\"109.txt\", \"111.txt\", \"151.txt\", \"160.txt\", \"177.txt\", \n",
    "                    \"179.txt\",\"181.txt\", \"196.txt\", \"199.txt\", \"220.txt\", \n",
    "                    \"222.txt\", \"226.txt\", \"247.txt\", \"273.txt\", \"281.txt\", \n",
    "                    \"282.txt\", \"285.txt\", \"287.txt\", \"288.txt\", \"297.txt\", \n",
    "                    \"304.txt\", \"342.txt\", \"347.txt\", \"360.txt\", \"390.txt\", \n",
    "                    \"400.txt\", \"428.txt\", \"56.txt\", \"58.txt\", \"6.txt\"] \n",
    "    fileCount = len(articleNames)\n",
    "    \n",
    "    content = \"\"\n",
    "    urlPath = \"https://raw.githubusercontent.com/SaiManasaVedantam/NLP-QA-System-Datasets/main/Articles/\"\n",
    "\n",
    "    for i in range(fileCount):\n",
    "        print(\"\\nStarted Processing File : \" + articleNames[i])\n",
    "        fileName = urlPath + articleNames[i]\n",
    "        response = urllib.request.urlopen(fileName)\n",
    "        webContents = response.read()\n",
    "        stringTypeData = webContents.decode(\"utf-8\")\n",
    "        content = stringTypeData\n",
    "        count = 0\n",
    "        data_dict = {}\n",
    "\n",
    "        # Get tokenized sentences\n",
    "        sentences = []\n",
    "        sentences.extend(tokenizer.tokenize(content))\n",
    "\n",
    "        # Sentence count\n",
    "        #print(\"Total Sentences After splitting the document: \", len(sentences))\n",
    "        print(\"Extracting features for each sentence in the file...\")\n",
    "\n",
    "        # Extracting words\n",
    "        for sen in sentences:\n",
    "            count, data_dict = NLP_Pipeline(sen, count, data_dict, articleNames[i])\n",
    "\n",
    "        output_name = '../Pipeline-Output/Parsed-' + articleNames[i]\n",
    "        with open(output_name, 'w+', encoding='utf8') as output_file:\n",
    "            json.dump(data_dict, output_file,  indent=4, sort_keys=True, separators=(',', ': '), ensure_ascii=False)\n",
    "\n",
    "        print(\"Completed Processing File : \" + articleNames[i])\n",
    "\n",
    "    print(\"\\nTask 1 Successfully Completed !!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ffc5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ------------------------ TASK 2 - PART 1 ------------------------ \"\"\"\n",
    "\n",
    "# Turn off unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all the required packages\n",
    "import ssl\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import RequestsHttpConnection\n",
    "\n",
    "# Setup Elastic Search\n",
    "elastic = Elasticsearch([{'host': 'localhost', 'port': 9200, 'use_ssl' : False, 'ssl_verify' : False}], timeout=30, max_retries=10)\n",
    "\n",
    "# Obtain requests from the page\n",
    "req = requests.get(\"http://localhost:9200\", verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d1b4a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Indexing using Elastic Search\n",
    "\n",
    "def task2_part1():\n",
    "    # List of all article names in the repository\n",
    "    articleNames = [\"109.txt\", \"111.txt\", \"151.txt\", \"160.txt\", \"177.txt\", \n",
    "                    \"179.txt\",\"181.txt\", \"196.txt\", \"199.txt\", \"220.txt\", \n",
    "                    \"222.txt\", \"226.txt\", \"247.txt\", \"273.txt\", \"281.txt\", \n",
    "                    \"282.txt\", \"285.txt\", \"287.txt\", \"288.txt\", \"297.txt\", \n",
    "                    \"304.txt\", \"342.txt\", \"347.txt\", \"360.txt\", \"390.txt\", \n",
    "                    \"400.txt\", \"428.txt\", \"56.txt\", \"58.txt\", \"6.txt\"] \n",
    "    fileCount = len(articleNames)\n",
    "\n",
    "    # Use indexing\n",
    "    idx = 1\n",
    "\n",
    "    content = \"\"\n",
    "    urlPath = \"https://raw.githubusercontent.com/SaiManasaVedantam/NLP-QA-System-Datasets/main/Pipeline-Output/Parsed-\"\n",
    "\n",
    "    for i in range(fileCount):\n",
    "        print(\"\\nStarted Processing File : \" + articleNames[i])\n",
    "        fileName = urlPath + articleNames[i]\n",
    "        response = urllib.request.urlopen(fileName)\n",
    "        webContents = response.read()\n",
    "        stringTypeData = webContents.decode(\"utf-8\")\n",
    "        content = stringTypeData\n",
    "\n",
    "        # Obtain Json data from file contents\n",
    "        jsonFile = json.loads(content)\n",
    "\n",
    "        # Creating new index \"articles\" for each line in the article\n",
    "        for key, value in jsonFile.items():\n",
    "            elastic.index(index = \"articles\", doc_type = \"text\", id = idx, body = value)\n",
    "            # print(\"Here\")\n",
    "            idx += 1\n",
    "\n",
    "        print(\"Finished Processing File : \" + articleNames[i])\n",
    "\n",
    "    print(\"\\nElastic Search Successfully Completed !!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0baf545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ------------------------ TASK 2 - PART 2 ------------------------ \"\"\"\n",
    "# Import all necessary packages\n",
    "import dateparser\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d9693d18-0157-43fb-bc95-b1550019eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains features in the question by using the result obtained from the NLP Pipeline\n",
    "def questionFeatures(question):\n",
    "    # Get all the wordnet features\n",
    "    WNfeatures = question[1]['synonyms'] + question[1]['meronyms'] + question[1]['hyponyms'] + question[1]['holonyms'] + question[1]['hypernyms']\n",
    "       \n",
    "    # Create hints for easy search using Named Entities and the Sentence head\n",
    "    head = question[1]['head_word'][0]\n",
    "    NEs = question[1]['ner_tag']\n",
    "    NEhints = \"\"\n",
    "    namedEntities = []\n",
    "    \n",
    "    for word, entity in NEs.items():\n",
    "        namedEntities.append(entity)\n",
    "        if entity == 'ORGANIZATION' or entity == 'LOCATION' or entity == 'PERSON':\n",
    "            NEhints += \" \" + word + \" \"\n",
    "        if entity == 'TIME' or entity == 'DATE' or entity == 'NUMBER':\n",
    "            NEhints += \" \" + word + \" \"\n",
    "        \n",
    "    NEhints += \" \" + head + \" \"\n",
    "    \n",
    "    # Obtain question type and other features\n",
    "    queType = question[1]['question_type']\n",
    "    lemmas = question[1]['lemma']\n",
    "    stems = question[1]['stems']\n",
    "    depParse = question[1]['dependency_parse']\n",
    "\n",
    "    depList = list(list(x) for x in depParse)\n",
    "    depElements = []\n",
    "    \n",
    "    for i in depList:\n",
    "        if i[1] == 'nsubj' or i[1] == 'dobj':\n",
    "            depElements.append(i[0])\n",
    "     \n",
    "    # Retrieve main elements from the dependency parse result\n",
    "    dependencyList = list(list(x) for x in depElements)\n",
    "\n",
    "    return NEhints, WNfeatures, queType, lemmas, stems, dependencyList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fdf70bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and obtain matched sentences using the query string\n",
    "def GetMatchedSentences(queryStr, dependencyList):\n",
    "    # Used Lemmas with 2.5 importance\n",
    "    # Named Entities, Synonyms with 1.5 importance\n",
    "    # Holonyms, Meronyms with 0.3 importance\n",
    "    # Hypernyms, Hyponyms, POS tags with 0.5 importance\n",
    "    # Heads with 1.5 importance\n",
    "    querybody = {\n",
    "        \"query\": {\n",
    "            \"dis_max\": {\n",
    "                \"queries\": [\n",
    "                    # Boost the value of each feature as per the need\n",
    "                    {\"multi_match\": {'query': queryStr, \"fields\": [\n",
    "                        #\"lemma^2.2\", \"ner_tag^1.8\", \"synonyms^1.9\", \"holonyms^0.2\", \"meronyms^0.2\", \"hypernyms^0.4\", \"hyponyms^0.4\", \"head_word^1.6\", \"tags^0.2\"]}},\n",
    "                        \"lemma^2.2\", \"ner_tag^1.8\", \"synonyms^1.9\", \"holonyms^0.2\", \"meronyms^0.2\", \"hypernyms^0.4\", \"hyponyms^0.4\", \"head_word^1.6\", \"tags^0.2\"]}},\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    result = elastic.search(index = \"articles\", body=querybody)\n",
    "    answers = result['hits']['hits']\n",
    "    depParses, sentences, scores, articles, NEs = [], [], [], [], []\n",
    "    \n",
    "    for i in range(len(answers)):\n",
    "        sentence = result['hits']['hits'][i]['_source']['sentence']\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        score = result['hits']['hits'][i]['_score']\n",
    "        scores.append(score)\n",
    "        \n",
    "        depParse = result['hits']['hits'][i]['_source']['dependency_parse']\n",
    "        depParses.append(depParse)\n",
    "        \n",
    "        article = result['hits']['hits'][i]['_source']['file_name']\n",
    "        articles.append(article)\n",
    "        \n",
    "        NE = result['hits']['hits'][i]['_source']['ner_tag']\n",
    "        NEs.append(NE)\n",
    "        \n",
    "    return sentences, scores, depParses, articles, NEs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2c0e66f9-9ce3-40d3-9395-16e0daab6232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the match score to know how well a statement is matched\n",
    "def FindScore(queType, NEhints, sentences, scores, depParses, articles, NEs, dependencyList):\n",
    "    # Add additional World Knowledge to implement a much deeper NLP pipeline\n",
    "    # Named Entities\n",
    "    \n",
    "    # IMPLEMENTS A DEEPER NLP PIPELINE USING THE ADDITIONAL FEATURES\n",
    "    organizations = ['ORGANIZATION']\n",
    "    persons = ['PERSON']\n",
    "    locations = ['LOCATION', 'PLACE', 'CITY', 'COUNTRY', 'STATE_OR_PROVINCE']\n",
    "    times = ['TIME', 'DATE', 'NUMBER']\n",
    "    # times2 = ['BC', 'AD', 'CENTURY']\n",
    "    \n",
    "    # Feeding world knowledge for a deeper pipeline\n",
    "    keywords = NEhints.split()\n",
    "    keywords = [item.lower() for item in keywords]\n",
    "    \n",
    "    # Obtain relations using Dependency Parse result\n",
    "    count = 0\n",
    "    relations = []\n",
    "    for dep in depParses:\n",
    "        for i in dep:\n",
    "            if i[1] == 'nsubj' or i[1] == 'dboj':\n",
    "                if i[0] in dependencyList:\n",
    "                    relations.append([count,i[0]])\n",
    "        count += 1\n",
    "        \n",
    "    # Get question type\n",
    "    questionType = queType[0].lower()\n",
    "    answers = [] \n",
    "\n",
    "    # Handle different question types\n",
    "    if questionType == 'who' or questionType == 'whom':\n",
    "        for NE in NEs:\n",
    "            # Obtain all the named entities which are initially stored as a stringified dictionary\n",
    "            NEdict = eval(NE)\n",
    "            ans = []\n",
    "            for key, value in NEdict.items():\n",
    "                if value in persons or organizations:\n",
    "                    ans.append(key)\n",
    "                if (ans != [] and key == ',') or (ans != [] and key == 'and'):\n",
    "                    ans.append(key)\n",
    "                    \n",
    "            answers.append(' '.join(ans))\n",
    "\n",
    "    if questionType == 'when':\n",
    "        for NE in NEs:\n",
    "            # Obtain all the named entities which are initially stored as a stringified dictionary\n",
    "            NEdict = eval(NE)\n",
    "            ans = []\n",
    "            for key, value in NEdict.items():\n",
    "                if value in times and dateparser.parse(key) is not None:\n",
    "                    ans.append(key)\n",
    "\n",
    "            answers.append(' '.join(ans))\n",
    "            \n",
    "    for idx in range(len(answers)):\n",
    "        if len(answers[idx]) < 3:\n",
    "            scores[idx] -= 100\n",
    "\n",
    "    results = zip(sentences, articles, scores)\n",
    "    sortedResults = sorted(results, key = lambda x: x[2])\n",
    "\n",
    "    return reversed(sortedResults)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8952ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains contents from validation set & returns list of questions\n",
    "def getValidationData():\n",
    "    valFile = open(\"Validation-Data.txt\", encoding='UTF-8')\n",
    "    valData = valFile.read()\n",
    "    valData = valData.strip()\n",
    "    valList = valData.split(\"\\n\")\n",
    "    \n",
    "    totalQue = []\n",
    "    totalAns = []\n",
    "    \n",
    "    for articleQueList in valList:\n",
    "        queList = articleQueList.split(\"]]\")\n",
    "        questions = ast.literal_eval(queList[0] + \"]]\")\n",
    "        \n",
    "        for QApair in questions[1]:\n",
    "            question = re.sub('\\?', '', QApair[0])\n",
    "            totalQue.append(question)\n",
    "            answer = QApair[1]\n",
    "            totalAns.append(answer)\n",
    " \n",
    "    return totalQue, totalAns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9180976e-853a-403d-8ca7-7dcddad23e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains best possible answer for the query\n",
    "def getAnswer(question):\n",
    "    count = 0\n",
    "    data_dict = {}\n",
    "\n",
    "    # Pass the question through NLP pipeline\n",
    "    count, queFromPipeline = NLP_Pipeline(question.lower(), count, data_dict, None)\n",
    "\n",
    "    # Obtain features of the question which already passed through the NLP pipeline\n",
    "    NEhints, WNfeatures, queType, lemmas, stems, dependencyList = questionFeatures(queFromPipeline)\n",
    "\n",
    "    # Form a query string with best possible features for reliable answers\n",
    "    queryStr = NEhints + \" \" +' '.join(WNfeatures) + \" \" + ' '.join(lemmas) +  \" \" +' '.join(stems)\n",
    "\n",
    "    # Run the match query against indexed articles and obtain matched sentences\n",
    "    sentences, scores, depParses, articles, NEs = GetMatchedSentences(queryStr, dependencyList)\n",
    "    #print(articles)\n",
    "\n",
    "    # Obtain only the relevant sentences\n",
    "    relevantSentences = FindScore(queType, NEhints, sentences, scores, depParses, articles, NEs, dependencyList)\n",
    "    #print(tuple(relevantSentences))\n",
    "\n",
    "    answer_candidates = []\n",
    "    article_candidates = []\n",
    "\n",
    "    for ans in relevantSentences:\n",
    "        #print(ans)\n",
    "        answer_candidates.append(ans[0])\n",
    "        article_candidates.append(ans[1])\n",
    "\n",
    "    # Result sentence\n",
    "    answer = None if len(answer_candidates) == 0 else answer_candidates[0]\n",
    "    article = None if len(article_candidates) == 0 else article_candidates[0]\n",
    "    \n",
    "    return answer, article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "094c3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the pipeline on the validation set and obtains accuracy\n",
    "def validateAndGetAccuracy():\n",
    "    questions, answers = getValidationData()\n",
    "    total = len(questions)\n",
    "    correct = 0\n",
    "    idx = 1\n",
    "  \n",
    "    for que, expectedAns in zip(questions, answers):\n",
    "        #print(\"\\n\", que)\n",
    "            \n",
    "        obtainedAns, obtainedArticle = getAnswer(que)\n",
    "        #print(obtainedAns)\n",
    "        #print(obtainedArticle)\n",
    "\n",
    "        if obtainedAns is None:\n",
    "            continue\n",
    "                \n",
    "        elif expectedAns in obtainedAns:\n",
    "            correct += 1\n",
    "            \n",
    "        # Tracks how many questions are completed & prints status for every 500 questions\n",
    "        if idx % 500 == 0:\n",
    "            print(\"Completed answering\", idx, \"questions in Validation Data\")\n",
    "        idx += 1\n",
    "        \n",
    "    errors = total - correct\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(\"Correct: \", correct, \"\\t Total: \", total, \"\\t Incorrect: \", errors)\n",
    "    print(\"Validation Accuracy: \", round(accuracy, 2), \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5b024953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the pipeline on the sample questions with different levels of complexity\n",
    "def runPipelineOnSample():\n",
    "    questions = readInput(\"Sample-Questions.txt\")\n",
    "    answers = readInput(\"Sample-Answers.txt\")\n",
    "    for question, answer in zip(questions, answers):\n",
    "        obtainedAns, obtainedArticle = getAnswer(question)\n",
    "        print(\"\\nExpected: \", answer)\n",
    "        print(\"\\nObtained: \", obtainedAns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "58482b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ------------------------ TASK 3 ------------------------ \"\"\"\n",
    "# Reads content from the input file using fileName & returns questions\n",
    "# It considers the relative path to be in the same location as this ipynb\n",
    "def readInput(fileName):\n",
    "    inputData = open(fileName).read()\n",
    "    inputData = inputData.strip()\n",
    "    questions = inputData.splitlines()\n",
    "   \n",
    "    return questions\n",
    "\n",
    "# Checks if the given file exists in the path\n",
    "def checkFile(fileName):\n",
    "    if os.path.isfile(fileName):\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "164da1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces output in the required format & save as .csv\n",
    "def processAndGenerateOutput(questions):\n",
    "    # Saves the output for all questions in a list\n",
    "    headers = [\"Question\", \"Answer's Article-ID\", \"Answer\"]\n",
    "    finalOutput = []\n",
    "    finalOutput.append(headers)\n",
    "    \n",
    "    for que in questions:\n",
    "        obtainedAns, obtainedArticle = getAnswer(que)\n",
    "        \n",
    "        # Stores output for each question in a list\n",
    "        outputData = []\n",
    "        outputData.append(que)\n",
    "        outputData.append(obtainedArticle)\n",
    "        outputData.append(obtainedAns)\n",
    "\n",
    "        # Appends each question's output to the final output list\n",
    "        finalOutput.append(outputData)\n",
    "        \n",
    "        with open('Output.csv', 'w+', encoding='UTF8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(finalOutput)\n",
    "        \n",
    "    print(\"The output CSV file is ready!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2180bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a boolean flag to check if the System is already trained\n",
    "isTrained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSystem():\n",
    "    #task1()\n",
    "    #task2_part1()\n",
    "    global isTrained\n",
    "    isTrained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b122d63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the Question Answering System is successfully completed !!\n",
      "\n",
      "Accuracy on Validation Dataset: \n",
      "Completed answering 500 questions in Validation Data\n",
      "Completed answering 1000 questions in Validation Data\n",
      "Completed answering 1500 questions in Validation Data\n",
      "Completed answering 2000 questions in Validation Data\n",
      "Completed answering 2500 questions in Validation Data\n",
      "Correct:  1259 \t Total:  2505 \t Incorrect:  1246\n",
      "Validation Accuracy:  50.26 %\n",
      "\n",
      "Expected:  \"Subsequently, Khomeini accepted a truce mediated by the UN.\"\n",
      "\n",
      "Obtained:  Subsequently, Khomeini accepted a truce mediated by the UN.\n",
      "\n",
      "Expected:  \"The empire collapsed in 330 BC following the conquests of Alexander the Great.\"\n",
      "\n",
      "Obtained:  Alexander's march east put him in confrontation with the Nanda Empire of Magadha and the Gangaridai of Bengal.\n",
      "\n",
      "Expected:  \"The Leader of the Revolution (\"Supreme Leader\") is responsible for delineation and supervision of the general policies of the Islamic Republic of Iran.\"\n",
      "\n",
      "Obtained:  The Leader of the Revolution (\"Supreme Leader\") is responsible for delineation and supervision of the general policies of the Islamic Republic of Iran.\n",
      "\n",
      "Expected:  \"Roughly 150 Tucson companies are involved in the design and manufacture of optics and optoelectronics systems, earning Tucson the nickname Optics Valley.\"\n",
      "\n",
      "Obtained:  Roughly 150 Tucson companies are involved in the design and manufacture of optics and optoelectronics systems, earning Tucson the nickname \"Optics Valley\".\n",
      "\n",
      "Expected:  \"Arizona, south of the Gila River was legally bought from Mexico in the Gadsden Purchase on June 8, 1854.\"\n",
      "\n",
      "Obtained:  Arizona, south of the Gila River was legally bought from Mexico in the Gadsden Purchase on June 8, 1854.\n",
      "\n",
      "Expected:  \"Arizona, south of the Gila River was legally bought from Mexico in the Gadsden Purchase on June 8, 1854.\"\n",
      "\n",
      "Obtained:  Arizona, south of the Gila River was legally bought from Mexico in the Gadsden Purchase on June 8, 1854.\n",
      "\n",
      "Expected:  \"The Fajr-3 (MIRV) is currently Iran's most advanced ballistic missile, it is a liquid fuel missile with an undisclosed range which was developed and produced domestically.\"\n",
      "\n",
      "Obtained:  The Fajr-3 (MIRV) is currently Iran's most advanced ballistic missile, it is a liquid fuel missile with an undisclosed range which was developed and produced domestically.\n",
      "\n",
      "Expected:  \"In 1941, Reza Shah was forced to abdicate in favor of his son, Mohammad Reza Pahlavi, and established the Persian Corridor, a massive supply route that would last until the end of the ongoing war.\"\n",
      "\n",
      "Obtained:  In 1935, Reza Shah requested the international community to refer to the country by its native name, Iran.\n",
      "\n",
      "Expected:  “On November 4, 1979, a group of students seized the United States Embassy and took the embassy with 52 personnel and citizens hostage, after the United States refused to return Mohammad Reza Pahlavi to Iran to face trial in the court of the new regime.”\n",
      "\n",
      "Obtained:  On November 4, 1979, a group of students seized the United States Embassy and took the embassy with 52 personnel and citizens hostage, after the United States refused to return Mohammad Reza Pahlavi to Iran to face trial in the court of the new regime.\n",
      "\n",
      "Expected:  \"The Leader of the Revolution (\"Supreme Leader\") is responsible for delineation and supervision of the general policies of the Islamic Republic of Iran.\"\n",
      "\n",
      "Obtained:  The Assembly elects the Supreme Leader and has the constitutional authority to remove the Supreme Leader from power at any time.\n",
      "\n",
      "Expected:  \"The Fajr-3 (MIRV) is currently Iran's most advanced ballistic missile, it is a liquid fuel missile with an undisclosed range which was developed and produced domestically.\"\n",
      "\n",
      "Obtained:  The Fajr-3 (MIRV) is currently Iran's most advanced ballistic missile, it is a liquid fuel missile with an undisclosed range which was developed and produced domestically.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    fName = input(\"\\nEnter input file name along with extension (.txt only): \")\\n    exists = True\\n    exists = checkFile(fName)\\n    \\n    if exists == False:\\n        print(\"\\nFile does not exist in the expected path !!\")\\n        print(\"Retry with valid file name !!\")\\n        \\n    else:\\n        questions = readInput(fName)\\n        print(\"\\nStarted obtained answers for the questions posed...\")\\n        processAndGenerateOutput(questions)\\n        \\n        print(\"\\nAccuracy on Validation Dataset: \")\\n        validateAndGetAccuracy()'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    # Train the system once\n",
    "    if isTrained is False:\n",
    "          TrainSystem()\n",
    "        \n",
    "    print(\"\\nTraining the Question Answering System is successfully completed !!\")\n",
    "    \n",
    "    print(\"\\nAccuracy on Validation Dataset: \")\n",
    "    validateAndGetAccuracy()\n",
    "    runPipelineOnSample()\n",
    "        \n",
    "\"\"\"\n",
    "    # Filename for testing: Sample-Questions.txt  (8 Correct, 3 Incorrect)\n",
    "    fName = input(\"\\nEnter input file name along with extension (.txt only): \")\n",
    "    exists = True\n",
    "    exists = checkFile(fName)\n",
    "    \n",
    "    if exists == False:\n",
    "        print(\"\\nFile does not exist in the expected path !!\")\n",
    "        print(\"Retry with valid file name !!\")\n",
    "        \n",
    "    else:\n",
    "        questions = readInput(fName)\n",
    "        print(\"\\nStarted obtained answers for the questions posed...\")\n",
    "        processAndGenerateOutput(questions)\n",
    "        \n",
    "        print(\"\\nAccuracy on Validation Dataset: \")\n",
    "        validateAndGetAccuracy()\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
