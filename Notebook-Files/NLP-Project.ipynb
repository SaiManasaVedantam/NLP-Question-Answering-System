{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "965747f9-7be1-48b9-a15a-7567efc5f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all the required packages\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import urllib\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.wsd import lesk\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import xlwt\n",
    "from xlwt import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "397dda37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start common things globally\n",
    "stop_words = stopwords.words('english') + list(string.punctuation)\n",
    "dependencyParser = CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "namedEntityTagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "914f4f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs Word tokenization on sentences\n",
    "def Tokenization(sentence):\n",
    "    tokens = [i for i in nltk.word_tokenize(sentence.lower()) if i not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Performs Word Lemmatization : Uses context\n",
    "def Lemmatization(word_tokens):\n",
    "    lemmas = []\n",
    "    for token in word_tokens:\n",
    "        lemmas.append(wordnet_lemmatizer.lemmatize(token))\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "# Performs Stemming : Uses word stem\n",
    "def Stemming(word_tokens):\n",
    "    stems = [porter.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "# Performs POS tagging\n",
    "def POSTagging(sentence):\n",
    "    word_tokens = [i for i in nltk.word_tokenize(sentence.lower()) if i not in stop_words]\n",
    "    POStags = nltk.pos_tag(word_tokens)\n",
    "    return POStags   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a764a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs Dependency Parsing\n",
    "def DependencyParsing(sentence):\n",
    "    # Perform dependency parsing\n",
    "    parse, = dependencyParser.raw_parse(sentence)\n",
    "    \n",
    "    # Dependency parsing to parse tree based patterns as features\n",
    "    depParseResult = list(parse.triples())\n",
    "    \n",
    "    return depParseResult\n",
    "\n",
    "\n",
    "# Obtains Named Entities\n",
    "def NamedEntities(sentence, tokens):\n",
    "    # Word tokenize again and use them if NEs are present\n",
    "    namedTokens = nltk.word_tokenize(sentence)\n",
    "    NEtags = None\n",
    "    \n",
    "    try:\n",
    "        NEtags = namedEntityTagger.tag(namedTokens)\n",
    "    except:\n",
    "        NEtags = namedEntityTagger.tag(tokens)\n",
    "        \n",
    "    return NEtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7eb3f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains sentence heads\n",
    "def getHeads(sentence, word_tokens):\n",
    "    # Create a head list to add the heads\n",
    "    headList = []\n",
    "    \n",
    "    # Split the sentence\n",
    "    stripedSen = sentence.strip(\" '\\\"\")\n",
    "    if stripedSen != \"\":\n",
    "        # Perform dependency parse\n",
    "        depParse = dependencyParser.raw_parse(stripedSen)\n",
    "        parseTree = list(depParse)[0]\n",
    "        headWord = \"\"\n",
    "        headWord = [k[\"word\"] for k in parseTree.nodes.values() if k[\"head\"] == 0][0]\n",
    "        \n",
    "        # Appends head if it's not empty\n",
    "        if headWord != \"\":\n",
    "            headList.append([headWord])\n",
    "            \n",
    "        # Obtain head word based on two cases\n",
    "        else:\n",
    "            for i, pp in enumerate(tagged):\n",
    "                if pp.startswith(\"VB\"):\n",
    "                    headList.append([word_tokens[i]])\n",
    "                    break\n",
    "            if headWord == \"\":\n",
    "                for i, pp in enumerate(tagged):\n",
    "                    if pp.startswith(\"NN\"):\n",
    "                        headList.append([word_tokens[i]])\n",
    "                        break\n",
    "                        \n",
    "    # For empty sentence, we just append \"\" as head\n",
    "    else:\n",
    "        headList.append([\"\"])\n",
    " \n",
    "    return headList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17836052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains WordNet Features\n",
    "def WordNetFeatures(sentence, word_tokens):\n",
    "    # Creates dictionaries for important word senses\n",
    "    hypernyms_list = []\n",
    "    hyponyms_list = []\n",
    "    meronyms_list = []\n",
    "    holonyms_list = []\n",
    "    synonyms_list = []\n",
    "    \n",
    "    # Populates the above dictionaries according to the word senses associated with them\n",
    "    for token in word_tokens:\n",
    "        # Extracts best sense for each word using LESK\n",
    "        best_sense = lesk(sentence, token)\n",
    "        \n",
    "        if best_sense is not None:\n",
    "            # Obtains Synonyms\n",
    "            synonym = token\n",
    "            if best_sense. lemmas()[0].name() != token:\n",
    "                synonym = best_sense.lemmas()[0].name()\n",
    "            synonyms_list.append(synonym)\n",
    "            \n",
    "            # Obtains Hypernyms\n",
    "            if best_sense.hypernyms() != []:\n",
    "                hypernyms_list.append(best_sense.hypernyms()[0].lemmas()[0].name())\n",
    "        \n",
    "            # Obtains Hyponyms\n",
    "            if best_sense.hyponyms() != []:\n",
    "                hyponyms_list.append(best_sense.hyponyms()[0].lemmas()[0].name())\n",
    "            \n",
    "            # Obtains Meronyms\n",
    "            if best_sense.part_meronyms() != []:\n",
    "                meronyms_list.append(best_sense.part_meronyms()[0].lemmas()[0].name())\n",
    "                \n",
    "            # Obtains Holonyms\n",
    "            if best_sense.part_holonyms() != []:\n",
    "                holonyms_list.append(best_sense.part_holonyms()[0].lemmas()[0].name())\n",
    "          \n",
    "        # When there's no best sense, the token itself is the Synonym\n",
    "        else:\n",
    "            synonyms_list.append(token)\n",
    "            \n",
    "    return hypernyms_list, hyponyms_list, meronyms_list, holonyms_list, synonyms_list\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe6dbdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP pipeline through which all the articles & question will pass\n",
    "def NLP_Pipeline(sentence, count, data_dict, articleName = None):\n",
    "    #print(\"\\n------SENTENCE------\")\n",
    "    #print(sen)\n",
    "\n",
    "    word_tokens = Tokenization(sentence)\n",
    "    #print(\"\\nWord Tokenization : Done\")\n",
    "    #print(word_tokens)\n",
    "\n",
    "    word_NEtags = NamedEntities(sentence, word_tokens)\n",
    "    #print(\"\\nNamed Entity Tagging : Done\")\n",
    "    #print(word_NEtags)\n",
    "    \n",
    "    word_lemmas = Lemmatization(word_tokens)\n",
    "    #print(\"Word Lemmatization : Done\")\n",
    "    #print(word_lemmas)\n",
    "    \n",
    "    word_stems = Stemming(word_tokens)\n",
    "    #print(\"Word Stemming : Done\")\n",
    "    #print(word_stems)\n",
    "\n",
    "    word_POStags = POSTagging(sentence)\n",
    "    #print(\"POS Tagging : Done\")\n",
    "    #print(word_POStags)\n",
    "\n",
    "    hypernyms, hyponyms, meronyms, holonyms, synonyms = WordNetFeatures(sentence, word_tokens)\n",
    "    #print(\"WordNet Feature Extraction : Done\")\n",
    "    #print(holonyms)\n",
    "            \n",
    "    depParse = DependencyParsing(sentence)\n",
    "    #print(\"Dependency Parsing : Done\")\n",
    "    #print(depParse)\n",
    "\n",
    "    headList = getHeads(sentence, word_tokens)\n",
    "    #print(\"Obtaining Heads : Done\")\n",
    "    #print(headList)\n",
    "\n",
    "    # Process data format to suit the Elastic Search requirements\n",
    "    count = count + 1\n",
    "    data_dict[count] = {}\n",
    "            \n",
    "    data_dict[count][\"sentence\"] = {}\n",
    "    data_dict[count][\"sentence\"] = sentence\n",
    "            \n",
    "    data_dict[count][\"tokenized_text\"] = {}\n",
    "    data_dict[count][\"tokenized_text\"] = word_tokens\n",
    "            \n",
    "    data_dict[count][\"lemma\"] = {}\n",
    "    data_dict[count][\"lemma\"] = word_lemmas\n",
    "    \n",
    "    data_dict[count][\"stems\"] = {}\n",
    "    data_dict[count][\"stems\"] = word_stems\n",
    "    \n",
    "    data_dict[count][\"ner_tag\"] = {}\n",
    "    if articleName is not None:\n",
    "        data_dict[count][\"ner_tag\"] = str(dict(word_NEtags))\n",
    "    else:\n",
    "        data_dict[count][\"ner_tag\"] = dict(word_NEtags)\n",
    "            \n",
    "    data_dict[count][\"tags\"] = {}\n",
    "    data_dict[count][\"tags\"] = word_POStags\n",
    "            \n",
    "    data_dict[count][\"dependency_parse\"] = {}\n",
    "    data_dict[count][\"dependency_parse\"] = depParse\n",
    "            \n",
    "    data_dict[count][\"synonyms\"] = {}\n",
    "    data_dict[count][\"synonyms\"] = synonyms\n",
    "            \n",
    "    data_dict[count][\"hypernyms\"] = {}\n",
    "    data_dict[count][\"hypernyms\"] = hypernyms\n",
    "            \n",
    "    data_dict[count][\"hyponyms\"] = {}\n",
    "    data_dict[count][\"hyponyms\"] = hyponyms\n",
    "            \n",
    "    data_dict[count][\"meronyms\"] = {}\n",
    "    data_dict[count][\"meronyms\"] = meronyms\n",
    "            \n",
    "    data_dict[count][\"holonyms\"] = {}\n",
    "    data_dict[count][\"holonyms\"] = holonyms\n",
    "            \n",
    "    data_dict[count][\"head_word\"] = {}\n",
    "    data_dict[count][\"head_word\"] = headList[0]\n",
    "    \n",
    "    # For question, we don't have the article name and then it will have a questionType\n",
    "    if articleName is not None:\n",
    "        data_dict[count][\"file_name\"] = {}\n",
    "        data_dict[count][\"file_name\"] = articleName\n",
    "        \n",
    "        \n",
    "    # For question, we should add the question type\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        questionTypes = [\"who\", \"when\", \"what\", \"whom\"]\n",
    "        queType = [i for i in questionTypes if i in tokens]\n",
    "        data_dict[count][\"question_type\"] = {}\n",
    "        data_dict[count][\"question_type\"] = queType\n",
    "    \n",
    "    return count, data_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44ea0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ------------------------ TASK 1 ------------------------ \"\"\"\n",
    "\n",
    "# List of all article names in the repository\n",
    "articleNames = [\"109.txt\", \"111.txt\", \"151.txt\", \"160.txt\", \"177.txt\", \n",
    "                \"179.txt\",\"181.txt\", \"196.txt\", \"199.txt\", \"220.txt\", \n",
    "                \"222.txt\", \"226.txt\", \"247.txt\", \"273.txt\", \"281.txt\", \n",
    "                \"282.txt\", \"285.txt\", \"287.txt\", \"288.txt\", \"297.txt\", \n",
    "                \"304.txt\", \"342.txt\", \"347.txt\", \"360.txt\", \"390.txt\", \n",
    "                \"400.txt\", \"428.txt\", \"56.txt\", \"58.txt\", \"6.txt\"] \n",
    "fileCount = len(articleNames)\n",
    "    \n",
    "content = \"\"\n",
    "urlPath = \"https://raw.githubusercontent.com/SaiManasaVedantam/NLP-QA-System-Datasets/main/Articles/\"\n",
    "\n",
    "for i in range(fileCount):\n",
    "    print(\"\\nStarted Processing File : \" + articleNames[i])\n",
    "    fileName = urlPath + articleNames[i]\n",
    "    response = urllib.request.urlopen(fileName)\n",
    "    webContents = response.read()\n",
    "    stringTypeData = webContents.decode(\"utf-8\")\n",
    "    content = stringTypeData\n",
    "    count = 0\n",
    "    data_dict = {}\n",
    "\n",
    "    # Get tokenized sentences\n",
    "    sentences = []\n",
    "    sentences.extend(tokenizer.tokenize(content))\n",
    "\n",
    "    # Sentence count\n",
    "    #print(\"Total Sentences After splitting the document: \", len(sentences))\n",
    "    print(\"Extracting features for each sentence in the file...\")\n",
    "    \n",
    "    # Extracting words\n",
    "    for sen in sentences:\n",
    "        count, data_dict = NLP_Pipeline(sen, count, data_dict, articleNames[i])\n",
    "                \n",
    "    output_name = '../Pipeline-Output/Parsed-' + articleNames[i]\n",
    "    with open(output_name, 'w+', encoding='utf8') as output_file:\n",
    "        json.dump(data_dict, output_file,  indent=4, sort_keys=True, separators=(',', ': '), ensure_ascii=False)\n",
    "        \n",
    "    print(\"Completed Processing File : \" + articleNames[i])\n",
    "        \n",
    "print(\"\\nTask 1 Successfully Completed !!!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ffc5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ------------------------ TASK 2 - PART 1 ------------------------ \"\"\"\n",
    "\n",
    "# Turn off unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all the required packages\n",
    "import ssl\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import RequestsHttpConnection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1b4a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started Processing File : 109.txt\n",
      "Finished Processing File : 109.txt\n",
      "\n",
      "Started Processing File : 111.txt\n",
      "Finished Processing File : 111.txt\n",
      "\n",
      "Started Processing File : 151.txt\n",
      "Finished Processing File : 151.txt\n",
      "\n",
      "Started Processing File : 160.txt\n",
      "Finished Processing File : 160.txt\n",
      "\n",
      "Started Processing File : 177.txt\n",
      "Finished Processing File : 177.txt\n",
      "\n",
      "Started Processing File : 179.txt\n",
      "Finished Processing File : 179.txt\n",
      "\n",
      "Started Processing File : 181.txt\n",
      "Finished Processing File : 181.txt\n",
      "\n",
      "Started Processing File : 196.txt\n",
      "Finished Processing File : 196.txt\n",
      "\n",
      "Started Processing File : 199.txt\n",
      "Finished Processing File : 199.txt\n",
      "\n",
      "Started Processing File : 220.txt\n",
      "Finished Processing File : 220.txt\n",
      "\n",
      "Started Processing File : 222.txt\n",
      "Finished Processing File : 222.txt\n",
      "\n",
      "Started Processing File : 226.txt\n",
      "Finished Processing File : 226.txt\n",
      "\n",
      "Started Processing File : 247.txt\n",
      "Finished Processing File : 247.txt\n",
      "\n",
      "Started Processing File : 273.txt\n",
      "Finished Processing File : 273.txt\n",
      "\n",
      "Started Processing File : 281.txt\n",
      "Finished Processing File : 281.txt\n",
      "\n",
      "Started Processing File : 282.txt\n",
      "Finished Processing File : 282.txt\n",
      "\n",
      "Started Processing File : 285.txt\n",
      "Finished Processing File : 285.txt\n",
      "\n",
      "Started Processing File : 287.txt\n",
      "Finished Processing File : 287.txt\n",
      "\n",
      "Started Processing File : 288.txt\n",
      "Finished Processing File : 288.txt\n",
      "\n",
      "Started Processing File : 297.txt\n",
      "Finished Processing File : 297.txt\n",
      "\n",
      "Started Processing File : 304.txt\n",
      "Finished Processing File : 304.txt\n",
      "\n",
      "Started Processing File : 342.txt\n",
      "Finished Processing File : 342.txt\n",
      "\n",
      "Started Processing File : 347.txt\n",
      "Finished Processing File : 347.txt\n",
      "\n",
      "Started Processing File : 360.txt\n",
      "Finished Processing File : 360.txt\n",
      "\n",
      "Started Processing File : 390.txt\n",
      "Finished Processing File : 390.txt\n",
      "\n",
      "Started Processing File : 400.txt\n",
      "Finished Processing File : 400.txt\n",
      "\n",
      "Started Processing File : 428.txt\n",
      "Finished Processing File : 428.txt\n",
      "\n",
      "Started Processing File : 56.txt\n",
      "Finished Processing File : 56.txt\n",
      "\n",
      "Started Processing File : 58.txt\n",
      "Finished Processing File : 58.txt\n",
      "\n",
      "Started Processing File : 6.txt\n",
      "Finished Processing File : 6.txt\n",
      "\n",
      "Elastic Search Successfully Completed !!!\n"
     ]
    }
   ],
   "source": [
    "# List of all article names in the repository\n",
    "articleNames = [\"109.txt\", \"111.txt\", \"151.txt\", \"160.txt\", \"177.txt\", \n",
    "                \"179.txt\",\"181.txt\", \"196.txt\", \"199.txt\", \"220.txt\", \n",
    "                \"222.txt\", \"226.txt\", \"247.txt\", \"273.txt\", \"281.txt\", \n",
    "                \"282.txt\", \"285.txt\", \"287.txt\", \"288.txt\", \"297.txt\", \n",
    "                \"304.txt\", \"342.txt\", \"347.txt\", \"360.txt\", \"390.txt\", \n",
    "                \"400.txt\", \"428.txt\", \"56.txt\", \"58.txt\", \"6.txt\"] \n",
    "fileCount = len(articleNames)\n",
    "    \n",
    "# Setup Elastic Search\n",
    "elastic = Elasticsearch([{'host': 'localhost', 'port': 9200, 'use_ssl' : False, 'ssl_verify' : False}], timeout=30, max_retries=10)\n",
    "    \n",
    "# Obtain requests from the page\n",
    "req = requests.get(\"http://localhost:9200\", verify=False)\n",
    "    \n",
    "# Use indexing\n",
    "idx = 1\n",
    "    \n",
    "content = \"\"\n",
    "urlPath = \"https://raw.githubusercontent.com/SaiManasaVedantam/NLP-QA-System-Datasets/main/Pipeline-Output/Parsed-\"\n",
    "    \n",
    "for i in range(fileCount):\n",
    "    print(\"\\nStarted Processing File : \" + articleNames[i])\n",
    "    fileName = urlPath + articleNames[i]\n",
    "    response = urllib.request.urlopen(fileName)\n",
    "    webContents = response.read()\n",
    "    stringTypeData = webContents.decode(\"utf-8\")\n",
    "    content = stringTypeData\n",
    "        \n",
    "    # Obtain Json data from file contents\n",
    "    jsonFile = json.loads(content)\n",
    "        \n",
    "    # Creating new index \"articles\" for each line in the article\n",
    "    for key, value in jsonFile.items():\n",
    "        elastic.index(index = \"articles\", doc_type = \"text\", id = idx, body = value)\n",
    "        # print(\"Here\")\n",
    "        idx += 1\n",
    "            \n",
    "    print(\"Finished Processing File : \" + articleNames[i])\n",
    "        \n",
    "print(\"\\nElastic Search Successfully Completed !!!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0baf545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ------------------------ TASK 2 - PART 2 ------------------------ \"\"\"\n",
    "# Import all necessary packages\n",
    "import dateparser\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9693d18-0157-43fb-bc95-b1550019eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains features in the question by using the result obtained from the NLP Pipeline\n",
    "def questionFeatures(question):\n",
    "    # Get all the wordnet features\n",
    "    WNfeatures = question[1]['synonyms'] + question[1]['meronyms'] + question[1]['hyponyms'] + question[1]['holonyms'] + question[1]['hypernyms']\n",
    "       \n",
    "    # Create hints for easy search using Named Entities and the Sentence head\n",
    "    head = question[1]['head_word'][0]\n",
    "    NEs = question[1]['ner_tag']\n",
    "    NEhints = \"\"\n",
    "    namedEntities = []\n",
    "    \n",
    "    for word, entity in NEs.items():\n",
    "        namedEntities.append(entity)\n",
    "        if entity == 'ORGANIZATION' or entity == 'LOCATION' or entity == 'PERSON':\n",
    "            NEhints += \" \" + word + \" \"\n",
    "            \n",
    "    NEhints += \" \" + head + \" \"\n",
    "    \n",
    "    # Obtain question type and other features\n",
    "    queType = question[1]['question_type']\n",
    "    lemmas = question[1]['lemma']\n",
    "    stems = question[1]['stems']\n",
    "    depParse = question[1]['dependency_parse']\n",
    "\n",
    "    depList = list(list(x) for x in depParse)\n",
    "    depElements = []\n",
    "    \n",
    "    for i in depList:\n",
    "        if i[1] == 'nsubj' or i[1] == 'dobj':\n",
    "            depElements.append(i[0])\n",
    "     \n",
    "    # Retrieve main elements from the dependency parse result\n",
    "    dependencyList = list(list(x) for x in depElements)\n",
    "\n",
    "    return NEhints, WNfeatures, queType, lemmas, stems, dependencyList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf70bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and obtain matched sentences using the query string\n",
    "def GetMatchedSentences(queryStr, dependencyList):\n",
    "    querybody = {\n",
    "        \"query\": {\n",
    "            \"dis_max\": {\n",
    "                \"queries\": [\n",
    "                    # { \"match\": { \"lemma\": {\"query\": spclQuery,\"boost\": 2}  }},\n",
    "                    {\"multi_match\": {'query': queryStr, \"fields\": [\n",
    "                        # \"lemma^2.0\", \"synonyms^0.5\", \"meronyms^0.1\", \"holonyms^0.1\", \"hypernyms^0.1\", \"hyponyms^0.1\"]}},\n",
    "                        \"lemma^2\", \"ner_tag\", \"synonyms^0.7\", \"meronyms^0.1\", \"holonyms^0.1\", \"hypernyms^0.5\", \"hyponyms^0.2\"]}},\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    result = elastic.search(index = \"articles\", body=querybody)\n",
    "    answers = result['hits']['hits']\n",
    "    depParses, sentences, scores, articles, NEs = [], [], [], [], []\n",
    "    \n",
    "    for i in range(len(answers)):\n",
    "        sentence = result['hits']['hits'][i]['_source']['sentence']\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        score = result['hits']['hits'][i]['_score']\n",
    "        scores.append(score)\n",
    "        \n",
    "        depParse = result['hits']['hits'][i]['_source']['dependency_parse']\n",
    "        depParses.append(depParse)\n",
    "        \n",
    "        article = result['hits']['hits'][i]['_source']['file_name']\n",
    "        articles.append(article)\n",
    "        \n",
    "        NE = result['hits']['hits'][i]['_source']['ner_tag']\n",
    "        NEs.append(NE)\n",
    "        \n",
    "    return sentences, scores, depParses, articles, NEs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c0e66f9-9ce3-40d3-9395-16e0daab6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Build a deeeper NLP pipeline for better results\\ndef deeperPipeline(NEhints, WNfeatures, question, queType):\\n    helpers = \"\"\\n    \\n    # We can obtain some domain knowledge for person, organization, location\\n    # For some random organization\\n    if \\'Apple\\' in keywords:\\n        helpers += \" Apple inc. computer apple Apple Inc. \"\\n    if \\'die\\' in keywords:\\n        helpers += \\' assassination \\'\\n    if \\'born\\' in keywords:\\n        helpers += \\' life \\'\\n    \\n    month = \" january jan february feb march mar april apr may june jun july jul august aug september sep october oct november nov december dec \"\\n    year = \" 1969 1970 1971 1980 1981 1982 1983 1984 1975 1976 1977 1978 1979 2001 2002 2003 2004 2005 2006 2007 2010 2012 2013 2014 2015\"\\n    # where locations\\n    if ques_type[0].lower() == \\'who\\':\\n        if \\'found\\' in question:\\n        print(\"Who question\")\\n        \\n    if ques_type[0].lower() == \\'when\\':\\n        print(\"When question\")\\n        \\n    if ques_type[0].lower() == \\'where\\':\\n        if \\'birth\\' in question:\\n            heuristics += \\' born \\'\\n        heuristics += \\' locate \\'\\n        print(\"Where question\")\\n\\n    return heuristics\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the match score to know how well a statement is matched\n",
    "def FindScore(queType, NEhints, sentences, scores, depParses, articles, NEs, dependencyList):\n",
    "    # Add additional World Knowledge to implement a much deeper NLP pipeline\n",
    "    # Named Entities\n",
    "    organizations = ['ORGANIZATION']\n",
    "    persons = ['PERSON']\n",
    "    locations = ['LOCATION', 'PLACE', 'CITY', 'COUNTRY', 'STATE_OR_PROVINCE']\n",
    "    times = ['TIME', 'DATE', 'NUMBER']\n",
    "    \n",
    "    # Feeding world knowledge for a deeper pipeline\n",
    "    deaths = ['die', 'died', 'assassination']\n",
    "    births = ['born', 'birth', 'life']\n",
    "    keywords = NEhints.split()\n",
    "    keywords = [item.lower() for item in keywords]\n",
    "    \n",
    "    # Obtain relations using Dependency Parse result\n",
    "    count = 0\n",
    "    relations = []\n",
    "    for dep in depParses:\n",
    "        for i in dep:\n",
    "            if i[1] == 'nsubj' or i[1] == 'dboj':\n",
    "                if i[0] in dependencyList:\n",
    "                    relations.append([count,i[0]])\n",
    "        count += 1\n",
    "        \n",
    "    # Get question type\n",
    "    questionType = queType[0].lower()\n",
    "    answers = [] \n",
    "    \n",
    "    # Set for relation\n",
    "    \"\"\"print(\"\\n\", relations)\n",
    "    for reln in relations:\n",
    "        idx = relations[0]\n",
    "        print(idx)\n",
    "        scores[idx] += 100\"\"\"\n",
    "\n",
    "    # Handle different question types\n",
    "    if questionType == 'who' or questionType == 'whom':\n",
    "        for NE in NEs:\n",
    "            # Obtain all the named entities which are initially stored as a stringified dictionary\n",
    "            NEdict = eval(NE)\n",
    "            ans = []\n",
    "            for key, value in NEdict.items():\n",
    "                if value in persons or organizations:\n",
    "                    ans.append(key)\n",
    "                if (ans != [] and key == ',') or (ans != [] and key == 'and'):\n",
    "                    ans.append(key)\n",
    "                    \n",
    "            answers.append(' '.join(ans))\n",
    "\n",
    "    if questionType == 'when':\n",
    "        for NE in NEs:\n",
    "            # Obtain all the named entities which are initially stored as a stringified dictionary\n",
    "            NEdict = eval(NE)\n",
    "            ans = []\n",
    "            for key, value in NEdict.items():\n",
    "                if value in times and dateparser.parse(key) is not None:\n",
    "                    ans.append(key)\n",
    "\n",
    "            answers.append(' '.join(ans))\n",
    "\n",
    "    \"\"\"if questionType == 'what':\n",
    "        for NE in NEs:\n",
    "            # Obtain all the named entities which are initially stored as a stringified dictionary\n",
    "            NEdict = eval(NE)\n",
    "            ans = []\n",
    "            for key, value in NEdict.items():\n",
    "                if value in locations or value in organizations:\n",
    "                    ans.append(key)\n",
    "                if (ans != [] and key == ',') or (ans != [] and key == 'and'):\n",
    "                    ans.append(key)\n",
    "                    \n",
    "            answers.append(' '.join(ans))\"\"\"\n",
    "\n",
    "    \n",
    "    for idx in range(len(answers)):\n",
    "        if len(answers[idx]) < 3:\n",
    "            scores[idx] -= 100\n",
    "    \n",
    "    \"\"\"# Level 2 handling for When questions as it can also be about births & deaths\n",
    "    dieconcept = 0       \n",
    "    if questionType == 'when':\n",
    "        for key in range(len(sentences)):\n",
    "            for j in deaths:\n",
    "                if j in sentences[key]:\n",
    "                    pattern = r\"\\((.*?)\\)\"\n",
    "                    try:\n",
    "                        matched = re.findall(pattern, sentences[key])\n",
    "                        splits = matched[0].split(' ')\n",
    "                        splitjoin = ' '.join(splits[4:])\n",
    "                        answers[key] = splitjoin\n",
    "                        dieconcept = 1\n",
    "                    except:\n",
    "                        pass\n",
    "                    scores[key] += 50\n",
    "                    \n",
    "        if dieconcept == 0:\n",
    "            for key in range(0,len(sentences)):\n",
    "                for j in births:\n",
    "                    if j in sentences[key]:\n",
    "                        pattern = r\"\\((.*?)\\)\"\n",
    "                        try:\n",
    "                            matched = re.findall(pattern, sentences[key])\n",
    "                            splits = matched[0].split(' ')\n",
    "                            splitjoin = ' '.join(splits)\n",
    "                            answers[key] = splitjoin\n",
    "                            dieconcept = 1\n",
    "                        except:\n",
    "                            pass\n",
    "                        scores[key] += 10\"\"\"\n",
    "\n",
    "    results = zip(sentences, articles, scores)\n",
    "    sortedResults = sorted(results, key = lambda x: x[2])\n",
    "\n",
    "    return reversed(sortedResults)\n",
    "\n",
    "\n",
    "\"\"\"# Build a deeeper NLP pipeline for better results\n",
    "def deeperPipeline(NEhints, WNfeatures, question, queType):\n",
    "    helpers = \"\"\n",
    "    \n",
    "    # We can obtain some domain knowledge for person, organization, location\n",
    "    # For some random organization\n",
    "    if 'Apple' in keywords:\n",
    "        helpers += \" Apple inc. computer apple Apple Inc. \"\n",
    "    if 'die' in keywords:\n",
    "        helpers += ' assassination '\n",
    "    if 'born' in keywords:\n",
    "        helpers += ' life '\n",
    "    \n",
    "    month = \" january jan february feb march mar april apr may june jun july jul august aug september sep october oct november nov december dec \"\n",
    "    year = \" 1969 1970 1971 1980 1981 1982 1983 1984 1975 1976 1977 1978 1979 2001 2002 2003 2004 2005 2006 2007 2010 2012 2013 2014 2015\"\n",
    "    # where locations\n",
    "    if ques_type[0].lower() == 'who':\n",
    "        if 'found' in question:\n",
    "        print(\"Who question\")\n",
    "        \n",
    "    if ques_type[0].lower() == 'when':\n",
    "        print(\"When question\")\n",
    "        \n",
    "    if ques_type[0].lower() == 'where':\n",
    "        if 'birth' in question:\n",
    "            heuristics += ' born '\n",
    "        heuristics += ' locate '\n",
    "        print(\"Where question\")\n",
    "\n",
    "    return heuristics\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8952ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains contents from validation set & returns list of questions\n",
    "def getValidationData():\n",
    "    valFile = open(\"Validation-Data.txt\", encoding='UTF-8')\n",
    "    valData = valFile.read()\n",
    "    valData = valData.strip()\n",
    "    valList = valData.split(\"\\n\")\n",
    "    \n",
    "    totalQue = []\n",
    "    totalAns = []\n",
    "    \n",
    "    for articleQueList in valList:\n",
    "        queList = articleQueList.split(\"]]\")\n",
    "        questions = ast.literal_eval(queList[0] + \"]]\")\n",
    "        \n",
    "        for QApair in questions[1]:\n",
    "            question = re.sub('\\?', '', QApair[0])\n",
    "            totalQue.append(question)\n",
    "            answer = QApair[1]\n",
    "            totalAns.append(answer)\n",
    " \n",
    "    return totalQue, totalAns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9180976e-853a-403d-8ca7-7dcddad23e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains best possible answer for the query\n",
    "def getAnswer(question):\n",
    "    count = 0\n",
    "    data_dict = {}\n",
    "\n",
    "    # Pass the question through NLP pipeline\n",
    "    count, queFromPipeline = NLP_Pipeline(question.lower(), count, data_dict, None)\n",
    "\n",
    "    # Obtain features of the question which already passed through the NLP pipeline\n",
    "    NEhints, WNfeatures, queType, lemmas, stems, dependencyList = questionFeatures(queFromPipeline)\n",
    "\n",
    "    # Form a query string with best possible features for reliable answers\n",
    "    queryStr = NEhints + \" \" +' '.join(WNfeatures) + \" \" + ' '.join(lemmas) +  \" \" +' '.join(stems)\n",
    "\n",
    "    # Run the match query against indexed articles and obtain matched sentences\n",
    "    sentences, scores, depParses, articles, NEs = GetMatchedSentences(queryStr, dependencyList)\n",
    "    #print(articles)\n",
    "\n",
    "    # Obtain only the relevant sentences\n",
    "    relevantSentences = FindScore(queType, NEhints, sentences, scores, depParses, articles, NEs, dependencyList)\n",
    "    #print(tuple(relevantSentences))\n",
    "\n",
    "    # answer = relevantSentences[0][0]\n",
    "    # article = relevantSentences[0][1]\n",
    "    answer_candidates = []\n",
    "    article_candidates = []\n",
    "\n",
    "    for ans in relevantSentences:\n",
    "        answer_candidates.append(ans[0])\n",
    "        article_candidates.append(ans[1])\n",
    "\n",
    "    # Result sentence\n",
    "    # candidates = [ans[0] for ans in relevantSentences]\n",
    "    # article_candidates = [ans[1] for ans in relevantSentences]\n",
    "    answer = None if len(answer_candidates) == 0 else answer_candidates[0]\n",
    "    article = None if len(article_candidates) == 0 else article_candidates[0]\n",
    "    #print(answer)\n",
    "    \n",
    "    return answer, article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "094c3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the pipeline on the validation set and obtains accuracy\n",
    "def validateAndGetAccuracy():\n",
    "    questions, answers = getValidationData()\n",
    "    total = len(questions)\n",
    "    correct = 0\n",
    "    idx = 1\n",
    "\n",
    "    wb = Workbook()\n",
    "    sheet1 = wb.add_sheet('Sheet 1')\n",
    "\n",
    "    sheet1.write(0, 0, 'index')\n",
    "    sheet1.write(0, 1, 'question')\n",
    "    sheet1.write(0, 2, 'answer_sentence')\n",
    "    sheet1.write(0, 3, 'document_id')\n",
    "\n",
    "    \n",
    "    for que, expectedAns in zip(questions, answers):\n",
    "        #print(\"\\n\", que)\n",
    "            \n",
    "        obtainedAns, obtainedArticle = getAnswer(que)\n",
    "        #print(obtainedAns)\n",
    "        #print(obtainedArticle)\n",
    "        #print(expectedAns)\n",
    "            \n",
    "        if obtainedAns is None:\n",
    "            continue\n",
    "                \n",
    "        elif expectedAns in obtainedAns:\n",
    "            correct += 1\n",
    "            \n",
    "        sheet1.write(idx, 0, str(idx))\n",
    "        sheet1.write(idx, 1, que)\n",
    "        sheet1.write(idx, 2, obtainedAns)\n",
    "        sheet1.write(idx, 3, obtainedArticle)\n",
    "        # Tracks how many questions are completed & prints status for every 250 questions\n",
    "        if idx % 250 == 0:\n",
    "            print(\"Completed answering\", idx, \"questions in Validation Data\")\n",
    "        idx += 1\n",
    "\n",
    "    wb.save('output.xls')\n",
    "    errors = total - correct\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(\"Correct: \", correct, \"\\t Total: \", total, \"\\t Incorrect: \", errors)\n",
    "    print(\"Validation Accuracy: \", round(accuracy, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c6154a74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed answering 250 questions in Validation Data\n",
      "Completed answering 500 questions in Validation Data\n",
      "Completed answering 750 questions in Validation Data\n",
      "Completed answering 1000 questions in Validation Data\n",
      "Completed answering 1250 questions in Validation Data\n",
      "Completed answering 1500 questions in Validation Data\n",
      "Completed answering 1750 questions in Validation Data\n",
      "Completed answering 2000 questions in Validation Data\n",
      "Completed answering 2250 questions in Validation Data\n",
      "Completed answering 2500 questions in Validation Data\n",
      "Correct:  1222 \t Total:  2505 \t Incorrect:  1283\n",
      "Validation Accuracy:  48.78 %\n"
     ]
    }
   ],
   "source": [
    "validateAndGetAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "getAnswer(\"What is the Leader of the Revolution also known as in Iran?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58482b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ------------------------ TASK 3 ------------------------ \"\"\"\n",
    "# Reads content from the input file using fileName & returns questions\n",
    "# It considers the relative path to be in the same location as this ipynb\n",
    "def readInput(fileName):\n",
    "    fName = fileName + \".txt\"\n",
    "    \n",
    "    inputData = open(fName).read()\n",
    "    inputData = inputData.strip()\n",
    "    questions = inputData.splitlines()\n",
    "   \n",
    "    return questions\n",
    "\n",
    "\n",
    "# Produces output in the required format & save as .csv\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
