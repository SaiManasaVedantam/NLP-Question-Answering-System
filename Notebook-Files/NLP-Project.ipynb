{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965747f9-7be1-48b9-a15a-7567efc5f1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started Processing File : 109.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 109.txt\n",
      "\n",
      "Started Processing File : 111.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 111.txt\n",
      "\n",
      "Started Processing File : 151.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 151.txt\n",
      "\n",
      "Started Processing File : 160.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 160.txt\n",
      "\n",
      "Started Processing File : 177.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 177.txt\n",
      "\n",
      "Started Processing File : 179.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 179.txt\n",
      "\n",
      "Started Processing File : 181.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 181.txt\n",
      "\n",
      "Started Processing File : 196.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 196.txt\n",
      "\n",
      "Started Processing File : 199.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 199.txt\n",
      "\n",
      "Started Processing File : 220.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 220.txt\n",
      "\n",
      "Started Processing File : 222.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 222.txt\n",
      "\n",
      "Started Processing File : 226.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 226.txt\n",
      "\n",
      "Started Processing File : 247.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 247.txt\n",
      "\n",
      "Started Processing File : 273.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 273.txt\n",
      "\n",
      "Started Processing File : 281.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 281.txt\n",
      "\n",
      "Started Processing File : 282.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 282.txt\n",
      "\n",
      "Started Processing File : 285.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 285.txt\n",
      "\n",
      "Started Processing File : 287.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 287.txt\n",
      "\n",
      "Started Processing File : 288.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 288.txt\n",
      "\n",
      "Started Processing File : 297.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 297.txt\n",
      "\n",
      "Started Processing File : 304.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 304.txt\n",
      "\n",
      "Started Processing File : 342.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 342.txt\n",
      "\n",
      "Started Processing File : 347.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 347.txt\n",
      "\n",
      "Started Processing File : 360.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 360.txt\n",
      "\n",
      "Started Processing File : 390.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 390.txt\n",
      "\n",
      "Started Processing File : 400.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 400.txt\n",
      "\n",
      "Started Processing File : 428.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 428.txt\n",
      "\n",
      "Started Processing File : 56.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 56.txt\n",
      "\n",
      "Started Processing File : 58.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 58.txt\n",
      "\n",
      "Started Processing File : 6.txt\n",
      "Extracting features for each sentence in the file...\n",
      "Completed Processing File : 6.txt\n",
      "\n",
      "Task 1 Successfully Completed !!!\n"
     ]
    }
   ],
   "source": [
    "# Turn off unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all the required packages\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import urllib\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.wsd import lesk\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "\n",
    "# Start common things globally\n",
    "stop_words = stopwords.words('english') + list(string.punctuation)\n",
    "dependencyParser = CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "namedEntityTagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
    "\n",
    "# Performs Word tokenization on sentences\n",
    "def Tokenization(sentence):\n",
    "    tokens = [i for i in nltk.word_tokenize(sentence.lower()) if i not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Performs Word Lemmatization\n",
    "def Lemmatization(word_tokens):\n",
    "    lemmas = []\n",
    "    for token in word_tokens:\n",
    "        lemmas.append(wordnet_lemmatizer.lemmatize(token))\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "# Performs POS tagging\n",
    "def POSTagging(word_tokens):\n",
    "    POStags = nltk.pos_tag(word_tokens)\n",
    "    return POStags   \n",
    "\n",
    "\n",
    "# Obtains sentence heads\n",
    "def getHeads(sentence, word_tokens):\n",
    "    # Create a head list to add the heads\n",
    "    headList = []\n",
    "    \n",
    "    # Split the sentence\n",
    "    stripedSen = sentence.strip(\" '\\\"\")\n",
    "    if stripedSen != \"\":\n",
    "        # Perform dependency parse\n",
    "        depParse = dependencyParser.raw_parse(stripedSen)\n",
    "        parseTree = list(depParse)[0]\n",
    "        headWord = \"\"\n",
    "        headWord = [k[\"word\"] for k in parseTree.nodes.values() if k[\"head\"] == 0][0]\n",
    "        \n",
    "        # Appends head if it's not empty\n",
    "        if headWord != \"\":\n",
    "            headList.append([headWord])\n",
    "            \n",
    "        # Obtain head word based on two cases\n",
    "        else:\n",
    "            for i, pp in enumerate(tagged):\n",
    "                if pp.startswith(\"VB\"):\n",
    "                    headList.append([word_tokens[i]])\n",
    "                    break\n",
    "            if headWord == \"\":\n",
    "                for i, pp in enumerate(tagged):\n",
    "                    if pp.startswith(\"NN\"):\n",
    "                        headList.append([word_tokens[i]])\n",
    "                        break\n",
    "                        \n",
    "    # For empty sentence, we just append \"\" as head\n",
    "    else:\n",
    "        headList.append([\"\"])\n",
    " \n",
    "    return headList\n",
    "\n",
    "\n",
    "# Obtains WordNet Features\n",
    "def WordNetFeatures(sentence, word_tokens):\n",
    "    # Creates dictionaries for important word senses\n",
    "    hypernyms_list = []\n",
    "    hyponyms_list = []\n",
    "    meronyms_list = []\n",
    "    holonyms_list = []\n",
    "    synonyms_list = []\n",
    "    \n",
    "    # Populates the above dictionaries according to the word senses associated with them\n",
    "    for token in word_tokens:\n",
    "        # Extracts best sense for each word using LESK\n",
    "        best_sense = lesk(sentence, token)\n",
    "        \n",
    "        if best_sense is not None:\n",
    "            # Obtains Synonyms\n",
    "            synonym = token\n",
    "            if best_sense. lemmas()[0].name() != token:\n",
    "                synonym = best_sense.lemmas()[0].name()\n",
    "            synonyms_list.append(synonym)\n",
    "            \n",
    "            # Obtains Hypernyms\n",
    "            if best_sense.hypernyms() != []:\n",
    "                hypernyms_list.append(best_sense.hypernyms()[0].lemmas()[0].name())\n",
    "        \n",
    "            # Obtains Hyponyms\n",
    "            if best_sense.hyponyms() != []:\n",
    "                hyponyms_list.append(best_sense.hyponyms()[0].lemmas()[0].name())\n",
    "            \n",
    "            # Obtains Meronyms\n",
    "            if best_sense.part_meronyms() != []:\n",
    "                meronyms_list.append(best_sense.part_meronyms()[0].lemmas()[0].name())\n",
    "                \n",
    "            # Obtains Holonyms\n",
    "            if best_sense.part_holonyms() != []:\n",
    "                holonyms_list.append(best_sense.part_holonyms()[0].lemmas()[0].name())\n",
    "          \n",
    "        # When there's no best sense, the token itself is the Synonym\n",
    "        else:\n",
    "            synonyms_list.append(token)\n",
    "            \n",
    "    return hypernyms_list, hyponyms_list, meronyms_list, holonyms_list, synonyms_list\n",
    "   \n",
    "    \n",
    "# Performs Dependency Parsing\n",
    "def DependencyParsing(sentence):\n",
    "    # Perform dependency parsing\n",
    "    parse, = dependencyParser.raw_parse(sentence)\n",
    "    \n",
    "    # Dependency parsing to parse tree based patterns as features\n",
    "    depParseResult = list(parse.triples())\n",
    "    \n",
    "    return depParseResult\n",
    "    \n",
    "    \n",
    "# Obtains Named Entities\n",
    "def NamedEntities(sentence, tokens):\n",
    "    # Word tokenize again and use them if NEs are present\n",
    "    namedTokens = nltk.word_tokenize(sentence)\n",
    "    NEtags = None\n",
    "    \n",
    "    try:\n",
    "        NEtags = namedEntityTagger.tag(namedTokens)\n",
    "    except:\n",
    "        NEtags = namedEntityTagger.tag(tokens)\n",
    "        \n",
    "    return NEtags\n",
    "\n",
    "# NLP pipeline through which all the articles & question will pass\n",
    "def NLP_Pipeline(sentence, count, corpus_dict, articleName = None):\n",
    "    #print(\"\\n------SENTENCE------\")\n",
    "    #print(sen)\n",
    "\n",
    "    word_tokens = Tokenization(sentence)\n",
    "    #print(\"\\nWord Tokenization : Done\")\n",
    "    #print(word_tokens)\n",
    "\n",
    "    word_NEtags = NamedEntities(sentence, word_tokens)\n",
    "    #print(\"\\nNamed Entity Tagging : Done\")\n",
    "    #print(word_NEtags)\n",
    "    \n",
    "    word_lemmas = Lemmatization(word_tokens)\n",
    "    #print(\"Word Lemmatization : Done\")\n",
    "    #print(word_lemmas)\n",
    "\n",
    "    word_POStags = POSTagging(word_tokens)\n",
    "    #print(\"POS Tagging : Done\")\n",
    "    #print(word_POStags)\n",
    "\n",
    "    hypernyms, hyponyms, meronyms, holonyms, synonyms = WordNetFeatures(sentence, word_tokens)\n",
    "    #print(\"WordNet Feature Extraction : Done\")\n",
    "    #print(holonyms)\n",
    "            \n",
    "    depParse = DependencyParsing(sentence)\n",
    "    #print(\"Dependency Parsing : Done\")\n",
    "    #print(depParse)\n",
    "\n",
    "    headList = getHeads(sentence, word_tokens)\n",
    "    #print(\"Obtaining Heads : Done\")\n",
    "    #print(headList)\n",
    "\n",
    "    # Process data format to suit the Elastic Search requirements\n",
    "    count = count + 1\n",
    "    corpus_dict[count] = {}\n",
    "            \n",
    "    corpus_dict[count][\"sentence\"] = {}\n",
    "    corpus_dict[count][\"sentence\"] = sentence\n",
    "            \n",
    "    corpus_dict[count][\"tokenized_text\"] = {}\n",
    "    corpus_dict[count][\"tokenized_text\"] = word_tokens\n",
    "            \n",
    "    corpus_dict[count][\"lemma\"] = {}\n",
    "    corpus_dict[count][\"lemma\"] = word_lemmas\n",
    "    \n",
    "    corpus_dict[count][\"ner_tag\"] = {}\n",
    "    if articleName is not None:\n",
    "        corpus_dict[count][\"ner_tag\"] = str(dict(word_NEtags))\n",
    "    else:\n",
    "        corpus_dict[count][\"ner_tag\"] = dict(word_NEtags)\n",
    "            \n",
    "    corpus_dict[count][\"tagged\"] = {}\n",
    "    corpus_dict[count][\"tagged\"] = word_POStags\n",
    "            \n",
    "    corpus_dict[count][\"dependency_parse\"] = {}\n",
    "    corpus_dict[count][\"dependency_parse\"] = depParse\n",
    "            \n",
    "    corpus_dict[count][\"synonyms\"] = {}\n",
    "    corpus_dict[count][\"synonyms\"] = synonyms\n",
    "            \n",
    "    corpus_dict[count][\"hypernyms\"] = {}\n",
    "    corpus_dict[count][\"hypernyms\"] = hypernyms\n",
    "            \n",
    "    corpus_dict[count][\"hyponyms\"] = {}\n",
    "    corpus_dict[count][\"hyponyms\"] = hyponyms\n",
    "            \n",
    "    corpus_dict[count][\"meronyms\"] = {}\n",
    "    corpus_dict[count][\"meronyms\"] = meronyms\n",
    "            \n",
    "    corpus_dict[count][\"holonyms\"] = {}\n",
    "    corpus_dict[count][\"holonyms\"] = holonyms\n",
    "            \n",
    "    corpus_dict[count][\"head_word\"] = {}\n",
    "    corpus_dict[count][\"head_word\"] = headList[0]\n",
    "    \n",
    "    # For question, we don't have the article name and then it will have a questionType\n",
    "    if articleName is not None:\n",
    "        corpus_dict[count][\"file_name\"] = {}\n",
    "        corpus_dict[count][\"file_name\"] = articleName\n",
    "        \n",
    "        \n",
    "    # For question, we should add the question type\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        questionTypes = [\"who\", \"when\", \"what\", \"whom\"]\n",
    "        queType = [i for i in questionTypes if i in tokens]\n",
    "        corpus_dict[count][\"type_of_question\"] = {}\n",
    "        corpus_dict[count][\"type_of_question\"] = queType\n",
    "    \n",
    "    return count, corpus_dict\n",
    "    \n",
    "    \n",
    "# Main method\n",
    "if __name__ == \"__main__\":\n",
    "    # List of all article names in the repository\n",
    "    articleNames = [\"109.txt\", \"111.txt\", \"151.txt\", \"160.txt\", \"177.txt\", \n",
    "                    \"179.txt\",\"181.txt\", \"196.txt\", \"199.txt\", \"220.txt\", \n",
    "                    \"222.txt\", \"226.txt\", \"247.txt\", \"273.txt\", \"281.txt\", \n",
    "                    \"282.txt\", \"285.txt\", \"287.txt\", \"288.txt\", \"297.txt\", \n",
    "                    \"304.txt\", \"342.txt\", \"347.txt\", \"360.txt\", \"390.txt\", \n",
    "                    \"400.txt\", \"428.txt\", \"56.txt\", \"58.txt\", \"6.txt\"] \n",
    "    fileCount = len(articleNames)\n",
    "    \n",
    "    content = \"\"\n",
    "    folderPath = \"https://raw.githubusercontent.com/SaiManasaVedantam/NLP-QA-System-Datasets/main/Articles/\"\n",
    "    for i in range(fileCount):\n",
    "        print(\"\\nStarted Processing File : \" + articleNames[i])\n",
    "        fileName = folderPath + articleNames[i]\n",
    "        response = urllib.request.urlopen(fileName)\n",
    "        webContents = response.read()\n",
    "        stringTypeData = webContents.decode(\"utf-8\")\n",
    "        content = stringTypeData\n",
    "        count = 0\n",
    "        corpus_dict = {}\n",
    "\n",
    "        # Obtain wordnet lemmatizer\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # Get tokenized content\n",
    "        sentences = []\n",
    "        tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "        sentences.extend(tokenizer.tokenize(content))\n",
    "\n",
    "        # Sentence count\n",
    "        #print(\"Total Sentences After splitting the document: \", len(sentences))\n",
    "        print(\"Extracting features for each sentence in the file...\")\n",
    "    \n",
    "        # Extracting words\n",
    "        for sen in sentences:\n",
    "            count, corpus_dict = NLP_Pipeline(sen, count, corpus_dict, articleNames[i])\n",
    "                \n",
    "        output_name = '../Pipeline-Output/Parsed-' + articleNames[i]\n",
    "        with open(output_name, 'w+', encoding='utf8') as output_file:\n",
    "            json.dump(corpus_dict, output_file,  indent=4, sort_keys=True, separators=(',', ': '), ensure_ascii=False)\n",
    "        \n",
    "        print(\"Completed Processing File : \" + articleNames[i])\n",
    "        \n",
    "    print(\"\\nTask 1 Successfully Completed !!!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba0e22dd-9991-432c-b65a-e15d7689405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started Processing File : 109.txt\n",
      "Finished Processing File : 109.txt\n",
      "\n",
      "Started Processing File : 111.txt\n",
      "Finished Processing File : 111.txt\n",
      "\n",
      "Started Processing File : 151.txt\n",
      "Finished Processing File : 151.txt\n",
      "\n",
      "Started Processing File : 160.txt\n",
      "Finished Processing File : 160.txt\n",
      "\n",
      "Started Processing File : 177.txt\n",
      "Finished Processing File : 177.txt\n",
      "\n",
      "Started Processing File : 179.txt\n",
      "Finished Processing File : 179.txt\n",
      "\n",
      "Started Processing File : 181.txt\n",
      "Finished Processing File : 181.txt\n",
      "\n",
      "Started Processing File : 196.txt\n",
      "Finished Processing File : 196.txt\n",
      "\n",
      "Started Processing File : 199.txt\n",
      "Finished Processing File : 199.txt\n",
      "\n",
      "Started Processing File : 220.txt\n",
      "Finished Processing File : 220.txt\n",
      "\n",
      "Started Processing File : 222.txt\n",
      "Finished Processing File : 222.txt\n",
      "\n",
      "Started Processing File : 226.txt\n",
      "Finished Processing File : 226.txt\n",
      "\n",
      "Started Processing File : 247.txt\n",
      "Finished Processing File : 247.txt\n",
      "\n",
      "Started Processing File : 273.txt\n",
      "Finished Processing File : 273.txt\n",
      "\n",
      "Started Processing File : 281.txt\n",
      "Finished Processing File : 281.txt\n",
      "\n",
      "Started Processing File : 282.txt\n",
      "Finished Processing File : 282.txt\n",
      "\n",
      "Started Processing File : 285.txt\n",
      "Finished Processing File : 285.txt\n",
      "\n",
      "Started Processing File : 287.txt\n",
      "Finished Processing File : 287.txt\n",
      "\n",
      "Started Processing File : 288.txt\n",
      "Finished Processing File : 288.txt\n",
      "\n",
      "Started Processing File : 297.txt\n",
      "Finished Processing File : 297.txt\n",
      "\n",
      "Started Processing File : 304.txt\n",
      "Finished Processing File : 304.txt\n",
      "\n",
      "Started Processing File : 342.txt\n",
      "Finished Processing File : 342.txt\n",
      "\n",
      "Started Processing File : 347.txt\n",
      "Finished Processing File : 347.txt\n",
      "\n",
      "Started Processing File : 360.txt\n",
      "Finished Processing File : 360.txt\n",
      "\n",
      "Started Processing File : 390.txt\n",
      "Finished Processing File : 390.txt\n",
      "\n",
      "Started Processing File : 400.txt\n",
      "Finished Processing File : 400.txt\n",
      "\n",
      "Started Processing File : 428.txt\n",
      "Finished Processing File : 428.txt\n",
      "\n",
      "Started Processing File : 56.txt\n",
      "Finished Processing File : 56.txt\n",
      "\n",
      "Started Processing File : 58.txt\n",
      "Finished Processing File : 58.txt\n",
      "\n",
      "Started Processing File : 6.txt\n",
      "Finished Processing File : 6.txt\n",
      "Elastic Search Successfully Completed !!!\n"
     ]
    }
   ],
   "source": [
    "# Turn off unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all the required packages\n",
    "import ssl\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import RequestsHttpConnection\n",
    "\n",
    "# Main method\n",
    "if __name__ == \"__main__\":\n",
    "    # List of all article names in the repository\n",
    "    articleNames = [\"109.txt\", \"111.txt\", \"151.txt\", \"160.txt\", \"177.txt\", \n",
    "                    \"179.txt\",\"181.txt\", \"196.txt\", \"199.txt\", \"220.txt\", \n",
    "                    \"222.txt\", \"226.txt\", \"247.txt\", \"273.txt\", \"281.txt\", \n",
    "                    \"282.txt\", \"285.txt\", \"287.txt\", \"288.txt\", \"297.txt\", \n",
    "                    \"304.txt\", \"342.txt\", \"347.txt\", \"360.txt\", \"390.txt\", \n",
    "                    \"400.txt\", \"428.txt\", \"56.txt\", \"58.txt\", \"6.txt\"] \n",
    "    fileCount = len(articleNames)\n",
    "    \n",
    "    # Setup Elastic Search\n",
    "    elastic = Elasticsearch([{'host': 'localhost', 'port': 9200, 'use_ssl' : False, 'ssl_verify' : False}], timeout=30, max_retries=10)\n",
    "    \n",
    "    # Obtain requests from the page\n",
    "    req = requests.get(\"http://localhost:9200\", verify=False)\n",
    "    \n",
    "    # Use indexing\n",
    "    idx = 1\n",
    "    \n",
    "    content = \"\"\n",
    "    folderPath = \"https://raw.githubusercontent.com/SaiManasaVedantam/NLP-QA-System-Datasets/main/Pipeline-Output/Parsed-\"\n",
    "    for i in range(fileCount):\n",
    "        print(\"\\nStarted Processing File : \" + articleNames[i])\n",
    "        fileName = folderPath + articleNames[i]\n",
    "        response = urllib.request.urlopen(fileName)\n",
    "        webContents = response.read()\n",
    "        stringTypeData = webContents.decode(\"utf-8\")\n",
    "        content = stringTypeData\n",
    "        \n",
    "        # Obtain Json data from file contents\n",
    "        jsonFile = json.loads(content)\n",
    "        \n",
    "        # Creating new index \"articles\" for each line in the article\n",
    "        for key, value in jsonFile.items():\n",
    "            elastic.index(index = \"articles\", doc_type = \"text\", id = idx, body = value)\n",
    "            # print(\"Here\")\n",
    "            idx += 1\n",
    "            \n",
    "        print(\"Finished Processing File : \" + articleNames[i])\n",
    "        \n",
    "    print(\"Elastic Search Successfully Completed !!!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9693d18-0157-43fb-bc95-b1550019eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains features in the question by using the result obtained from the NLP Pipeline\n",
    "def questionFeatures(question):\n",
    "    # Get all the wordnet features\n",
    "    WNfeatures = question[1]['synonyms'] + question[1]['meronyms'] + question[1]['hyponyms'] + question[1]['holonyms'] + question[1]['hypernyms']\n",
    "       \n",
    "    # Create hints for easy search using Named Entities and the Sentence head\n",
    "    head = question[1]['head_word'][0]\n",
    "    NEs = question[1]['ner_tag']\n",
    "    NEhints = \"\"\n",
    "    namedEntities = []\n",
    "    \n",
    "    for word, entity in NEs.items():\n",
    "        namedEntities.append(entity)\n",
    "        if entity == 'ORGANIZATION' or entity == 'LOCATION' or entity == 'PERSON':\n",
    "            NEhints += \" \" + word + \" \"\n",
    "            \n",
    "    NEhints += \" \" + head + \" \"\n",
    "    \n",
    "    # Obtain question type and other features\n",
    "    queType = question[1]['type_of_question']\n",
    "    lemmas = question[1]['lemma']\n",
    "    depParse = question[1]['dependency_parse']\n",
    "\n",
    "    depList = list(list(x) for x in depParse)\n",
    "    depElements = []\n",
    "    \n",
    "    for i in depList:\n",
    "        if i[1] == 'nsubj' or i[1] == 'dobj':\n",
    "            depElements.append(i[0])\n",
    "     \n",
    "    # Retrieve main elements from the dependency parse result\n",
    "    dependencyList = list(list(x) for x in depElements)\n",
    "\n",
    "    return NEhints, WNfeatures, queType, lemmas, dependencyList\n",
    "\n",
    "\n",
    "# Check and obtain matched sentences using the query string\n",
    "def GetMatchedSentences(queryStr, dependencyList):\n",
    "    querybody = {\n",
    "        \"query\": {\n",
    "            \"dis_max\": {\n",
    "                \"queries\": [\n",
    "                    # { \"match\": { \"lemma\": {\"query\": spclQuery,\"boost\": 2}  }},\n",
    "                    {\"multi_match\": {'query': queryStr, \"fields\": [\n",
    "                        # \"lemma^2.0\", \"synonyms^0.5\", \"meronyms^0.1\", \"holonyms^0.1\", \"hypernyms^0.1\", \"hyponyms^0.1\"]}},\n",
    "                        \"lemma^2\", \"ner_tag\", \"synonyms\", \"meronyms^0.5\", \"holonyms^0.5\", \"hypernyms^0.5\", \"hyponyms^0.5\"]}},\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    result = elastic.search(index = \"articles\", body=querybody)\n",
    "    answers = result['hits']['hits']\n",
    "    depParses, sentences, scores, articles, NEs = [], [], [], [], []\n",
    "    \n",
    "    for i in range(len(answers)):\n",
    "        sentence = result['hits']['hits'][i]['_source']['sentence']\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        score = result['hits']['hits'][i]['_score']\n",
    "        scores.append(score)\n",
    "        \n",
    "        depParse = result['hits']['hits'][i]['_source']['dependency_parse']\n",
    "        depParses.append(depParse)\n",
    "        \n",
    "        article = result['hits']['hits'][i]['_source']['file_name']\n",
    "        articles.append(article)\n",
    "        \n",
    "        NE = result['hits']['hits'][i]['_source']['ner_tag']\n",
    "        NEs.append(NE)\n",
    "        \n",
    "    return sentences, scores, depParses, articles, NEs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c0e66f9-9ce3-40d3-9395-16e0daab6232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the match score to know how well a statement is matched\n",
    "def FindScore(queType, NEhints, sentences, scores, depParses, articles, NEs, dependencyList):\n",
    "    # Add additional World Knowledge to implement a much deeper NLP pipeline\n",
    "    # Named Entities\n",
    "    organizations = ['ORGANIZATION']\n",
    "    persons = ['PERSON']\n",
    "    locations = ['LOCATION', 'PLACE', 'CITY', 'COUNTRY', 'STATE_OR_PROVINCE']\n",
    "    times = ['TIME', 'DATE', 'NUMBER']\n",
    "    \n",
    "    # Feeding world knowledge for a deeper pipeline\n",
    "    deaths = ['die', 'died', 'assassination']\n",
    "    births = ['born', 'birth', 'life']\n",
    "    keywords = NEhints.split()\n",
    "    keywords = [item.lower() for item in keywords]\n",
    "    \n",
    "    # Obtain relations using Dependency Parse result\n",
    "    count = 0\n",
    "    relations = []\n",
    "    for dep in depParses:\n",
    "        for i in dep:\n",
    "            if i[1] == 'nsubj' or i[1] == 'dboj':\n",
    "                if i[0] in dependencyList:\n",
    "                    relations.append([count,i[0]])\n",
    "        count += 1\n",
    "\n",
    "    # Get question type\n",
    "    questionType = queType[0].lower()\n",
    "    answers = []\n",
    "    \n",
    "    # Set for relation\n",
    "    for reln in relations:\n",
    "        idx = relations[0]\n",
    "        scores[idx] += 100\n",
    "        # print(sentenses[ano[0]])\n",
    "\n",
    "    # Handle different question types\n",
    "    if questionType == 'who' or questionType == 'whom':\n",
    "        for NE in NEs:\n",
    "            # Obtain all the named entities which are initially stored as a stringified dictionary\n",
    "            NEdict = eval(NE)\n",
    "            ans = []\n",
    "            for key, value in NEdict.items():\n",
    "                if value in persons or organizations:\n",
    "                    ans.append(key)\n",
    "                if (ans != [] and key == ',') or (ans != [] and key == 'and'):\n",
    "                    ans.append(key)\n",
    "                    \n",
    "            answers.append(' '.join(ans))\n",
    "\n",
    "    if questionType == 'when':\n",
    "        for NE in NEs:\n",
    "            # Obtain all the named entities which are initially stored as a stringified dictionary\n",
    "            NEdict = eval(NE)\n",
    "            ans = []\n",
    "            for key, value in NEdict.items():\n",
    "                if value in times and dateparser.parse(key) is not None:\n",
    "                    ans.append(key)\n",
    "\n",
    "            answers.append(' '.join(ans))\n",
    "\n",
    "    \"\"\"if questionType == 'what':\n",
    "        for NE in NEs:\n",
    "            # Obtain all the named entities which are initially stored as a stringified dictionary\n",
    "            NEdict = eval(NE)\n",
    "            ans = []\n",
    "            for key, value in NEdict.items():\n",
    "                if value in locations or value in organizations:\n",
    "                    ans.append(key)\n",
    "                if (ans != [] and key == ',') or (ans != [] and key == 'and'):\n",
    "                    ans.append(key)\n",
    "                    \n",
    "            answers.append(' '.join(ans))\"\"\"\n",
    "\n",
    "    \n",
    "    for idx in range(len(answers)):\n",
    "        if len(answers[idx]) < 3:\n",
    "            scores[idx] -= 100\n",
    "    \n",
    "    \"\"\"# Level 2 handling for When questions as it can also be about births & deaths\n",
    "    dieconcept = 0       \n",
    "    if questionType == 'when':\n",
    "        for key in range(len(sentences)):\n",
    "            for j in deaths:\n",
    "                if j in sentences[key]:\n",
    "                    pattern = r\"\\((.*?)\\)\"\n",
    "                    try:\n",
    "                        matched = re.findall(pattern, sentences[key])\n",
    "                        splits = matched[0].split(' ')\n",
    "                        splitjoin = ' '.join(splits[4:])\n",
    "                        answers[key] = splitjoin\n",
    "                        dieconcept = 1\n",
    "                    except:\n",
    "                        pass\n",
    "                    scores[key] += 50\n",
    "                    \n",
    "        if dieconcept == 0:\n",
    "            for key in range(0,len(sentences)):\n",
    "                for j in births:\n",
    "                    if j in sentences[key]:\n",
    "                        pattern = r\"\\((.*?)\\)\"\n",
    "                        try:\n",
    "                            matched = re.findall(pattern, sentences[key])\n",
    "                            splits = matched[0].split(' ')\n",
    "                            splitjoin = ' '.join(splits)\n",
    "                            answers[key] = splitjoin\n",
    "                            dieconcept = 1\n",
    "                        except:\n",
    "                            pass\n",
    "                        scores[key] += 10\"\"\"\n",
    "\n",
    "    results = zip(sentences, scores)\n",
    "    sortedResults = sorted(results, key = lambda x: x[1])\n",
    "\n",
    "    return reversed(sortedResults)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9180976e-853a-403d-8ca7-7dcddad23e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('By the 1500s, Ismail I from Ardabil, established the Safavid Dynasty, with Tabriz as the capital.', 65.544586) \n",
      "\n",
      "('Establishment of the Safavid Dynasty in 1501, converted the Iranian people from Sunni Islam to Twelver Shia Islam, and made Twelver Shia Islam the official religion of Iran.', 38.842426) \n",
      "\n",
      "('Safavid conversion of Iran from Sunnism to Shiism marked one of the most important turning points in Iranian and Muslim history.', 28.374105) \n",
      "\n",
      "('During this period the 3 Tamil Dynasties Chera dynasty, Chola dynasty and the Pandyan Dynasty ruled parts of southern India.', 27.94619) \n",
      "\n",
      "('The centuries-long geopolitical and ideological rivalry between Safavid Iran and the neighboring Ottoman Empire, led to numerous Ottoman–Persian Wars.', 26.817837) \n",
      "\n",
      "('Iran was predominantly Sunni, but Ismail instigated a forced conversion to the Shia branch of Islam, by which the Shia Islam spread throughout the Safavid territories in the Caucasus, Iran, Anatolia, and Mesopotamia.', 23.583992) \n",
      "\n",
      "('The Metropolitan Museum of Art writes that the Mongol rulers of the Yuan dynasty \"adopted Chinese political and cultural models; ruling from their capitals in Dadu, they assumed the role of Chinese emperors,\" although Tibetologist Thomas Laird dismissed the Yuan dynasty as a non-Chinese polity and plays down its Chinese characteristics.', 23.334194) \n",
      "\n",
      "('China Daily states that when the following Qing dynasty replaced the Ming dynasty, it merely \"strengthened administration of Tibet.\"', 23.299038) \n",
      "\n",
      "('The Safavid Era peaked in the reign of Abbas the Great, 1587–1629, surpassing their Ottoman arch rivals in strength, and making the empire a leading hub in Western Eurasia for the sciences and arts.', 23.02861) \n",
      "\n",
      "('From the 10th century, Sindh was ruled by the Rajput Soomra dynasty, and later, in the mid-13th century by the Rajput Samma dynasty.', 22.850765) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What was the capital of the Safavid Dynasty?\"\n",
    "count = 0\n",
    "\n",
    "# Pass the question through NLP pipeline\n",
    "count, queFromPipeline = NLP_Pipeline(question.lower(), count, corpus_dict, None)\n",
    "\n",
    "# Obtain features of the question which already passed through the NLP pipeline\n",
    "NEhints, WNfeatures, queType, lemmas, dependencyList = questionFeatures(queFromPipeline)\n",
    "\n",
    "# Form a query string with best possible features for reliable answers\n",
    "queryStr = NEhints + \" \" +' '.join(WNfeatures) + \" \" + ' '.join(lemmas)\n",
    "\n",
    "# Run the match query against indexed articles and obtain matched sentences\n",
    "sentences, scores, depParses, articles, NEs = GetMatchedSentences(queryStr, dependencyList)\n",
    "\n",
    "# Obtain only the relevant sentences\n",
    "relevantSentences = FindScore(queType, NEhints, sentences, scores, depParses, articles, NEs, dependencyList)\n",
    "\n",
    "for sent in relevantSentences:\n",
    "    print(sent, \"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a77b1-380a-4a0b-852c-3c64ee5ae0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\"\"\"\n",
    "WORK TO DO:\n",
    "\n",
    "1. Handle case-sensitive questions.\n",
    "-> Currently converting to lower. Need to check if it's ok.\n",
    "\n",
    "2. Handle WHAT case.\n",
    "-> Currently considering locations & organizations but that's not good.\n",
    "\n",
    "3. Handle output format.\n",
    "-> We should produce output as specified in the description.\n",
    "\n",
    "4. Handle multi-type questions.\n",
    "-> Easy, Medium, Hard.\n",
    "\n",
    "5. Find a way to produce top ranked result.\n",
    "-> Currently unreliable system.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
